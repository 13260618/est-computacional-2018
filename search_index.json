[
["index.html", "Estadística Computacional Información del curso", " Estadística Computacional María Teresa Ortiz Información del curso Notas del curso Estadística Computacional de los programas de maestría en Ciencia de Datos y en Computación del ITAM. Las notas fueron desarrolladas en 2014 por Teresa Ortiz quien las actualiza anualmente. En caso de encontrar errores o tener sugerencias del material se agradece la propuesta de correcciones mediante pull requests. Ligas Notas: https://tereom.github.io/est-computacional-2018/ Correo: teresa.ortiz.mancera@gmail.com GitHub: https://github.com/tereom/est-computacional-2018 Este trabajo está bajo una Licencia Creative Commons Atribución 4.0 Internacional. "],
["temario.html", "Temario", " Temario Manipulación y visualización de datos Visualización de datos. Manipulación y limpieza de datos. Temas selectos de programación en R. Referencias: Tufte (2006), Cleveland (1993), Wickham and Grolemund (2017), H. Wickham (2014). Inferencia y remuestreo Repaso de probabilidad. Muestreo y probabilidad. Inferencia. El principio del plug-in. Bootstrap Cálculo de errores estándar e intervalos de confianza. Estructuras de datos complejos. Introducción a modelos probabilísticos. Referencias: Ross (1998), Efron and Tibshirani (1993). Modelos de probabilidad y simulación Variables aleatorias y modelos probabilísticos. Familias importantes: discretas y continuas. Teoría básica de simulación El generador uniforme de números aleatorios. El método de la transformación inversa. Simulación de variables aleatorias discretas con soporte finito. Otras variables aleatorias. Simulación para modelos gráficos Modelos probabilíticos gráficos. Simulación (e.g. ANOVA, regresión simple). Inferencia paramétrica y remuestreo Modelos paramétricos. Bootsrap paramétrico. Inferencia de gráficas Referencias: Gelman and Hill (2007). Métodos computacionales e inferencia Bayesiana Inferencia bayesiana. Métodos diretos Familias conjugadas. Aproximación por cuadrícula. Aceptación y rechazo. MCMC Cadenas de Markov. Metropolis-Hastings. Muestreador de Gibbs. Monte Carlo Hamiltoniano. Diagnósticos de convergencia. Referencias: Kruschke (2015), Gelman et al. (2013). Calificación Tareas 20% (se envían por correo con título EstComp-TareaXX). Exámen parcial (proyecto y exámen en clase) 40%: Proyecto a casa: La descripción y material estará disponible en una liga aquí a partir del 3 de octubre y las respuestas se enviarán por correo a más tardar el 8 de octubre a las 16:00 horas. Cada día tarde se descuenta un punto del máximo puntaje (10), es decir, si entregan el 8 después de las 16:00 horas se califica sobre 9, si entregan el 9 después de las 16:00 horas se califica sobre 8, … El proyecto se puede realizar individual o en parejas. Examen en clase: Individual el 8 de octubre de 16:00 a 17:00 horas. Pueden consultar internet, notas, o cualquier recurso que traigan a clase. Examen final 40%. Software https://www.r-project.org https://www.rstudio.com http://mc-stan.org Otros recursos Socrative (Room ESTCOMP): Para encuestas y ejercicios en clase. DataCamp: Tenemos una cuenta de DataCamp para clases que les da acceso gratuito a todos los recursos de DataCamp a lo largo del semestre. Registro correos: Si quieres recibir noticias del curso y/o solicitar suscripción gratuita a DataCamp registra tu correo en este documento. Referencias "],
["introduccion-a-visualizacion.html", "Sección 1 Introducción a visualización", " Sección 1 Introducción a visualización “The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey El cuarteto de Ascombe En 1971 un estadístico llamado Frank Anscombe (fundador del departamento de Estadística de la Universidad de Yale) publicó cuatro conjuntos de datos, cada uno consiste de 11 observaciones y tienen las mismas propiedades estadísticas. Sin embargo, cuando analizamos los datos de manera gráfica en un histograma encontramos rápidamente que los conjuntos de datos son muy distintos. Media de \\(x\\): 9 Varianza muestral de \\(x\\): 11 Media de \\(y\\): 7.50 Varianza muestral de \\(y\\): 4.12 Correlación entre \\(x\\) y \\(y\\): 0.816 Línea de regresión lineal: \\(y = 3.00 + 0.500x\\) En la gráfica del primer conjunto de datos, se ven datos como los que se tendrían en una relación lineal simple con un modelo que cumple los supuestos de normalidad. La segunda gráfica (arriba a la derecha) muestra unos datos que tienen una asociación pero definitivamente no es lineal. En la tercera gráfica (abajo a la izquierda) están puntos alineados perfectamente en una línea recta, excepto por uno de ellos. En la última gráfica podemos ver un ejemplo en el cual basta tener una observación atípica para que se produzca un coeficiente de correlación alto aún cuando en realidad no existe una asociación lineal entre las dos variables. El cuarteto de Ascombe inspiró una técnica para crear datos que comparten las propiedades estadísticas al igual que en el cuarteto, pero que producen gráficas muy distintas (Matejka, Fitzmaurice). "],
["introduccion.html", "1.1 Introducción", " 1.1 Introducción La visualización de datos no trata de hacer gráficas “bonitas” o “divertidas”, ni de simplificar lo complejo o ayudar a una persona “que no entiende mucho” a entender ideas complejas. Más bien, trata de aprovechar nuestra gran capacidad de procesamiento visual para exhibir de manera clara aspectos importantes de los datos. El siguiente ejemplo de (Tufte 2006), ilustra claramente la diferencia entre estos dos enfoques. A la izquierda están gráficas (más o menos típicas de Powerpoint) basadas en la filosofía de simplificar, de intentar no “ahogar” al lector con datos. El resultado es una colección incoherente, de bajo contenido, que no tiene mucho qué decir y que es, “indeferente al contenido y la evidencia”. A la derecha está una variación del rediseño de Tufte en forma de tabla, que en este caso particular es una manera eficiente de mostrar claramente los patrones que hay en este conjunto simple de datos. ¿Qué principios son los que soportan la efectividad de esta tabla sobre la gráfica de la derecha? Veremos que hay dos conjuntos de principios importantes: unos relacionados con el diseño y otros con la naturaleza del análisis de datos, independientemente del método de visualización. Visualización de datos en la estadística La estadística tradicionalmente se divide en dos partes: una parte de naturaleza exploratoria, donde jugamos el papel de detectives en búsqueda de los elementos de evidencia importante, y una parte de naturaleza inferencial, donde nos convertimos en jueces donde le damos pesos de credibilidad a la evidencia que presenta el detective. Estas dos partes tienen interacción fuerte en la práctica, pero por razones históricas se considera “superior” a la parte inferencial por encima de la exploratoria. Aunque en el proceso de inferencia las gráficas cada vez son más importantes, la visualización entra más claramente dentro del análisis exploratorio de datos. Y como en un principio no es claro como la visualización aporta al proceso de la inferencia, se le consideró por mucho tiempo como un área de poca importancia para la estadística: una herramienta que en todo caso sirve para comunicar ideas simples, de manera deficiente, y a personas poco sofisticadas. El peor lado de este punto de vista consiste en restringirse a el análisis estadístico rutinario Cleveland (1993): aplicar las recetas y negarse a ver los datos de distinta manera (¡incluso pensar que esto puede sesgar los resultados, o que nos podría engañar!). El siguiente ejemplo muestra un caso grave y real (no simulado) de este análisis estadístico rutinario (tomado de Cleveland (1994)). A la derecha mostramos los resultados de un experimento de agricultura. Se cultivaron diez variedades de cebada en seis sitios de Minnesota, en 1921 y 1932. Este es uno de los primeros ejemplos en el que se aplicaron las ideas de Fisher en cuanto a diseño de experimentos. Estos datos fueron reanalizados desde esa época por muchos agrónomos. Hasta muy recientemente se detectó la anomalía en el comportamiento de los años en el sitio Morris, el cual es evidente en la gráfica. Investigación posterior ha mostrado convincentemente que en algún momento alguien volteó las etiquetas de los años en este sitio. Este ejemplo muestra, en primer lugar, que la visualización es crucial en el proceso de análisis de datos: sin ella estamos expuestos a no encontrar aspectos importantes de los datos (errores) que deben ser discutidos - aún cuando nuestra receta de análisis no considere estos aspectos. Ninguna receta puede aproximarse a describir todas las complejidades y detalles en un conjunto de datos de tamaño razonable (este ejemplo, en realidad, es chico). Sin embargo, la visualización de datos, por su enfoque menos estructurado, y el hecho de que se apoya en un medio con un “ancho de banda” mayor al que puede producir un cierto número de cantidades resumen, es ideal para investigar estos aspectos y detalles. Visualización popular de datos Publicaciones populares (periódicos, revistas, sitios internet) muchas veces incluyen visualización de datos como parte de sus artículos o reportajes. En general siguen el mismo patrón que en la visión tradicionalista de la estadística: sirven más para divertir que para explicar, tienden a explicar ideas simples y conjuntos chicos de datos, y se consideran como una “ayuda” para los “lectores menos sofisticados”. Casi siempre se trata de gráficas triviales (muchas veces con errores graves) que no aportan mucho a artículos que tienen un nivel de complejidad mucho mayor (es la filosofía: lo escrito para el adulto, lo graficado para el niño). Referencias "],
["teoria-de-visualizacion-de-datos.html", "1.2 Teoría de visualización de datos", " 1.2 Teoría de visualización de datos Existe teoría fundamentada acerca de la visualización. Después del trabajo pionero de Tukey, los principios e indicadores de Tufte se basan en un estudio de la historia de la graficación y ejercicios de muestreo de la práctica gráfica a lo largo de varias disciplinas (¿cuáles son las mejores gráficas? ¿por qué? El trabajo de Cleveland es orientado a la práctica del análisis de datos (¿cuáles gráficas nos han ayudado a mostrar claramente los resultados del análisis?), por una parte, y a algunos estudios de percepción visual. Principios generales del diseño analítico Aplicables a una presentación o análisis completos, y como guía para construir nuevas visualizaciones (Tufte 2006). Principio 1. Muestra comparaciones, contrastes, diferencias. Principio 2. Muestra causalidad, mecanismo, explicación, estructura sistemática. Principio 3. Muestra datos multivariados, es decir, más de una o dos variables. Principio 4. Integra palabras, números, imágenes y diagramas. Principio 5. Describe la totalidad de la evidencia. Muestra fuentes usadas y problemas relevantes. Principio 6. Las presentaciones analíticas, a fin de cuentas, se sostienen o caen dependiendo de la calidad, relevancia e integridad de su contenido. Técnicas de visualización Esta categoría incluye técnicas específicas que dependen de la forma de nuestros datos y el tipo de pregunta que queremos investigar (Tukey (1977), Cleveland (1993), Cleveland (1994), Tufte (2006)). Tipos de gráficas: cuantiles, histogramas, caja y brazos, gráficas de dispersión, puntos/barras/ líneas, series de tiempo. Técnicas para mejorar gráficas: Transformación de datos, transparencia, vibración, banking 45, suavizamiento y bandas de confianza. Pequeños múltiplos Tablas Indicadores de calidad gráfica Aplicables a cualquier gráfica en particular. Estas son guías concretas y relativamente objetivas para evaluar la calidad de una gráfica (Tufte 1986). Integridad Gráfica. El factor de engaño, es decir, la distorsión gráfica de las cantidades representadas, debe ser mínimo. Chartjunk. Minimizar el uso de decoración gráfica que interfiera con la interpretación de los datos: 3D, rejillas, rellenos con patrones. Tinta de datos. Maximizar la proporción de tinta de datos vs. tinta total de la gráfica. For non-data- ink, less is more. For data-ink, less is a bore. Densidad de datos. Las mejores gráficas tienen mayor densidad de datos, que es la razón entre el tamaño del conjunto de datos y el área de la gráfica. Las gráficas se pueden encoger mucho. Percepción visual. Algunas tareas son más fáciles para el ojo humano que otras (Cleveland 1994). Factor de engaño, chartjunk y pies El factor de engaño es el cociente entre el efecto mostrado en una gráfica y el efecto correspondiente en los datos. Idealmente, el factor de engaño debe ser 1 (ninguna distorsión). El chartjunk son aquellos elementos gráficos que no corresponden a variación de datos, o que entorpecen la interpretación de una gráfica. Estos son los indicadores de calidad más fáciles de entender y aplicar, y afortunadamente cada vez son menos comunes. Un diseño popular que califica como chartjunk y además introduce factores de engaño es el pie de 3D. En la gráfica de la derecha, podemos ver como la rebanada C se ve más grande que la rebanada A, aunque claramente ese no es el caso (factor de engaño). La razón es la variación en la perspectiva que no corresponde a variación en los datos (chartjunk). Corregimos quitando el efecto 3D. Esto reduce el factor de engaño pero hay todavía elementos que pueden mejorar la comprensión: se trata de la decodificiacion que hay que hacer categoría - color - cuantificación. Podemos agregar las etiquetas como se muestra en la serie de la derecha, pero entonces: ¿por qué no mostrar simplemente la tabla de datos? ¿qué agrega el pie a la interpretación? La deficiencias en el pie se pueden ver claramente al intentar graficar más categorías (13) . En el primer pie no podemos distinguir realmente cuáles son las categorías grandes y cuáles las chicas, y es muy difícil tener una imagen mental clara de estos datos. Agregar los porcentajes ayuda, pero entonces, otra vez, preguntamos cuál es el propósito del pie. La tabla de la izquierda hace todo el trabajo (una vez que ordenamos las categrías de la más grande a la más chica). Es posible hacer una gráfica de barras como la de abajo a la izquierda. Hay otros tipos de chartjunk comunes: uno es la textura de barras, por ejemplo. El efecto es la producción de un efecto moiré que es desagradable y quita la atención de los datos, como en la gráfica de barras de abajo. Otro común son las rejillas, como mostramos en las gráficas de la izquierda. Nótese como en estos casos hay efectos ópticos no planeados que degradan la percepción de los patrones en los datos. Series de tiempo y promedio de 45 Las series de tiempo son una especie particular de las gráficas de dispersión, en donde la dimensión horizontal es el tiempo. Buscamos entender cómo varía una medición dada en el tiempo. Estas gráficas son mas útiles cuando se construyen usando el principio del promedio de 45 grados: los patrones de variación en el tiempo se distinguen mejor (aproximadamente) cuando el promedio de pendiente (en valor absoluto) en las gráficas está cercano a 45 grados. El siguiente ejempo, que muestra la actividad de manchas solares del sol, muestra claramente este principio: Espto también es un principio para decidir la razón de aspecto de cualquier gráfica de dispersión (y también gráficas de barras). Esta regla supera el principio de que “las escalas deben comenzar en cero”. En realidad este último principio cuida contra dos errores en la graficación: no poner atención a la escala e intentar comparar gráficas que no están dibujadas en la misma escala. Poniendo atención a estos dos aspectos (incluso llamado a veces la atención a estos puntos, Stephen Few) no hay necesidad de seguir la regla del 0. Pequeños múltiplos y densidad gráfica La densidad de una gráfica es el tamaño del conjunto de datos que se grafica comparado con el área total de la gráfica. En el siguiente ejemplo, graficamos en logaritmo-10 de cabezas de ganado en Francia (cerdos, res, ovejas y caballos). La gráfica de la izquierda es pobre en densidad pues sólo representa 4 datos. La manera más fácil de mejorar la densidad es hacer más chica la gráfica: La razón de este encogimiento es una que tiene qué ver con las oportunidades perdidas de una gráfica grande. Si repetimos este mismo patrón (misma escala, mismos tipos de ganado) para distintos países obtenemos la siguiente gráfica: Esta es una gráfica de puntos. Es útil como sustituto de una gráfica de barras, y es superior en el sentido de que una mayor proporción de la tinta que se usa es tinta de datos. Otra vez, mayor proporción de tinta de datos representa más oportunidades que se pueden capitalizar, como muestra la gráfica de punto y líneas que mostramos al principio (rendimiento en campos de cebada). Tinta de datos Maximizar la proporción de tinta de datos en nuestras gráficas tiene beneficios inmediatos. La regla es: si hay tinta que no representa variación en los datos, o la eliminación de esa tinta no representa pérdidas de significado, esa tinta debe ser eliminada. El ejemplo más claro es el de las rejillas en gráficas y tablas: ¿Por qué usar grises en lugar de negros? La respuesta tiene qué ver con el principio de tinta de datos: si marcamos las diferencias sutil pero claramente, tenemos más oportunidades abiertas para hacer énfasis en lo que nos interesa: a una gráfica o tabla saturada no se le puede hacer más - es difícil agregar elementos adicionales que ayuden a la comprensión. Si comenzamos marcando con sutileza, entonces se puede hacer más. Los mapas geográficos son un buen ejemplo de este principio. El espacio en blanco es suficientemente bueno para indicar las fronteras en una tabla, y facilita la lectura: Percepción de escala Entre la percepción visual y la interpretación de una gráfica están implícitas tareas visuales específicas que las personas debemos realizar para ver correctamente la gráfica. En la década de los ochenta, William S. Cleveland y Robert McGill realizaron algunos experimentos identificando y clasificando estas tareas para diferentes tipos de gráficos (Cleveland and McGill 1984). En estos, se le pregunta a la persona que compare dos valores dentro de una gráfica, por ejemplo, en dos barras en una gráfica de barras, o dos rebanadas de una gráfica de pie. Los resultados de Cleveland y McGill fueron replicados por Heer y Bostock en 2010 y los resultados se muestran en las gráficas de la derecha: Minard Concluimos esta sección con una gráfica que, aunque poco común, ejemplifica los principios de una buena gráfica, y es reconocida como una de las mejores visualizaciones de la historia. Una gráfica excelente, presenta datos interesantes de forma bien diseñada: es una cuestión de fondo, de diseño, y estadística… [Se] compone de ideas complejas comunicadas con claridad, precisión y eficiencia. … [Es] lo que da al espectador la mayor cantidad de ideas, en el menor tiempo, con la menor cantidad de tinta, y en el espacio más pequeño. … Es casi siempre multivariado. … Una excelente gráfica debe decir la verdad acerca de los datos. (Tufte, 1983) La famosa visualización de Charles Joseph Minard de la marcha de Napoleón sobre Moscú, ilustra los principios de una buena gráfica. Tufte señala que esta imagen “bien podría ser el mejor gráfico estadístico jamás dibujado”, y sostiene que “cuenta una historia rica y coherente con sus datos multivariados, mucho más esclarecedora que un solo número que rebota en el tiempo”. Se representan seis variables: el tamaño del ejército, su ubicación en una superficie bidimensional, la dirección del movimiento del ejército y la temperatura en varias fechas durante la retirada de Moscú“. Hoy en día Minard es reconocido como uno de los principales contribuyentes a la teoría de análisis de datos y creación de infografías con un fundamento estadístico. Se grafican 6 variables: el número de tropas de Napoleón, la distancia, la temperatura, la latitud y la longitud, la dirección en que viajaban las tropas y la localización relativa a fechas específicas. La gráfica de Minard, como la describe E.J. Marey, parece “desafiar la pluma del historiador con su brutal elocuencia”, la combinación de datos del mapa, y la serie de tiempo, dibujados en 1869, “retratan una secuencia de pérdidas devastadoras que sufrieron las tropas de Napoleón en 1812”. Comienza en la izquierda, en la frontera de Polonia y Rusia, cerca del río Niemen. La línea gruesa dorada muestra el tamaño de la Gran Armada (422,000) en el momento en que invadía Rusia en junio de 1812. El ancho de esta banda indica el tamaño de la armada en cada punto del mapa. En septiembre, la armada llegó a Moscú, que ya había sido saqueada y dejada desértica, con sólo 100,000 hombres. El camino del retiro de Napoleón desde Moscú está representado por la línea oscuara (gris) que está en la parte inferior, que está relacionada a su vez con la temperatura y las fechas en el diagrama de abajo. Fue un invierno muy frío, y muchos se congelaron en su salida de Rusia. Como se muestra en el mapa, cruzar el río Berezina fue un desastre, y el ejército de Napoleón logró regresar a Polonia con tan sólo 10,000 hombres. También se muestran los movimientos de las tropas auxiliaries, que buscaban proteger por atrás y por la delantera mientras la armada avanzaba hacia Moscú. La gráfica de Minard cuenta una historia rica y cohesiva, coherente con datos multivariados y con los hechos históricos, y que puede ser más ilustrativa que tan sólo representar un número rebotando a lo largo del tiempo. Referencias "],
["introduccion-a-r-y-al-paquete-ggplot2.html", "Sección 2 Introducción a R y al paquete ggplot2", " Sección 2 Introducción a R y al paquete ggplot2 ¿Qué es R? R es un lenguaje de programación y un ambiente de cómputo estadístico R es software libre (no dice qué puedes o no hacer con el software), de código abierto (todo el código de R se puede inspeccionar - y se inspecciona). Cuando instalamos R, instala la base de R. Mucha de la funcionalidad adicional está en paquetes (conjunto de funciones y datos documentados) que la comunidad contribuye. ¿Cómo entender R? Hay una sesión de R corriendo. La consola de R es la interfaz entre R y nosotros. En la sesión hay objetos. Todo en R es un objeto: vectores, tablas, funciones, etc. Operamos aplicando funciones a los objetos y creando nuevos objetos. ¿Por qué R? R funciona en casi todas las plataformas (Mac, Windows, Linux e incluso en Playstation 3). R es un lenguaje de programación completo, permite desarrollo de DSLs. R promueve la investigación reproducible. R está actualizado gracias a que tiene una activa comunidad. Solo en CRAN hay cerca de 10,000 paquetes (funcionalidad adicional de R creadas creada por la comunidad). R se puede combinar con otras herramientas. R tiene capacidades gráficas muy sofisticadas. R es popular (Revolutions blog). "],
["r-primeros-pasos.html", "2.1 R: primeros pasos", " 2.1 R: primeros pasos Para comenzar se debe descargar R, esta descarga incluye R básico y un editor de textos para escribir código. Después de descargar R se recomienda descargar RStudio (gratis y libre). Rstudio es un ambiente de desarrollo integrado para R: incluye una consola, un editor de texto y un conjunto de herramientas para administrar el espacio de trabajo cuando se utiliza R. Algunos shortcuts útiles en RStudio son: En el editor command/ctrl + enter: enviar código a la consola ctrl + 2: mover el cursor a la consola En la consola flecha hacia arriba: recuperar comandos pasados ctrl + flecha hacia arriba: búsqueda en los comandos ctrl + 1: mover el cursor al editor R en análisis de datos El estándar científico para contestar preguntas o tomar decisiones es uno que se basa en el análisis de datos. Aquí consideramos técnicas cuantitativas: recolectar, organizar, entender, interpretar y extraer información de colecciones de datos predominantemente numéricos. Todas estas tareas son partes del análisis de datos, cuyo proceso podría resumirse con el siguiente diagrama: Es importante la forma en que nos movemos dentro de estos procesos en el análisis de datos y en este curso buscamos dar herramientas para facilitar cumplir los siguientes principios: Reproducibilidad. Debe ser posible reproducir el análisis en todos sus pasos, en cualquier momento. Claridad. Los pasos del análisis deben estar documentados apropiadamente, de manera que las decisiones importantes puedan ser entendidas y explicadas claramente. Dedicaremos las primeras sesiones a aprender herramientas básicas para poder movernos agilmente a lo largo de las etapas de análisis utilizando R y nos enfocaremos en los paquetes que forman parte del tidyverse. Paquetes y el Tidyverse La mejor manera de usar R para análisis de datos es aprovechando la gran cantidad de paquetes que aportan funcionalidad adicional. Desde Rstudio podemos instalar paquetes (Tools - &gt; Install packages o usar la función install.packages(&quot;nombre_paquete&quot;)). Una vez instalados, podemos cargarlos a nuestra sesión de R mediante library. Por ejemplo, para cargar el paquete readr hacemos: # print(read_csv) # Error in print(read_csv) : object &#39;read_csv&#39; not found library(tidyverse) print(read_csv) ## function (file, col_names = TRUE, col_types = NULL, locale = default_locale(), ## na = c(&quot;&quot;, &quot;NA&quot;), quoted_na = TRUE, quote = &quot;\\&quot;&quot;, comment = &quot;&quot;, ## trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, ## n_max), progress = show_progress()) ## { ## tokenizer &lt;- tokenizer_csv(na = na, quoted_na = TRUE, quote = quote, ## comment = comment, trim_ws = trim_ws) ## read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, ## locale = locale, skip = skip, comment = comment, n_max = n_max, ## guess_max = guess_max, progress = progress) ## } ## &lt;bytecode: 0x6dfa9b8&gt; ## &lt;environment: namespace:readr&gt; read_csv es una función que aporta el paquete readr, que a su vez está incluido en el tidyverse. Los paquetes se instalan una sola vez, sin embargo, se deben cargar (ejecutar library(tidyverse)) en cada sesión de R que los ocupemos. En estas notas utilizaremos la colección de paquetes incluídos en el tidyverse. Estos paquetes de R están diseñados para ciencia de datos, y para funcionar juntos como parte de un flujo de trabajo. La siguiente imagen tomada de Why the tidyverse (Joseph Rickert) indica que paquetes del tidyverse se utilizan para cada etapa del análisis de datos. knitr::include_graphics(&quot;imagenes/tidyverse.png&quot;) Recursos Existen muchos recursos gratuitos para aprender R, y resolver nuestras dudas, enlistamos algunos. Buscar ayuda: Google, StackOverflow. Para aprender más sobre un paquete o una función pueden visitar Rdocumentation.org. La referencia principal de estas notas es el libro R for Data Science de Hadley Wickham. Para aprender los comandos básicos de R Try R y Datacamp cuentan con excelentes cursos interactivos. Para aprender programación avanzada en R, el libro gratuito Advanced R de Hadley Wickham es una buena referencia. En particular es conveniente leer la guía de estilo (para todos: principiantes, intermedios y avanzados). Para mantenerse al tanto de las noticias de la comunidad de R pueden visitar R-bloggers. Más del tidyverse: Why the tidyverse Para aprovechar la funcionalidad de RStudio. "],
["visualizacion-con-ggplot2.html", "2.2 Visualización con ggplot2", " 2.2 Visualización con ggplot2 Utilizaremos el paquete ggplot2, fue desarrollado por Hadley Wickham y es una implementación de la gramática de las gráficas (Wilkinson et al. 2005). Gráficas de dispersión library(ggplot2) # Cargamos el paquete en nuestra sesión Usaremos el conjunto de datos mpg que se incluye en R, puedes encontrar información de esta base de datos tecleando ?mpg. data(mpg) ?mpg glimpse(mpg) ## Observations: 234 ## Variables: 11 ## $ manufacturer &lt;chr&gt; &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;... ## $ model &lt;chr&gt; &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4 qua... ## $ displ &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0,... ## $ year &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1... ## $ cyl &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6... ## $ trans &lt;chr&gt; &quot;auto(l5)&quot;, &quot;manual(m5)&quot;, &quot;manual(m6)&quot;, &quot;auto(av)... ## $ drv &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;,... ## $ cty &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 1... ## $ hwy &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 2... ## $ fl &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;,... ## $ class &lt;chr&gt; &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;comp... Comencemos con nuestra primera gráfica: ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) En ggplot2 se inicia una gráfica con la instrucción ggplot(), debemos especificar explicitamente que base de datos usamos, este es el primer argumento en la función ggplot. Una vez que creamos la base añadimos capas, y dentro de aes() escribimos las variables que queremos graficar y el atributo de la gráfica al que queremos mapearlas. La función geom_point() añade una capa de puntos, hay muchas funciones geometrías incluídas en ggplot2: geom_line(), geom_boxplot(), geom_histogram,… Cada una acepta distintos argumentos para mapear las variables en los datos a características estéticas de la gráfica. En el ejemplo de arriba mapeamos displ al eje x, hwy al eje y, pero geom_point() nos permite representar más variables usando la forma, color y/o tamaño del punto. Esta flexibilidad nos permite entender o descubrir patrones más interesantes en los datos. ggplot(mpg) + geom_point(aes(x = displ, y = hwy, color = class)) Experimenta con los aesthetics color (color), tamaño (size) y forma (shape).             ¿Qué diferencia hay entre las variables categóricas y las continuas?             ¿Qué ocurre cuando combinas varios aesthetics? El mapeo de las propiedades estéticas se denomina escalamiento y depende del tipo de variable, las variables discretas (por ejemplo, genero, escolaridad, país) se mapean a distintas escalas que las variables continuas (variables numéricas como edad, estatura, etc.), los defaults para algunos atributos son (los escalamientos se pueden modificar): aes Discreta Continua Color (color) Arcoiris de colores Gradiente de colores Tamaño (size) Escala discreta de tamaños Mapeo lineal entre el área y el valor Forma (shape) Distintas formas No aplica Transparencia (alpha) No aplica Mapeo lineal a la transparencia Los geoms controlan el tipo de gráfica p &lt;- ggplot(mpg, aes(x = displ, y = hwy)) p + geom_line() # en este caso no es una buena gráfica ¿Qué problema tiene la siguiente gráfica? p &lt;- ggplot(mpg, aes(x = cty, y = hwy)) p + geom_point() p + geom_jitter() ¿Cómo podemos mejorar la siguiente gráfica? ggplot(mpg, aes(x = class, y = hwy)) + geom_point() Intentemos reodenar los niveles de la variable clase ggplot(mpg, aes(x = reorder(class, hwy), y = hwy)) + geom_point() Podemos probar otros geoms. ggplot(mpg, aes(x = reorder(class, hwy), y = hwy)) + geom_jitter() ggplot(mpg, aes(x = reorder(class, hwy), y = hwy)) + geom_boxplot() También podemos usar más de un geom! ggplot(mpg, aes(x = reorder(class, hwy), y = hwy)) + geom_jitter() + geom_boxplot() Lee la ayuda de reorder y repite las gráficas anteriores ordenando por la mediana de hwy.             ¿Cómo harías para graficar los puntos encima de las cajas de boxplot? Paneles Veamos ahora como hacer páneles de gráficas, la idea es hacer varios múltiplos de una gráfica donde cada múltiplo representa un subconjunto de los datos, es una práctica muy útil para explorar relaciones condicionales. En ggplot podemos usar facet_wrap() para hacer paneles dividiendo los datos de acuerdo a las categorías de una sola variable ggplot(mpg, aes(x = displ, y = hwy)) + geom_jitter() + facet_wrap(~ cyl) También podemos hacer una cuadrícula de 2 dimensiones usando facet_grid(filas~columnas) ggplot(mpg, aes(x = displ, y = hwy)) + geom_jitter() + facet_grid(.~ class) ggplot(mpg, aes(x = displ, y = hwy)) + geom_jitter() + facet_grid(drv ~ class) Los páneles pueden ser muy útiles para entender relaciones en nuestros datos. En la siguiente gráfica es difícil entender si existe una relación entre radiación solar y ozono. data(airquality) ggplot(airquality, aes(x = Solar.R, y = Ozone)) + geom_point() ## Warning: Removed 42 rows containing missing values (geom_point). Veamos que ocurre si realizamos páneles separando por velocidad del viento. library(Hmisc) airquality$Wind.cat &lt;- cut2(airquality$Wind, g = 3) ggplot(airquality, aes(x = Solar.R, y = Ozone)) + geom_point() + facet_wrap(~ Wind.cat) Podemos agregar un suavizador (loess) para ver mejor la relación de las variables en cada panel. ggplot(airquality, aes(x = Solar.R, y = Ozone)) + geom_point() + facet_wrap(~ Wind.cat) + geom_smooth(method = &quot;lm&quot;) Escribe algunas preguntas que puedan contestar con estos datos. En ocasiones es necesario realizar transformaciones u obtener subconjuntos de los datos para poder responder preguntas de nuestro interés. library(dplyr) library(babynames) glimpse(babynames) ## Observations: 1,858,689 ## Variables: 5 ## $ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 188... ## $ sex &lt;chr&gt; &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F... ## $ name &lt;chr&gt; &quot;Mary&quot;, &quot;Anna&quot;, &quot;Emma&quot;, &quot;Elizabeth&quot;, &quot;Minnie&quot;, &quot;Margaret&quot;... ## $ n &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 128... ## $ prop &lt;dbl&gt; 0.072384329, 0.026679234, 0.020521700, 0.019865989, 0.017... Supongamos que queremos ver la tendencia del nombre “John”, para ello debemos generar un subconjunto de la base de datos. ¿Qué ocurre en la siguiente gráfica? babynames_John &lt;- filter(babynames, name == &quot;John&quot;) ggplot(babynames_John, aes(x = year, y = prop)) + geom_point() ggplot(babynames_John, aes(x = year, y = prop, color = sex)) + geom_line() La preparación de los datos es un aspecto muy importante del análisis y suele ser la fase que lleva más tiempo. Es por ello que el siguiente tema se enfocará en herramientas para hacer transformaciones de manera eficiente. Tarea. Explora la base de datos gapminder, estos datos están incluidos en el paquete del mismo nombre, para acceder a ellos basta con cargar el paquete: # install.packages(&quot;gapminder&quot;) library(gapminder) gapminder ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # ... with 1,694 more rows             realiza al menos 3 gráficas y explica las relaciones que encuentres. Debes usar lo que revisamos en estas notas: al menos una de las gráficas debe ser de páneles, realiza una gráfica con datos de México, y (opcional)si lo consideras interesante, puedes crear una variable categórica utilizando la función cut2 del paquete Hmisc. Recursos El libro R for Data Science (Wickham and Grolemund 2017) tiene un capítulo de visualización. Google, stackoverflow tiene un tag para ggplot2. Documentación con ejemplos en la página de ggplot2. Otro recurso muy útil es el acordeón de ggplot. La teoría detrás de ggplot2 se explica en el libro de ggplot2 (Wickham 2009), Referencias "],
["manipulacion-y-agrupacion-de-datos.html", "Sección 3 Manipulación y agrupación de datos", " Sección 3 Manipulación y agrupación de datos El material de la clase se puede descargar de aquí. En esta sección continuamos con la introducción a R para análisis de datos, en particular mostraremos herramientas de manipulación y transformación de datos. Trataremos los siguientes puntos: Estrategia separa-aplica-combina. Reestructura de datos y el principio de los datos limpios. Es sabido que limpieza y preparación de datos ocupan gran parte del tiempo del análisis de datos (Dasu y Johnson, 2003 y NYT’s ‘Janitor Work’ Is Key Hurdle to Insights), es por ello que vale la pena dedicar un tiempo a aprender técnicas que faciliten estas tareas, y entender que estructura en los datos es más conveniente para trabajar. "],
["transformacion-de-datos.html", "3.1 Transformación de datos", " 3.1 Transformación de datos Separa-aplica-combina (split-apply-combine) Muchos problemas de análisis de datos involucran la aplicación de la estrategia separa-aplica-combina (Wickham 2011), esta consiste en romper un problema en pedazos (de acuerdo a una variable de interés), operar sobre cada subconjunto de manera independiente (ej. calcular la media de cada grupo, ordenar observaciones por grupo, estandarizar por grupo) y después unir los pedazos nuevamente. El siguiente diagrama ejemplifiaca el paradigma de divide-aplica-combina: Separa la base de datos original. Aplica funciones a cada subconjunto. Combina los resultados en una nueva base de datos. Ahora, cuando pensamos como implementar la estrategia divide-aplica-combina es natural pensar en iteraciones, por ejemplo utilizar un ciclo for para recorrer cada grupo de interés y aplicar las funciones, sin embargo la aplicación de ciclos for desemboca en código difícil de entender por lo que preferimos trabajar con funciones creadas para estas tareas, usaremos el paquete dplyr que además de ser más claro suele ser más veloz. Estudiaremos las siguientes funciones: filter: obten un subconjunto de las filas de acuerdo a un criterio. select: selecciona columnas de acuerdo al nombre arrange: reordena las filas mutate: agrega nuevas variables summarise: reduce variables a valores (crear nuevas bases de datos con resúmenes de variables de la base original) Estas funciones trabajan de manera similar, el primer argumento que reciben es un data frame, los argumentos que siguen indican que operación se va a efectuar y el resultado es un nuevo data frame. Adicionalmente, se pueden usar con group_by que cambia el dominio de cada función, pasando de operar en el conjunto de datos completos a operar en grupos, esto lo veremos más adelante. Ejemplos y lectura de datos En esta sección trabajaremos con bases de datos de vuelos del aeropuerto de Houston. Comenzamos importando los datos a R. Para leer los datos usamos funciones del paquete readr que forma parte del tidyverse, notemos que si estamos usando RStudio podemos generar los comandos de lectura de datos usando la opción Import Dataset en la ventana de Environment. Si usamos la opción de importar datos usando la funcionalidad point-and-click de RStudio, es importante copiar los comandos al script de R para no perder reproducibilidad. library(tidyverse) flights &lt;- read_csv(&quot;data/flights.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; date = col_datetime(format = &quot;&quot;), #&gt; hour = col_integer(), #&gt; minute = col_integer(), #&gt; dep = col_integer(), #&gt; arr = col_integer(), #&gt; dep_delay = col_integer(), #&gt; arr_delay = col_integer(), #&gt; carrier = col_character(), #&gt; flight = col_integer(), #&gt; dest = col_character(), #&gt; plane = col_character(), #&gt; cancelled = col_integer(), #&gt; time = col_integer(), #&gt; dist = col_integer() #&gt; ) flights #&gt; # A tibble: 227,496 x 14 #&gt; date hour minute dep arr dep_delay arr_delay carrier #&gt; &lt;dttm&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 2011-01-01 12:00:00 14 0 1400 1500 0 -10 AA #&gt; 2 2011-01-02 12:00:00 14 1 1401 1501 1 -9 AA #&gt; 3 2011-01-03 12:00:00 13 52 1352 1502 -8 -8 AA #&gt; 4 2011-01-04 12:00:00 14 3 1403 1513 3 3 AA #&gt; 5 2011-01-05 12:00:00 14 5 1405 1507 5 -3 AA #&gt; 6 2011-01-06 12:00:00 13 59 1359 1503 -1 -7 AA #&gt; 7 2011-01-07 12:00:00 13 59 1359 1509 -1 -1 AA #&gt; 8 2011-01-08 12:00:00 13 55 1355 1454 -5 -16 AA #&gt; 9 2011-01-09 12:00:00 14 43 1443 1554 43 44 AA #&gt; 10 2011-01-10 12:00:00 14 43 1443 1553 43 43 AA #&gt; # ... with 227,486 more rows, and 6 more variables: flight &lt;int&gt;, #&gt; # dest &lt;chr&gt;, plane &lt;chr&gt;, cancelled &lt;int&gt;, time &lt;int&gt;, dist &lt;int&gt; weather &lt;- read_csv(&quot;data/weather.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; date = col_date(format = &quot;&quot;), #&gt; hour = col_integer(), #&gt; temp = col_double(), #&gt; dew_point = col_double(), #&gt; humidity = col_integer(), #&gt; pressure = col_double(), #&gt; visibility = col_double(), #&gt; wind_dir = col_character(), #&gt; wind_dir2 = col_integer(), #&gt; wind_speed = col_double(), #&gt; gust_speed = col_double(), #&gt; precip = col_double(), #&gt; conditions = col_character(), #&gt; events = col_character() #&gt; ) weather #&gt; # A tibble: 8,723 x 14 #&gt; date hour temp dew_point humidity pressure visibility wind_dir #&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 2011-01-01 0 59 28.9 32 29.9 10 NNE #&gt; 2 2011-01-01 1 57.2 28.4 33 29.9 10 NNE #&gt; 3 2011-01-01 2 55.4 28.4 36 29.9 10 NNW #&gt; 4 2011-01-01 3 53.6 28.4 38 29.9 10 North #&gt; 5 2011-01-01 4 NA NA NA 30.0 10 NNW #&gt; 6 2011-01-01 5 NA NA NA 30.0 10 North #&gt; 7 2011-01-01 6 53.1 17.1 24 30.0 10 North #&gt; 8 2011-01-01 7 53.1 16 23 30.1 10 North #&gt; 9 2011-01-01 8 54 18 24 30.1 10 North #&gt; 10 2011-01-01 9 55.4 17.6 23 30.1 10 NNE #&gt; # ... with 8,713 more rows, and 6 more variables: wind_dir2 &lt;int&gt;, #&gt; # wind_speed &lt;dbl&gt;, gust_speed &lt;dbl&gt;, precip &lt;dbl&gt;, conditions &lt;chr&gt;, #&gt; # events &lt;chr&gt; planes &lt;- read_csv(&quot;data/planes.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; plane = col_character(), #&gt; year = col_integer(), #&gt; mfr = col_character(), #&gt; model = col_character(), #&gt; no.eng = col_integer(), #&gt; no.seats = col_integer(), #&gt; speed = col_integer(), #&gt; engine = col_character(), #&gt; type = col_character() #&gt; ) planes #&gt; # A tibble: 2,853 x 9 #&gt; plane year mfr model no.eng no.seats speed engine type #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 N576AA 1991 MCDONNEL… DC-9-8… 2 172 NA Turbo-… Fixed win… #&gt; 2 N557AA 1993 MARZ BAR… KITFOX… 1 2 NA Recipr… Fixed win… #&gt; 3 N403AA 1974 RAVEN S55A NA 1 60 None Balloon #&gt; 4 N492AA 1989 MCDONNEL… DC-9-8… 2 172 NA Turbo-… Fixed win… #&gt; 5 N262AA 1985 MCDONNEL… DC-9-8… 2 172 NA Turbo-… Fixed win… #&gt; 6 N493AA 1989 MCDONNEL… DC-9-8… 2 172 NA Turbo-… Fixed win… #&gt; 7 N477AA 1988 MCDONNEL… DC-9-8… 2 172 NA Turbo-… Fixed win… #&gt; 8 N476AA 1988 MCDONNEL… DC-9-8… 2 172 NA Turbo-… Fixed win… #&gt; 9 N504AA NA AUTHIER … TIERRA… 1 2 NA Recipr… Fixed win… #&gt; 10 N565AA 1987 MCDONNEL… DC-9-8… 2 172 NA Turbo-… Fixed win… #&gt; # ... with 2,843 more rows airports &lt;- read_csv(&quot;data/airports.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; iata = col_character(), #&gt; airport = col_character(), #&gt; city = col_character(), #&gt; state = col_character(), #&gt; country = col_character(), #&gt; lat = col_double(), #&gt; long = col_double() #&gt; ) airports #&gt; # A tibble: 3,376 x 7 #&gt; iata airport city state country lat long #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 00M Thigpen Bay Springs MS USA 32.0 -89.2 #&gt; 2 00R Livingston Municipal Livingston TX USA 30.7 -95.0 #&gt; 3 00V Meadow Lake Colorado Springs CO USA 38.9 -105. #&gt; 4 01G Perry-Warsaw Perry NY USA 42.7 -78.1 #&gt; 5 01J Hilliard Airpark Hilliard FL USA 30.7 -81.9 #&gt; 6 01M Tishomingo County Belmont MS USA 34.5 -88.2 #&gt; 7 02A Gragg-Wade Clanton AL USA 32.9 -86.6 #&gt; 8 02C Capitol Brookfield WI USA 43.1 -88.2 #&gt; 9 02G Columbiana County East Liverpool OH USA 40.7 -80.6 #&gt; 10 03D Memphis Memorial Memphis MO USA 40.4 -92.2 #&gt; # ... with 3,366 more rows Filtrar Creamos una base de datos de juguete para mostrar el funcionamiento de cada instrucción: df_ej &lt;- tibble(genero = c(&quot;mujer&quot;, &quot;hombre&quot;, &quot;mujer&quot;, &quot;mujer&quot;, &quot;hombre&quot;), estatura = c(1.65, 1.80, 1.70, 1.60, 1.67)) df_ej #&gt; # A tibble: 5 x 2 #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 hombre 1.8 #&gt; 3 mujer 1.7 #&gt; 4 mujer 1.6 #&gt; 5 hombre 1.67 El primer argumento de filter() es el nombre del data frame, los subsecuentes son las expresiones que indican que filas filtrar. filter(df_ej, genero == &quot;mujer&quot;) #&gt; # A tibble: 3 x 2 #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 mujer 1.7 #&gt; 3 mujer 1.6 filter(df_ej, estatura &gt; 1.65 &amp; estatura &lt; 1.75) #&gt; # A tibble: 2 x 2 #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.7 #&gt; 2 hombre 1.67 Algunos operadores importantes para filtrar son: x &gt; 1 x &gt;= 1 x &lt; 1 x &lt;= 1 x != 1 x == 1 x %in% c(&quot;a&quot;, &quot;b&quot;) Debemos tener cuidado al usar == sqrt(2) ^ 2 == 2 #&gt; [1] FALSE 1/49 * 49 == 1 #&gt; [1] FALSE Los resultados de arriba se deben a que las computadoras usan aritmética de precisión finita: print(1/49 * 49, digits = 20) #&gt; [1] 0.99999999999999988898 Para estos casos es útil usar la función near() near(sqrt(2) ^ 2, 2) #&gt; [1] TRUE near(1 / 49 * 49, 1) #&gt; [1] TRUE Los operadores booleanos también son convenientes para filtrar: # Conjuntos a | b a &amp; b a &amp; !b xor(a, b) El siguiente esquema nos ayuda a entender que hace cada operación: knitr::include_graphics(&quot;imagenes/transform-logical.png&quot;) Encuentra todos los vuelos hacia SFO ó OAK.             Los vuelos con un retraso mayor a una hora.             En los que el retraso de llegada es más del doble que el retraso de salida. Un caso común es cuando se desea eliminar los datos con faltantes en una o más columnas de las tablas de datos, en R los datos faltantes se expresan como NA, para eliminar los faltantes en la variable dep_delay resulta natural escribir: filter(flights, dep_delay != NA) #&gt; # A tibble: 0 x 14 #&gt; # ... with 14 variables: date &lt;dttm&gt;, hour &lt;int&gt;, minute &lt;int&gt;, dep &lt;int&gt;, #&gt; # arr &lt;int&gt;, dep_delay &lt;int&gt;, arr_delay &lt;int&gt;, carrier &lt;chr&gt;, #&gt; # flight &lt;int&gt;, dest &lt;chr&gt;, plane &lt;chr&gt;, cancelled &lt;int&gt;, time &lt;int&gt;, #&gt; # dist &lt;int&gt; que nos devuelve una tabla vacía, sin embargo, si hay faltantes en esta variable. El problema resulta de usar el operador !=, pensemos ¿qué regresan las siguientes expresiones? 5 + NA NA / 2 sum(c(5, 4, NA)) mean(c(5, 4, NA)) NA &lt; 3 NA == 3 NA == NA Las expresiones anteriores regresan NA, el hecho que la media de un vector que incluye NAs o su suma regrese NAs se debe a que el default en R es propagar los valores faltantes, esto es, si deconozco el valor de una de las componentes de un vector, también desconozco la suma del mismo; sin embargo, muchas funciones tienen un argumento na.rm para removerlos, sum(c(5, 4, NA), na.rm = TRUE) #&gt; [1] 9 mean(c(5, 4, NA), na.rm = TRUE) #&gt; [1] 4.5 Aún queda pendiente, como filtrarlos en una tabla, para esto veamos que el manejo de datos faltantes en R utiliza una lógica ternaria (como SQL): NA == NA #&gt; [1] NA La expresión anterior puede resultar confusa, una manera de pensar en esto es considerar los NA como no sé, por ejemplo si no se la edad de Juan y no se la edad de Esteban, la respuesta a ¿Juan tiene la misma edad que Esteban? es no sé (NA). edad_Juan &lt;- NA edad_Esteban &lt;- NA edad_Juan == edad_Esteban #&gt; [1] NA edad_Jose &lt;- 32 # Juan es menor que José? edad_Juan &lt; edad_Jose #&gt; [1] NA Por tanto para determinar si un valor es faltante usamos la instrucción is.na(). is.na(NA) #&gt; [1] TRUE Y finalmente podemos filtrar con filter(flights, is.na(dep_delay)) Seleccionar Elegir columnas de un conjunto de datos. df_ej #&gt; # A tibble: 5 x 2 #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 hombre 1.8 #&gt; 3 mujer 1.7 #&gt; 4 mujer 1.6 #&gt; 5 hombre 1.67 select(df_ej, genero) #&gt; # A tibble: 5 x 1 #&gt; genero #&gt; &lt;chr&gt; #&gt; 1 mujer #&gt; 2 hombre #&gt; 3 mujer #&gt; 4 mujer #&gt; 5 hombre select(df_ej, -genero) #&gt; # A tibble: 5 x 1 #&gt; estatura #&gt; &lt;dbl&gt; #&gt; 1 1.65 #&gt; 2 1.8 #&gt; 3 1.7 #&gt; 4 1.6 #&gt; 5 1.67 select(df_ej, starts_with(&quot;g&quot;)) select(df_ej, contains(&quot;g&quot;)) Ve la ayuda de select (?select) y escribe tres maneras de seleccionar las variables de retraso (delay). Ordenar Ordenar de acuerdo al valor de una o más variables: arrange(df_ej, genero) #&gt; # A tibble: 5 x 2 #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 hombre 1.8 #&gt; 2 hombre 1.67 #&gt; 3 mujer 1.65 #&gt; 4 mujer 1.7 #&gt; 5 mujer 1.6 arrange(df_ej, desc(estatura)) #&gt; # A tibble: 5 x 2 #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 hombre 1.8 #&gt; 2 mujer 1.7 #&gt; 3 hombre 1.67 #&gt; 4 mujer 1.65 #&gt; 5 mujer 1.6 Ordena los vuelos por fecha de salida y hora.             ¿Cuáles son los vuelos con mayor retraso?             ¿Qué vuelos ganaron más tiempo en el aire? Mutar Mutar consiste en crear nuevas variables aplicando una función a columnas existentes: mutate(df_ej, estatura_cm = estatura * 100) #&gt; # A tibble: 5 x 3 #&gt; genero estatura estatura_cm #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 165 #&gt; 2 hombre 1.8 180 #&gt; 3 mujer 1.7 170 #&gt; 4 mujer 1.6 160 #&gt; 5 hombre 1.67 167 mutate(df_ej, estatura_cm = estatura * 100, estatura_in = estatura_cm * 0.3937) #&gt; # A tibble: 5 x 4 #&gt; genero estatura estatura_cm estatura_in #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 165 65.0 #&gt; 2 hombre 1.8 180 70.9 #&gt; 3 mujer 1.7 170 66.9 #&gt; 4 mujer 1.6 160 63.0 #&gt; 5 hombre 1.67 167 65.7 Calcula la velocidad en millas por hora a partir de la variable tiempo y la distancia (en millas). ¿Quá vuelo fue el más rápido?             Crea una nueva variable que muestre cuánto tiempo se ganó o perdió durante el vuelo. Hay muchas funciones que podemos usar para crear nuevas variables con mutate(), éstas deben cumplir ser funciones vectorizadas, es decir, reciben un vector de valores y devuelven un vector de la misma dimensión. Summarise y resúmenes por grupo Summarise sirve para crear nuevas bases de datos con resúmenes o agregaciones de los datos originales. summarise(df_ej, promedio = mean(estatura)) #&gt; # A tibble: 1 x 1 #&gt; promedio #&gt; &lt;dbl&gt; #&gt; 1 1.68 Podemos hacer resúmenes por grupo, primero creamos una base de datos agrupada: by_genero &lt;- group_by(df_ej, genero) by_genero #&gt; # A tibble: 5 x 2 #&gt; # Groups: genero [2] #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 hombre 1.8 #&gt; 3 mujer 1.7 #&gt; 4 mujer 1.6 #&gt; 5 hombre 1.67 y después operamos sobre cada grupo, creando un resumen a nivel grupo y uniendo los subconjuntos en una base nueva: summarise(by_genero, promedio = mean(estatura)) #&gt; # A tibble: 2 x 2 #&gt; genero promedio #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 hombre 1.74 #&gt; 2 mujer 1.65 Calcula el retraso promedio por fecha.             ¿Qué otros resúmenes puedes hacer para explorar el retraso por fecha? Algunas funciones útiles con summarise son min(x), median(x), max(x), quantile(x, p), n(), sum(x), sum(x &gt; 1), mean(x &gt; 1), sd(x). flights$date_only &lt;- as.Date(flights$date) by_date &lt;- group_by(flights, date_only) no_miss &lt;- filter(by_date, !is.na(dep)) delays &lt;- summarise(no_miss, mean_delay = mean(dep_delay), n = n()) Operador pipeline En R cuando uno hace varias operaciones es difícil leer y entender el código: hourly_delay &lt;- filter(summarise(group_by(filter(flights, !is.na(dep_delay)), date_only, hour), delay = mean(dep_delay), n = n()), n &gt; 10) La dificultad radica en que usualmente los parámetros se asignan después del nombre de la función usando (). El operador Forward Pipe (%&gt;%) cambia este orden, de manera que un parámetro que precede a la función es enviado (&quot;piped&quot;) a la función:x %&gt;% f(y)se vuelvef(x,y),x %&gt;% f(y) %&gt;% g(z)se vuelveg(f(x, y), z)`. Es así que podemos reescribir el código para poder leer las operaciones que vamos aplicando de izquierda a derecha y de arriba hacia abajo. Veamos como cambia el código anterior: hourly_delay &lt;- flights %&gt;% filter(!is.na(dep_delay)) %&gt;% group_by(date_only, hour) %&gt;% summarise(delay = mean(dep_delay), n = n()) %&gt;% filter(n &gt; 10) podemos leer %&gt;% como “después”. ¿Qué destinos tienen el promedio de retrasos más alto?             ¿Qué vuelos (compañía + vuelo) ocurren diario?             En promedio, ¿Cómo varían a lo largo del día los retrasos de vuelos no cancelados? (pista: hour + minute / 60) Variables por grupo En ocasiones es conveniente crear variables por grupo, por ejemplo estandarizar dentro de cada grupo z = (x - mean(x)) / sd(x). Veamos un ejemplo: planes &lt;- flights %&gt;% filter(!is.na(arr_delay)) %&gt;% group_by(plane) %&gt;% filter(n() &gt; 30) planes %&gt;% mutate(z_delay = (arr_delay - mean(arr_delay)) / sd(arr_delay)) %&gt;% filter(z_delay &gt; 5) #&gt; # A tibble: 1,403 x 16 #&gt; # Groups: plane [856] #&gt; date hour minute dep arr dep_delay arr_delay carrier #&gt; &lt;dttm&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 2011-01-28 12:00:00 15 16 1516 1916 351 326 CO #&gt; 2 2011-01-27 12:00:00 18 22 1822 1945 234 210 CO #&gt; 3 2011-01-27 12:00:00 21 37 2137 2254 242 219 CO #&gt; 4 2011-01-27 12:00:00 0 11 11 216 168 137 CO #&gt; 5 2011-01-27 12:00:00 22 37 2237 153 227 208 CO #&gt; 6 2011-01-27 12:00:00 21 28 2128 136 231 216 CO #&gt; 7 2011-01-26 12:00:00 11 46 1146 1633 171 193 CO #&gt; 8 2011-01-26 12:00:00 9 49 949 1436 144 180 CO #&gt; 9 2011-01-21 12:00:00 19 11 1911 2352 94 112 CO #&gt; 10 2011-01-20 12:00:00 6 35 635 807 780 775 CO #&gt; # ... with 1,393 more rows, and 8 more variables: flight &lt;int&gt;, #&gt; # dest &lt;chr&gt;, plane &lt;chr&gt;, cancelled &lt;int&gt;, time &lt;int&gt;, dist &lt;int&gt;, #&gt; # date_only &lt;date&gt;, z_delay &lt;dbl&gt; Verbos de dos tablas ¿Cómo mostramos los retrasos de los vuelos en un mapa? Para responder esta pregunta necesitamos unir la base de datos de vuelos con la de aeropuertos. location &lt;- airports %&gt;% select(dest = iata, name = airport, lat, long) flights %&gt;% group_by(dest) %&gt;% filter(!is.na(arr_delay)) %&gt;% summarise( arr_delay = mean(arr_delay), n = n() ) %&gt;% arrange(desc(arr_delay)) %&gt;% left_join(location) #&gt; Joining, by = &quot;dest&quot; #&gt; # A tibble: 116 x 6 #&gt; dest arr_delay n name lat long #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ANC 26.1 124 Ted Stevens Anchorage International 61.2 -150. #&gt; 2 CID 17.8 406 Eastern Iowa 41.9 -91.7 #&gt; 3 DSM 16.0 634 Des Moines International 41.5 -93.7 #&gt; 4 SFO 14.9 2800 San Francisco International 37.6 -122. #&gt; 5 BPT 14.3 3 Southeast Texas Regional 30.0 -94.0 #&gt; 6 GRR 13.7 665 Kent County International 42.9 -85.5 #&gt; 7 DAY 13.7 444 James M Cox Dayton Intl 39.9 -84.2 #&gt; 8 VPS 12.5 864 Eglin Air Force Base 30.5 -86.5 #&gt; 9 ECP 12.4 720 &lt;NA&gt; NA NA #&gt; 10 SAV 12.3 851 Savannah International 32.1 -81.2 #&gt; # ... with 106 more rows Hay varias maneras de unir dos bases de datos y debemos pensar en el obejtivo: x &lt;- tibble(name = c(&quot;John&quot;, &quot;Paul&quot;, &quot;George&quot;, &quot;Ringo&quot;, &quot;Stuart&quot;, &quot;Pete&quot;), instrument = c(&quot;guitar&quot;, &quot;bass&quot;, &quot;guitar&quot;, &quot;drums&quot;, &quot;bass&quot;, &quot;drums&quot;)) y &lt;- tibble(name = c(&quot;John&quot;, &quot;Paul&quot;, &quot;George&quot;, &quot;Ringo&quot;, &quot;Brian&quot;), band = c(&quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;FALSE&quot;)) x #&gt; # A tibble: 6 x 2 #&gt; name instrument #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John guitar #&gt; 2 Paul bass #&gt; 3 George guitar #&gt; 4 Ringo drums #&gt; 5 Stuart bass #&gt; 6 Pete drums y #&gt; # A tibble: 5 x 2 #&gt; name band #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John TRUE #&gt; 2 Paul TRUE #&gt; 3 George TRUE #&gt; 4 Ringo TRUE #&gt; 5 Brian FALSE inner_join(x, y) #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 4 x 3 #&gt; name instrument band #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John guitar TRUE #&gt; 2 Paul bass TRUE #&gt; 3 George guitar TRUE #&gt; 4 Ringo drums TRUE left_join(x, y) #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 6 x 3 #&gt; name instrument band #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John guitar TRUE #&gt; 2 Paul bass TRUE #&gt; 3 George guitar TRUE #&gt; 4 Ringo drums TRUE #&gt; 5 Stuart bass &lt;NA&gt; #&gt; 6 Pete drums &lt;NA&gt; semi_join(x, y) #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 4 x 2 #&gt; name instrument #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John guitar #&gt; 2 Paul bass #&gt; 3 George guitar #&gt; 4 Ringo drums anti_join(x, y) #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 2 x 2 #&gt; name instrument #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Stuart bass #&gt; 2 Pete drums Resumamos lo que observamos arriba: Tipo Acción inner Incluye únicamente las filas que aparecen tanto en x como en y left Incluye todas las filas en x y las filas de y que coincidan semi Incluye las filas de x que coincidan con y anti Incluye las filas de x que no coinciden con y Ahora combinamos datos a nivel hora con condiciones climáticas, ¿cuál es el tipo de unión adecuado? hourly_delay &lt;- flights %&gt;% group_by(date_only, hour) %&gt;% filter(!is.na(dep_delay)) %&gt;% summarise( delay = mean(dep_delay), n = n() ) %&gt;% filter(n &gt; 10) delay_weather &lt;- hourly_delay %&gt;% left_join(weather) #&gt; Joining, by = &quot;hour&quot; arrange(delay_weather, -delay) #&gt; # A tibble: 2,091,842 x 17 #&gt; # Groups: date_only [365] #&gt; date_only hour delay n date temp dew_point humidity #&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2011-05-12 23 184. 33 2011-01-02 43 28.9 58 #&gt; 2 2011-05-12 23 184. 33 2011-01-03 39 27 62 #&gt; 3 2011-05-12 23 184. 33 2011-01-04 50 45 83 #&gt; 4 2011-05-12 23 184. 33 2011-01-05 62.6 60.8 94 #&gt; 5 2011-05-12 23 184. 33 2011-01-06 53.1 36 52 #&gt; 6 2011-05-12 23 184. 33 2011-01-07 46.9 36 66 #&gt; 7 2011-05-12 23 184. 33 2011-01-08 50 43 77 #&gt; 8 2011-05-12 23 184. 33 2011-01-09 53.1 30 41 #&gt; 9 2011-05-12 23 184. 33 2011-01-10 41 37 86 #&gt; 10 2011-05-12 23 184. 33 2011-01-11 39.9 32 73 #&gt; # ... with 2,091,832 more rows, and 9 more variables: pressure &lt;dbl&gt;, #&gt; # visibility &lt;dbl&gt;, wind_dir &lt;chr&gt;, wind_dir2 &lt;int&gt;, wind_speed &lt;dbl&gt;, #&gt; # gust_speed &lt;dbl&gt;, precip &lt;dbl&gt;, conditions &lt;chr&gt;, events &lt;chr&gt; ¿Qué condiciones climáticas están asociadas con retrasos en las salidas de Houston?             Explora si los aviones más viejos están asociados a mayores retrasos, responde con una gráfica. Referencias "],
["datos-limpios.html", "3.2 Datos limpios", " 3.2 Datos limpios Una vez que importamos datos a R es conveniente limpiarlos, esto implica almacenarlos de una manera consisistente que nos permita enfocarnos en responder preguntas de los datos en lugar de estar luchando con los datos. Entonces, datos limpios son datos que facilitan las tareas del análisis de datos: Visualización: Resúmenes de datos usando gráficas, análisis exploratorio, o presentación de resultados. Manipulación: Manipulación de variables como agregar, filtrar, reordenar, transformar. Modelación: Ajustar modelos es sencillo si los datos están en la forma correcta. Los principios de datos limpios (Hadley Wickham 2014) proveen una manera estándar de organizar la información: Cada variable forma una columna. Cada observación forma un renglón. Cada tipo de unidad observacional forma una tabla. Vale la pena notar que los principios de los datos limpios se pueden ver como teoría de algebra relacional para estadísticos, estós principios equivalen a la tercera forma normal de Codd con enfoque en una sola tabla de datos en lugar de muchas conectadas en bases de datos relacionales. Veamos un ejemplo: La mayor parte de las bases de datos en estadística tienen forma rectangular, ¿cuántas variables tiene la siguiente tabla? tratamientoA tratamientoB Juan Aguirre - 2 Ana Bernal 16 11 José López 3 1 La tabla anterior también se puede estructurar de la siguiente manera: Juan Aguirre Ana Bernal José López tratamientoA - 16 3 tratamientoB 2 11 1 Si vemos los principios (cada variable forma una columna, cada observación forma un renglón, cada tipo de unidad observacional forma una tabla), ¿las tablas anteriores cumplen los principios? Para responder la pregunta identifiquemos primero cuáles son las variables y cuáles las observaciones de esta pequeña base. Las variables son: persona/nombre, tratamiento y resultado. Entonces, siguiendo los principios de datos limpios obtenemos la siguiente estructura: nombre tratamiento resultado Juan Aguirre a - Ana Bernal a 16 José López a 3 Juan Aguirre b 2 Ana Bernal b 11 José López b 1 Limpieza bases de datos Los principios de los datos limpios parecen obvios pero la mayor parte de los datos no los cumplen debido a: La mayor parte de la gente no está familiarizada con los principios y es difícil derivarlos por uno mismo. Los datos suelen estar organizados para facilitar otros aspectos que no son análisis, por ejemplo, la captura. Algunos de los problemas más comunes en las bases de datos que no están limpias son: Los encabezados de las columnas son valores y no nombres de variables. Más de una variable por columna. Las variables están organizadas tanto en filas como en columnas. Más de un tipo de observación en una tabla. Una misma unidad observacional está almacenada en múltiples tablas. La mayor parte de estos problemas se pueden arreglar con pocas herramientas, a continuación veremos como limpiar datos usando 2 funciones del paquete tidyr: gather: recibe múltiples columnas y las junta en pares de valores y nombres, convierte los datos anchos en largos. spread: recibe 2 columnas y las separa, haciendo los datos más anchos. Repasaremos los problemas más comunes que se encuentran en conjuntos de datos sucios y mostraremos como se puede manipular la tabla de datos (usando las funciones gather y spread) con el fin de estructurarla para que cumpla los principios de datos limpios. Los encabezados de las columanas son valores Usaremos ejemplos para entender los conceptos más facilmente. La primer base de datos está basada en una encuesta de Pew Research que investiga la relación entre ingreso y afiliación religiosa. ¿Cuáles son las variables en estos datos? library(tidyverse) pew &lt;- read_delim(&quot;http://stat405.had.co.nz/data/pew.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) #&gt; Parsed with column specification: #&gt; cols( #&gt; religion = col_character(), #&gt; `&lt;$10k` = col_integer(), #&gt; `$10-20k` = col_integer(), #&gt; `$20-30k` = col_integer(), #&gt; `$30-40k` = col_integer(), #&gt; `$40-50k` = col_integer(), #&gt; `$50-75k` = col_integer(), #&gt; `$75-100k` = col_integer(), #&gt; `$100-150k` = col_integer(), #&gt; `&gt;150k` = col_integer(), #&gt; `Don&#39;t know/refused` = col_integer() #&gt; ) pew #&gt; # A tibble: 18 x 11 #&gt; religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Agnostic 27 34 60 81 76 137 #&gt; 2 Atheist 12 27 37 52 35 70 #&gt; 3 Buddhist 27 21 30 34 33 58 #&gt; 4 Catholic 418 617 732 670 638 1116 #&gt; 5 Don’t k… 15 14 15 11 10 35 #&gt; 6 Evangel… 575 869 1064 982 881 1486 #&gt; 7 Hindu 1 9 7 9 11 34 #&gt; 8 Histori… 228 244 236 238 197 223 #&gt; 9 Jehovah… 20 27 24 24 21 30 #&gt; 10 Jewish 19 19 25 25 30 95 #&gt; 11 Mainlin… 289 495 619 655 651 1107 #&gt; 12 Mormon 29 40 48 51 56 112 #&gt; 13 Muslim 6 7 9 10 9 23 #&gt; 14 Orthodox 13 17 23 32 32 47 #&gt; 15 Other C… 9 7 11 13 13 14 #&gt; 16 Other F… 20 33 40 46 49 63 #&gt; 17 Other W… 5 2 3 4 2 7 #&gt; 18 Unaffil… 217 299 374 365 341 528 #&gt; # ... with 4 more variables: `$75-100k` &lt;int&gt;, `$100-150k` &lt;int&gt;, #&gt; # `&gt;150k` &lt;int&gt;, `Don&#39;t know/refused` &lt;int&gt; Esta base de datos tiene 3 variables: religión, ingreso y frecuencia. Para limpiarla es necesario apilar las columnas (alargar los datos). Notemos que al alargar los datos desapareceran las columnas que se agrupan y dan lugar a dos nuevas columnas: la correspondiente a clave y la correspondiente a valor. Entonces, para alargar una base de datos usamos la función gather que recibe los argumentos: data: base de datos que vamos a reestructurar. key: nombre de la nueva variable que contiene lo que fueron los nombres de columnas que apilamos. value: nombre de la variable que almacenará los valores que corresponden a cada key. …: lo último que especificamos son las columnas que vamos a apilar, la notación para seleccionarlas es la misma que usamos con select(). pew_tidy &lt;- gather(data = pew, income, frequency, -religion) pew_tidy #&gt; # A tibble: 180 x 3 #&gt; religion income frequency #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Agnostic &lt;$10k 27 #&gt; 2 Atheist &lt;$10k 12 #&gt; 3 Buddhist &lt;$10k 27 #&gt; 4 Catholic &lt;$10k 418 #&gt; 5 Don’t know/refused &lt;$10k 15 #&gt; 6 Evangelical Prot &lt;$10k 575 #&gt; 7 Hindu &lt;$10k 1 #&gt; 8 Historically Black Prot &lt;$10k 228 #&gt; 9 Jehovah&#39;s Witness &lt;$10k 20 #&gt; 10 Jewish &lt;$10k 19 #&gt; # ... with 170 more rows Observemos que en la tabla ancha teníamos bajo la columna &lt;$10k, en el renglón correspondiente a Agnostic un valor de 27, y podemos ver que este valor en la tabla larga se almacena bajo la columna frecuencia y corresponde a religión Agnostic, income &lt;$10k. También es importante ver que en este ejemplo especificamos las columnas a apilar identificando la que no vamos a alargar con un signo negativo: es decir apila todas las columnas menos religión. La nueva estructura de la base de datos nos permite, por ejemplo, hacer fácilmente una gráfica donde podemos comparar las diferencias en las frecuencias. Nota: En esta sección no explicaremos las funciones de graficación pues estas se cubren en las notas introductorias a R. En esta parte nos queremos concentrar en como limpiar datos y ejemplificar lo sencillo que es trabajar con datos limpios, esto es, una vez que los datos fueron reestructurados es fácil construir gráficas y resúmenes. ggplot(pew_tidy, aes(x = income, y = frequency, color = religion, group = religion)) + geom_line() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) Podemos hacer gráficas más interesantes si creamos nuevas variables: by_religion &lt;- group_by(pew_tidy, religion) pew_tidy_2 &lt;- pew_tidy %&gt;% filter(income != &quot;Don&#39;t know/refused&quot;) %&gt;% group_by(religion) %&gt;% mutate(percent = frequency / sum(frequency)) %&gt;% filter(sum(frequency) &gt; 1000) head(pew_tidy_2) #&gt; # A tibble: 6 x 4 #&gt; # Groups: religion [5] #&gt; religion income frequency percent #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Catholic &lt;$10k 418 0.0637 #&gt; 2 Evangelical Prot &lt;$10k 575 0.0724 #&gt; 3 Historically Black Prot &lt;$10k 228 0.138 #&gt; 4 Mainline Prot &lt;$10k 289 0.0471 #&gt; 5 Unaffiliated &lt;$10k 217 0.0698 #&gt; 6 Catholic $10-20k 617 0.0940 ggplot(pew_tidy_2, aes(x = income, y = percent, group = religion)) + facet_wrap(~ religion, nrow = 1) + geom_bar(stat = &quot;identity&quot;, fill = &quot;darkgray&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) En el código de arriba utilizamos las funciones group_by, filter y mutate que estudiaremos más adelante. Por ahora concentremonos en gather y spread. Otro ejemplo, veamos los datos de Billboard, aquí se registra la fecha en la que una canción entra por primera vez al top 100 de Billboard. billboard &lt;- read_csv(&quot;data/billboard.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; .default = col_integer(), #&gt; artist = col_character(), #&gt; track = col_character(), #&gt; time = col_time(format = &quot;&quot;), #&gt; date.entered = col_date(format = &quot;&quot;), #&gt; wk66 = col_character(), #&gt; wk67 = col_character(), #&gt; wk68 = col_character(), #&gt; wk69 = col_character(), #&gt; wk70 = col_character(), #&gt; wk71 = col_character(), #&gt; wk72 = col_character(), #&gt; wk73 = col_character(), #&gt; wk74 = col_character(), #&gt; wk75 = col_character(), #&gt; wk76 = col_character() #&gt; ) #&gt; See spec(...) for full column specifications. billboard #&gt; # A tibble: 317 x 81 #&gt; year artist track time date.entered wk1 wk2 wk3 wk4 wk5 #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;tim&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2000 2 Pac Baby… 04:22 2000-02-26 87 82 72 77 87 #&gt; 2 2000 2Ge+h… The … 03:15 2000-09-02 91 87 92 NA NA #&gt; 3 2000 3 Doo… Kryp… 03:53 2000-04-08 81 70 68 67 66 #&gt; 4 2000 3 Doo… Loser 04:24 2000-10-21 76 76 72 69 67 #&gt; 5 2000 504 B… Wobb… 03:35 2000-04-15 57 34 25 17 17 #&gt; 6 2000 98^0 Give… 03:24 2000-08-19 51 39 34 26 26 #&gt; 7 2000 A*Tee… Danc… 03:44 2000-07-08 97 97 96 95 100 #&gt; 8 2000 Aaliy… I Do… 04:15 2000-01-29 84 62 51 41 38 #&gt; 9 2000 Aaliy… Try … 04:03 2000-03-18 59 53 38 28 21 #&gt; 10 2000 Adams… Open… 05:30 2000-08-26 76 76 74 69 68 #&gt; # ... with 307 more rows, and 71 more variables: wk6 &lt;int&gt;, wk7 &lt;int&gt;, #&gt; # wk8 &lt;int&gt;, wk9 &lt;int&gt;, wk10 &lt;int&gt;, wk11 &lt;int&gt;, wk12 &lt;int&gt;, wk13 &lt;int&gt;, #&gt; # wk14 &lt;int&gt;, wk15 &lt;int&gt;, wk16 &lt;int&gt;, wk17 &lt;int&gt;, wk18 &lt;int&gt;, #&gt; # wk19 &lt;int&gt;, wk20 &lt;int&gt;, wk21 &lt;int&gt;, wk22 &lt;int&gt;, wk23 &lt;int&gt;, #&gt; # wk24 &lt;int&gt;, wk25 &lt;int&gt;, wk26 &lt;int&gt;, wk27 &lt;int&gt;, wk28 &lt;int&gt;, #&gt; # wk29 &lt;int&gt;, wk30 &lt;int&gt;, wk31 &lt;int&gt;, wk32 &lt;int&gt;, wk33 &lt;int&gt;, #&gt; # wk34 &lt;int&gt;, wk35 &lt;int&gt;, wk36 &lt;int&gt;, wk37 &lt;int&gt;, wk38 &lt;int&gt;, #&gt; # wk39 &lt;int&gt;, wk40 &lt;int&gt;, wk41 &lt;int&gt;, wk42 &lt;int&gt;, wk43 &lt;int&gt;, #&gt; # wk44 &lt;int&gt;, wk45 &lt;int&gt;, wk46 &lt;int&gt;, wk47 &lt;int&gt;, wk48 &lt;int&gt;, #&gt; # wk49 &lt;int&gt;, wk50 &lt;int&gt;, wk51 &lt;int&gt;, wk52 &lt;int&gt;, wk53 &lt;int&gt;, #&gt; # wk54 &lt;int&gt;, wk55 &lt;int&gt;, wk56 &lt;int&gt;, wk57 &lt;int&gt;, wk58 &lt;int&gt;, #&gt; # wk59 &lt;int&gt;, wk60 &lt;int&gt;, wk61 &lt;int&gt;, wk62 &lt;int&gt;, wk63 &lt;int&gt;, #&gt; # wk64 &lt;int&gt;, wk65 &lt;int&gt;, wk66 &lt;chr&gt;, wk67 &lt;chr&gt;, wk68 &lt;chr&gt;, #&gt; # wk69 &lt;chr&gt;, wk70 &lt;chr&gt;, wk71 &lt;chr&gt;, wk72 &lt;chr&gt;, wk73 &lt;chr&gt;, #&gt; # wk74 &lt;chr&gt;, wk75 &lt;chr&gt;, wk76 &lt;chr&gt; Notemos que el rank en cada semana (una vez que entró a la lista) está guardado en 75 columnas wk1 a wk75, este tipo de almacenamiento no es limpio pero puede ser útil al momento de ingresar la información. Para tener datos limpios apilamos las semanas de manera que sea una sola columna (nuevamente alargamos los datos): billboard_long &lt;- gather(billboard, week, rank, wk1:wk76, na.rm = TRUE) billboard_long #&gt; # A tibble: 5,307 x 7 #&gt; year artist track time date.entered week rank #&gt; * &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;tim&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2000 2 Pac Baby Don&#39;t Cry (Kee… 04:22 2000-02-26 wk1 87 #&gt; 2 2000 2Ge+her The Hardest Part Of… 03:15 2000-09-02 wk1 91 #&gt; 3 2000 3 Doors Down Kryptonite 03:53 2000-04-08 wk1 81 #&gt; 4 2000 3 Doors Down Loser 04:24 2000-10-21 wk1 76 #&gt; 5 2000 504 Boyz Wobble Wobble 03:35 2000-04-15 wk1 57 #&gt; 6 2000 98^0 Give Me Just One Ni… 03:24 2000-08-19 wk1 51 #&gt; 7 2000 A*Teens Dancing Queen 03:44 2000-07-08 wk1 97 #&gt; 8 2000 Aaliyah I Don&#39;t Wanna 04:15 2000-01-29 wk1 84 #&gt; 9 2000 Aaliyah Try Again 04:03 2000-03-18 wk1 59 #&gt; 10 2000 Adams, Yolan… Open My Heart 05:30 2000-08-26 wk1 76 #&gt; # ... with 5,297 more rows Notemos que en esta ocasión especificamos las columnas que vamos a apilar indicando el nombre de la primera de ellas seguido de : y por último el nombre de la última variable a apilar. Por otra parte, la instrucción na.rm = TRUE se utiliza para eliminar los renglones con valores faltantes en la columna de value (rank), esto es, eliminamos aquellas observaciones que tenían NA en la columnas wknum de la tabla ancha. Ahora realizamos una limpieza adicional creando mejores variables de fecha. billboard_tidy &lt;- billboard_long %&gt;% mutate( week = parse_number(week), date = date.entered + 7 * (week - 1), rank = as.numeric(rank) ) %&gt;% select(-date.entered) billboard_tidy #&gt; # A tibble: 5,307 x 7 #&gt; year artist track time week rank date #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;tim&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; #&gt; 1 2000 2 Pac Baby Don&#39;t Cry (Keep… 04:22 1 87 2000-02-26 #&gt; 2 2000 2Ge+her The Hardest Part Of … 03:15 1 91 2000-09-02 #&gt; 3 2000 3 Doors Down Kryptonite 03:53 1 81 2000-04-08 #&gt; 4 2000 3 Doors Down Loser 04:24 1 76 2000-10-21 #&gt; 5 2000 504 Boyz Wobble Wobble 03:35 1 57 2000-04-15 #&gt; 6 2000 98^0 Give Me Just One Nig… 03:24 1 51 2000-08-19 #&gt; 7 2000 A*Teens Dancing Queen 03:44 1 97 2000-07-08 #&gt; 8 2000 Aaliyah I Don&#39;t Wanna 04:15 1 84 2000-01-29 #&gt; 9 2000 Aaliyah Try Again 04:03 1 59 2000-03-18 #&gt; 10 2000 Adams, Yolanda Open My Heart 05:30 1 76 2000-08-26 #&gt; # ... with 5,297 more rows Nuevamente, podemos hacer gráficas facilmente. tracks &lt;- filter(billboard_tidy, track %in% c(&quot;Higher&quot;, &quot;Amazed&quot;, &quot;Kryptonite&quot;, &quot;Breathe&quot;, &quot;With Arms Wide Open&quot;)) ggplot(tracks, aes(x = date, y = rank)) + geom_line() + facet_wrap(~track, nrow = 1) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) Una columna asociada a más de una variable La siguiente base de datos proviene de la Organización Mundial de la Salud y contiene el número de casos confirmados de tuberculosis por país y año, la información esta por grupo demográfico de acuerdo a sexo (m, f), y edad (0-4, 5-14, etc). Los datos están disponibles en http://www.who.int/tb/country/data/download/en/. tb &lt;- read.csv(&quot;data/tb.csv&quot;) %&gt;% tbl_df() tb #&gt; # A tibble: 5,769 x 22 #&gt; iso2 year new_sp_m04 new_sp_m514 new_sp_m014 new_sp_m1524 new_sp_m2534 #&gt; * &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 AD 1989 NA NA NA NA NA #&gt; 2 AD 1990 NA NA NA NA NA #&gt; 3 AD 1991 NA NA NA NA NA #&gt; 4 AD 1992 NA NA NA NA NA #&gt; 5 AD 1993 NA NA NA NA NA #&gt; 6 AD 1994 NA NA NA NA NA #&gt; 7 AD 1996 NA NA 0 0 0 #&gt; 8 AD 1997 NA NA 0 0 1 #&gt; 9 AD 1998 NA NA 0 0 0 #&gt; 10 AD 1999 NA NA 0 0 0 #&gt; # ... with 5,759 more rows, and 15 more variables: new_sp_m3544 &lt;int&gt;, #&gt; # new_sp_m4554 &lt;int&gt;, new_sp_m5564 &lt;int&gt;, new_sp_m65 &lt;int&gt;, #&gt; # new_sp_mu &lt;int&gt;, new_sp_f04 &lt;int&gt;, new_sp_f514 &lt;int&gt;, #&gt; # new_sp_f014 &lt;int&gt;, new_sp_f1524 &lt;int&gt;, new_sp_f2534 &lt;int&gt;, #&gt; # new_sp_f3544 &lt;int&gt;, new_sp_f4554 &lt;int&gt;, new_sp_f5564 &lt;int&gt;, #&gt; # new_sp_f65 &lt;int&gt;, new_sp_fu &lt;int&gt; De manera similar a los ejemplos anteriores, utiliza la función gather para apilar las columnas correspondientes a sexo-edad.             Piensa en como podemos separar la “variable” sexo-edad en dos columnas. Ahora separaremos las variables sexo y edad de la columna demo, para ello debemos pasar a la función separate(), esta recibe como parámetros: el nombre de la base de datos, el nombre de la variable que deseamos separar en más de una, la posición de donde deseamos “cortar” (hay más opciones para especificar como separar, ver ?separate). El default es separar valores en todos los lugares que encuentre un caracter que no es alfanumérico (espacio, guión,…). tb_tidy &lt;- separate(tb_long, demo, c(&quot;sex&quot;, &quot;age&quot;), 8) tb_tidy #&gt; # A tibble: 35,750 x 5 #&gt; iso2 year sex age n #&gt; * &lt;fct&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 AD 2005 new_sp_m 04 0 #&gt; 2 AD 2006 new_sp_m 04 0 #&gt; 3 AD 2008 new_sp_m 04 0 #&gt; 4 AE 2006 new_sp_m 04 0 #&gt; 5 AE 2007 new_sp_m 04 0 #&gt; 6 AE 2008 new_sp_m 04 0 #&gt; 7 AG 2007 new_sp_m 04 0 #&gt; 8 AL 2005 new_sp_m 04 0 #&gt; 9 AL 2006 new_sp_m 04 1 #&gt; 10 AL 2007 new_sp_m 04 0 #&gt; # ... with 35,740 more rows table(tb_tidy$sex) #&gt; #&gt; new_sp_f new_sp_m #&gt; 17830 17920 # creamos un mejor código de genero tb_tidy &lt;- mutate(tb_tidy, sex = substr(sex, 8, 8)) table(tb_tidy$sex) #&gt; #&gt; f m #&gt; 17830 17920 Variables almacenadas en filas y columnas El problema más difícil es cuando las variables están tanto en filas como en columnas, veamos una base de datos de clima en Cuernavaca. ¿Cuáles son las variables en estos datos? clima &lt;- read_delim(&quot;data/clima.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) #&gt; Parsed with column specification: #&gt; cols( #&gt; .default = col_integer(), #&gt; id = col_character(), #&gt; element = col_character(), #&gt; d9 = col_character(), #&gt; d12 = col_character(), #&gt; d18 = col_character(), #&gt; d19 = col_character(), #&gt; d20 = col_character(), #&gt; d21 = col_character(), #&gt; d22 = col_character(), #&gt; d24 = col_character() #&gt; ) #&gt; See spec(...) for full column specifications. Estos datos tienen variables en columnas individuales (id, año, mes), en múltiples columnas (día, d1-d31) y en filas (tmin, tmax). Comencemos por apilar las columnas. clima_long &lt;- gather(clima, day, value, d1:d31, na.rm = TRUE) clima_long #&gt; # A tibble: 66 x 6 #&gt; id year month element day value #&gt; * &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 MX000017004 2010 12 TMAX d1 299 #&gt; 2 MX000017004 2010 12 TMIN d1 138 #&gt; 3 MX000017004 2010 2 TMAX d2 273 #&gt; 4 MX000017004 2010 2 TMIN d2 144 #&gt; 5 MX000017004 2010 11 TMAX d2 313 #&gt; 6 MX000017004 2010 11 TMIN d2 163 #&gt; 7 MX000017004 2010 2 TMAX d3 241 #&gt; 8 MX000017004 2010 2 TMIN d3 144 #&gt; 9 MX000017004 2010 7 TMAX d3 286 #&gt; 10 MX000017004 2010 7 TMIN d3 175 #&gt; # ... with 56 more rows Podemos crear algunas variables adicionales. clima_vars &lt;- clima_long %&gt;% mutate(day = parse_number(day), value = as.numeric(value) / 10) %&gt;% select(id, year, month, day, element, value) %&gt;% arrange(id, year, month, day) clima_vars #&gt; # A tibble: 66 x 6 #&gt; id year month day element value #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 MX000017004 2010 1 30 TMAX 27.8 #&gt; 2 MX000017004 2010 1 30 TMIN 14.5 #&gt; 3 MX000017004 2010 2 2 TMAX 27.3 #&gt; 4 MX000017004 2010 2 2 TMIN 14.4 #&gt; 5 MX000017004 2010 2 3 TMAX 24.1 #&gt; 6 MX000017004 2010 2 3 TMIN 14.4 #&gt; 7 MX000017004 2010 2 11 TMAX 29.7 #&gt; 8 MX000017004 2010 2 11 TMIN 13.4 #&gt; 9 MX000017004 2010 2 23 TMAX 29.9 #&gt; 10 MX000017004 2010 2 23 TMIN 10.7 #&gt; # ... with 56 more rows Finalmente, la columna element no es una variable, sino que almacena el nombre de dos variables, la operación que debemos aplicar (spread) es el inverso de apilar (gather): clima_tidy &lt;- spread(clima_vars, element, value) clima_tidy #&gt; # A tibble: 33 x 6 #&gt; id year month day TMAX TMIN #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 MX000017004 2010 1 30 27.8 14.5 #&gt; 2 MX000017004 2010 2 2 27.3 14.4 #&gt; 3 MX000017004 2010 2 3 24.1 14.4 #&gt; 4 MX000017004 2010 2 11 29.7 13.4 #&gt; 5 MX000017004 2010 2 23 29.9 10.7 #&gt; 6 MX000017004 2010 3 5 32.1 14.2 #&gt; 7 MX000017004 2010 3 10 34.5 16.8 #&gt; 8 MX000017004 2010 3 16 31.1 17.6 #&gt; 9 MX000017004 2010 4 27 36.3 16.7 #&gt; 10 MX000017004 2010 5 27 33.2 18.2 #&gt; # ... with 23 more rows Ahora es inmediato no solo hacer gráficas sino también ajustar un modelo. # ajustamos un modelo lineal donde la variable respuesta es temperatura # máxima, y la variable explicativa es el mes clima_lm &lt;- lm(TMAX ~ factor(month), data = clima_tidy) summary(clima_lm) #&gt; #&gt; Call: #&gt; lm(formula = TMAX ~ factor(month), data = clima_tidy) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.65 -0.92 -0.02 1.05 3.18 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 27.8000 1.8610 14.938 5.34e-13 *** #&gt; factor(month)2 -0.0500 2.0807 -0.024 0.98104 #&gt; factor(month)3 4.7667 2.1489 2.218 0.03717 * #&gt; factor(month)4 8.5000 2.6319 3.230 0.00385 ** #&gt; factor(month)5 5.4000 2.6319 2.052 0.05228 . #&gt; factor(month)6 1.2500 2.2793 0.548 0.58892 #&gt; factor(month)7 1.4500 2.2793 0.636 0.53123 #&gt; factor(month)8 0.4714 1.9895 0.237 0.81488 #&gt; factor(month)10 1.1000 2.0386 0.540 0.59491 #&gt; factor(month)11 0.3200 2.0386 0.157 0.87670 #&gt; factor(month)12 1.0500 2.2793 0.461 0.64955 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.861 on 22 degrees of freedom #&gt; Multiple R-squared: 0.6182, Adjusted R-squared: 0.4447 #&gt; F-statistic: 3.563 on 10 and 22 DF, p-value: 0.006196 Mas de un tipo de observación en una misma tabla En ocasiones las bases de datos involucran valores en diferentes niveles, en diferentes tipos de unidad observacional. En la limpieza de datos, cada unidad observacional debe estar almacenada en su propia tabla (esto esta ligado a normalización de una base de datos), es importante para evitar inconsistencias en los datos. ¿Cuáles son las unidades observacionales de los datos de billboard? billboard_tidy #&gt; # A tibble: 5,307 x 7 #&gt; year artist track time week rank date #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;tim&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; #&gt; 1 2000 2 Pac Baby Don&#39;t Cry (Keep… 04:22 1 87 2000-02-26 #&gt; 2 2000 2Ge+her The Hardest Part Of … 03:15 1 91 2000-09-02 #&gt; 3 2000 3 Doors Down Kryptonite 03:53 1 81 2000-04-08 #&gt; 4 2000 3 Doors Down Loser 04:24 1 76 2000-10-21 #&gt; 5 2000 504 Boyz Wobble Wobble 03:35 1 57 2000-04-15 #&gt; 6 2000 98^0 Give Me Just One Nig… 03:24 1 51 2000-08-19 #&gt; 7 2000 A*Teens Dancing Queen 03:44 1 97 2000-07-08 #&gt; 8 2000 Aaliyah I Don&#39;t Wanna 04:15 1 84 2000-01-29 #&gt; 9 2000 Aaliyah Try Again 04:03 1 59 2000-03-18 #&gt; 10 2000 Adams, Yolanda Open My Heart 05:30 1 76 2000-08-26 #&gt; # ... with 5,297 more rows Separemos esta base de datos en dos: la tabla canción que almacena artista, nombre de la canción y duración; la tabla rank que almacena el ranking de la canción en cada semana. song &lt;- billboard_tidy %&gt;% select(artist, track, year, time) %&gt;% unique() %&gt;% arrange(artist) %&gt;% mutate(song_id = row_number(artist)) song #&gt; # A tibble: 317 x 5 #&gt; artist track year time song_id #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;time&gt; &lt;int&gt; #&gt; 1 2 Pac Baby Don&#39;t Cry (Keep... 2000 04:22 1 #&gt; 2 2Ge+her The Hardest Part Of ... 2000 03:15 2 #&gt; 3 3 Doors Down Kryptonite 2000 03:53 3 #&gt; 4 3 Doors Down Loser 2000 04:24 4 #&gt; 5 504 Boyz Wobble Wobble 2000 03:35 5 #&gt; 6 98^0 Give Me Just One Nig... 2000 03:24 6 #&gt; 7 A*Teens Dancing Queen 2000 03:44 7 #&gt; 8 Aaliyah I Don&#39;t Wanna 2000 04:15 8 #&gt; 9 Aaliyah Try Again 2000 04:03 9 #&gt; 10 Adams, Yolanda Open My Heart 2000 05:30 10 #&gt; # ... with 307 more rows rank &lt;- billboard_tidy %&gt;% left_join(song, c(&quot;artist&quot;, &quot;track&quot;, &quot;year&quot;, &quot;time&quot;)) %&gt;% select(song_id, date, week, rank) %&gt;% arrange(song_id, date) %&gt;% tbl_df rank #&gt; # A tibble: 5,307 x 4 #&gt; song_id date week rank #&gt; &lt;int&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2000-02-26 1 87 #&gt; 2 1 2000-03-04 2 82 #&gt; 3 1 2000-03-11 3 72 #&gt; 4 1 2000-03-18 4 77 #&gt; 5 1 2000-03-25 5 87 #&gt; 6 1 2000-04-01 6 94 #&gt; 7 1 2000-04-08 7 99 #&gt; 8 2 2000-09-02 1 91 #&gt; 9 2 2000-09-09 2 87 #&gt; 10 2 2000-09-16 3 92 #&gt; # ... with 5,297 more rows Una misma unidad observacional está almacenada en múltiples tablas También es común que los valores sobre una misma unidad observacional estén separados en muchas tablas o archivos, es común que estas tablas esten divididas de acuerdo a una variable, de tal manera que cada archivo representa a una persona, año o ubicación. Para juntar los archivos hacemos lo siguiente: Leemos los archivos en una lista de tablas. Para cada tabla agregamos una columna que registra el nombre del archivo original. Combinamos las tablas en un solo data frame. Veamos un ejemplo, descarga la carpeta specdata, ésta contiene 332 archivos csv que almacenan información de monitoreo de contaminación en 332 ubicaciones de EUA. Cada archivo contiene información de una unidad de monitoreo y el número de identificación del monitor es el nombre del archivo. Los pasos en R (usando el paquete purrr), primero creamos un vector con los nombres de los archivos en un directorio, eligiendo aquellos que contengan las letras “.csv”. paths &lt;- dir(&quot;data/specdata&quot;, pattern = &quot;\\\\.csv$&quot;, full.names = TRUE) Después le asignamos el nombre del csv al nombre de cada elemento del vector. Este paso se realiza para preservar los nombres de los archivos ya que estos los asignaremos a una variable mas adelante. paths &lt;- set_names(paths, basename(paths)) La función map_df itera sobre cada dirección, lee el csv en dicha dirección y los combina en un data frame. specdata_us &lt;- map_df(paths, ~read_csv(., col_types = &quot;Tddi&quot;), .id = &quot;filename&quot;) # eliminamos la basura del id specdata &lt;- specdata_us %&gt;% mutate(monitor = parse_number(filename)) %&gt;% select(id = ID, monitor, date = Date, sulfate, nitrate) specdata #&gt; # A tibble: 772,087 x 5 #&gt; id monitor date sulfate nitrate #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 2003-01-01 00:00:00 NA NA #&gt; 2 1 1 2003-01-02 00:00:00 NA NA #&gt; 3 1 1 2003-01-03 00:00:00 NA NA #&gt; 4 1 1 2003-01-04 00:00:00 NA NA #&gt; 5 1 1 2003-01-05 00:00:00 NA NA #&gt; 6 1 1 2003-01-06 00:00:00 NA NA #&gt; 7 1 1 2003-01-07 00:00:00 NA NA #&gt; 8 1 1 2003-01-08 00:00:00 NA NA #&gt; 9 1 1 2003-01-09 00:00:00 NA NA #&gt; 10 1 1 2003-01-10 00:00:00 NA NA #&gt; # ... with 772,077 more rows Otras consideraciones En las buenas prácticas es importante tomar en cuenta los siguientes puntos: Incluir un encabezado con el nombre de las variables. Los nombres de las variables deben ser entendibles (e.g. AgeAtDiagnosis es mejor que AgeDx). En general los datos se deben guardar en un archivo por tabla. Escribir un script con las modificaciones que se hicieron a los datos crudos (reproducibilidad). Otros aspectos importantes en la limpieza de datos son: selección del tipo de variables (por ejemplo fechas), datos faltantes, typos y detección de valores atípicos. Recursos adicionales Data Import Cheat Sheet, RStudio. Data Transformation Cheat Sheet, RStudio. Referencias "],
["temas-selectos-de-r.html", "Sección 4 Temas selectos de R", " Sección 4 Temas selectos de R Esta sección describe algunos aspectos de R como lenguaje de programación (en contraste a introducir funciones para análisis de datos). Es importante tener en cuenta como funciona R para escribir código más claro, minimizando errores y más eficiente. Las referencias para esta sección son H. Wickham (2014) y Wickham and Grolemund (2017). Referencias "],
["funciones.html", "4.1 Funciones", " 4.1 Funciones “To understand computations in R, two slogans are helpful: * Everything that exists is an object. * Everything that happens is a function call.” — John Chambers Todas las operaciones en R son producto de la llamada a una función, esto incluye operaciones como +, operadores que controlan flujo como for, if y while, e incluso operadores para obtener subconjuntos como [ ] y $. a &lt;- 3 b &lt;- 4 `+`(a, b) #&gt; [1] 7 for (i in 1:2) print(i) #&gt; [1] 1 #&gt; [1] 2 `for`(i, 1:2, print(i)) #&gt; [1] 1 #&gt; [1] 2 Para escribir código eficiente y fácil de leer es importante saber esvribir funciones, se dice que si hiciste copy-paste de una sección de tu código 3 o más veces es momento de escribir una función. Escribimos una función para calcular un promedio ponderado: wtd_mean &lt;- function(x, wt = rep(1, length(x))) { sum(x * wt) / sum(wt) } Y se usa: wtd_mean(1:10) #&gt; [1] 5.5 wtd_mean(1:10, 10:1) #&gt; [1] 4 Escribe una función que reciba un vector y devuelva el mismo vector reescalado al rango 0 a 1. * Comienza escribirendo el código para un caso paricular, por ejemplo, empieza reescalando el vector . Tip: la función range() devuelve el rango de un vector. * Aplica tu función a las columnas a a d del data.frame df df &lt;- data.frame(ind = 1:10, a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10)). Estructura de una función Las funciones de R tienen tres partes: El cuerpo: el código dentro de la función body(wtd_mean) #&gt; { #&gt; sum(x * wt)/sum(wt) #&gt; } Los formales: la lista de argumentos que controlan como puedes llamar a la función, formals(wtd_mean) #&gt; $x #&gt; #&gt; #&gt; $wt #&gt; rep(1, length(x)) El ambiente: el mapeo de la ubicación de las variables de la función. library(ggplot2) environment(wtd_mean) #&gt; &lt;environment: R_GlobalEnv&gt; environment(ggplot) #&gt; &lt;environment: namespace:ggplot2&gt; Veamos mas ejemplos, ¿qué regresan las siguientes funciones? # 1 x &lt;- 5 f &lt;- function(){ y &lt;- 10 c(x = x, y = y) } rm(x, f) # 2 x &lt;- 5 g &lt;- function(){ x &lt;- 20 y &lt;- 10 c(x = x, y = y) } rm(x, g) # 3 x &lt;- 5 h &lt;- function(){ y &lt;- 10 i &lt;- function(){ z &lt;- 20 c(x = x, y = y, z = z) } i() } # 4 ¿qué ocurre si la corremos por segunda vez? j &lt;- function(){ if (!exists(&quot;a&quot;)){ a &lt;- 5 } else{ a &lt;- a + 1 } print(a) } x &lt;- 0 y &lt;- 10 # 5 ¿qué regresa k()? ¿y k()()? k &lt;- function(){ x &lt;- 1 function(){ y &lt;- 2 x + y } } Las reglas de búsqueda determinan como se busca el valor de una variable libre en una función. A nivel lenguaje R usa lexical scoping, una alternativa es dynamic scoping. En R (lexical scoping) los valores de los símbolos se basan en como se anidan las funciones cuando fueron creadas y no en como son llamadas. Esto es, en R no importa como son las llamadas a una función para saber como se va a buscar el valor de una variable. f &lt;- function(x) { x + y } f(2) #&gt; Error in f(2): object &#39;y&#39; not found Si creamos el objeto y. y &lt;- 1 f(2) #&gt; [1] 3 Como consecuencia de las reglas de búsqueda de R, todos los objetos deben ser guardados en memoria y, si uno no es cuidadoso se pueden cometer errores facilmente. y &lt;- 100 f(2) #&gt; [1] 102 Observaciones del uso de funciones Cuando llamamos a una función podemos especificar los argumentos en base a posición, nombre completo o nombre parcial: f &lt;- function(abcdef, bcde1, bcde2) { c(a = abcdef, b1 = bcde1, b2 = bcde2) } f(1, 2, 3) #&gt; a b1 b2 #&gt; 1 2 3 f(2, 3, abcdef = 1) #&gt; a b1 b2 #&gt; 1 2 3 # Podemos abreviar el nombre de los argumentos f(2, 3, a = 1) #&gt; a b1 b2 #&gt; 1 2 3 # Siempre y cuando la abreviación no sea ambigua f(1, 3, b = 1) #&gt; Error in f(1, 3, b = 1): argument 3 matches multiple formal arguments Los argumentos de las funciones en R se evaluan conforme se necesitan (lazy evaluation), f &lt;- function(a, b){ a ^ 2 } f(2) #&gt; [1] 4 La función anterior nunca utiliza el argumento b, de tal manera que f(2) no produce ningún error. Funciones con el mismo nombre en distintos paquetes: La función filter (incluída en R base) aplica un filtro lineal a una serie de tiempo de una variable. x &lt;- 1:100 filter(x, rep(1, 3)) #&gt; Time Series: #&gt; Start = 1 #&gt; End = 100 #&gt; Frequency = 1 #&gt; [1] NA 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 #&gt; [18] 54 57 60 63 66 69 72 75 78 81 84 87 90 93 96 99 102 #&gt; [35] 105 108 111 114 117 120 123 126 129 132 135 138 141 144 147 150 153 #&gt; [52] 156 159 162 165 168 171 174 177 180 183 186 189 192 195 198 201 204 #&gt; [69] 207 210 213 216 219 222 225 228 231 234 237 240 243 246 249 252 255 #&gt; [86] 258 261 264 267 270 273 276 279 282 285 288 291 294 297 NA Ahora cargamos dplyr. library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union filter(x, rep(1, 3)) #&gt; Error in UseMethod(&quot;filter_&quot;): no applicable method for &#39;filter_&#39; applied to an object of class &quot;c(&#39;integer&#39;, &#39;numeric&#39;)&quot; El problema es un conflicto en la función a llamar, nosotros requerimos usar filter de stats y no la función filter de dplyr. R utiliza por default la función que pertenece al último paquete que se cargó. search() #&gt; [1] &quot;.GlobalEnv&quot; &quot;package:dplyr&quot; &quot;package:ggplot2&quot; #&gt; [4] &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; #&gt; [7] &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; #&gt; [10] &quot;Autoloads&quot; &quot;package:base&quot; Una opción es especificar el paquete en la llamada de la función: stats::filter(x, rep(1, 3)) #&gt; Time Series: #&gt; Start = 1 #&gt; End = 100 #&gt; Frequency = 1 #&gt; [1] NA 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 #&gt; [18] 54 57 60 63 66 69 72 75 78 81 84 87 90 93 96 99 102 #&gt; [35] 105 108 111 114 117 120 123 126 129 132 135 138 141 144 147 150 153 #&gt; [52] 156 159 162 165 168 171 174 177 180 183 186 189 192 195 198 201 204 #&gt; [69] 207 210 213 216 219 222 225 228 231 234 237 240 243 246 249 252 255 #&gt; [86] 258 261 264 267 270 273 276 279 282 285 288 291 294 297 NA Una alternativa es el paquete conflicted que alerta cuando hay conflictos y tiene funciones para especificar a que paquete se desea dar preferencia por default en una sesión. "],
["vectores.html", "4.2 Vectores", " 4.2 Vectores En R se puede trabajar con distintas estructuras de datos, algunas son de una sola dimensión y otras permiten más, como indica el diagrama de abajo: Hasta ahora nos hemos centrado en trabajar con data.frames, y hemos usado vectores atómicos sin profundizar, en esta sección se explican características de los vectores, y veremos que son la base de los data.frames. En R hay dos tipos de vectores, esto es, estructuras de datos de una sola dimensión: los vectores atómicos y las listas. Los vectores atómicos pueden ser de 6 tipos: lógico, entero, double, caracter, complejo y raw. Los dos últimos son poco comunes. Vector atómico de tipo lógico: a &lt;- c(TRUE, FALSE, FALSE) a #&gt; [1] TRUE FALSE FALSE Numérico (double): b &lt;- c(5, 2, 4.1, 7, 9.2) b #&gt; [1] 5.0 2.0 4.1 7.0 9.2 b[1] #&gt; [1] 5 b[2] #&gt; [1] 2 b[2:4] #&gt; [1] 2.0 4.1 7.0 Las operaciones básicas con vectores atómicos son componente a componente: c &lt;- b + 10 c #&gt; [1] 15.0 12.0 14.1 17.0 19.2 d &lt;- sqrt(b) d #&gt; [1] 2.236068 1.414214 2.024846 2.645751 3.033150 b + d #&gt; [1] 7.236068 3.414214 6.124846 9.645751 12.233150 10 * b #&gt; [1] 50 20 41 70 92 b * d #&gt; [1] 11.180340 2.828427 8.301867 18.520259 27.904982 Y podemos crear secuencias como sigue: e &lt;- 1:10 e #&gt; [1] 1 2 3 4 5 6 7 8 9 10 f &lt;- seq(0, 1, 0.25) f #&gt; [1] 0.00 0.25 0.50 0.75 1.00 Para calcular características de vectores atómicos usamos funciones: # media del vector mean(b) #&gt; [1] 5.46 # suma de sus componentes sum(b) #&gt; [1] 27.3 # longitud del vector length(b) #&gt; [1] 5 Y ejemplo de vector atómico de tipo caracter y funciones: frutas &lt;- c(&#39;manzana&#39;, &#39;manzana&#39;, &#39;pera&#39;, &#39;plátano&#39;, &#39;fresa&#39;, &quot;kiwi&quot;) frutas #&gt; [1] &quot;manzana&quot; &quot;manzana&quot; &quot;pera&quot; &quot;plátano&quot; &quot;fresa&quot; &quot;kiwi&quot; grep(&quot;a&quot;, frutas) #&gt; [1] 1 2 3 4 5 gsub(&quot;a&quot;, &quot;x&quot;, frutas) #&gt; [1] &quot;mxnzxnx&quot; &quot;mxnzxnx&quot; &quot;perx&quot; &quot;plátxno&quot; &quot;fresx&quot; &quot;kiwi&quot; Las listas, a diferencia de los vectores atómicos, puden contener otras listas. Las listas son muy flexibles pues pueden almacenar objetos de cualquier tipo. x &lt;- list(1:3, &quot;Mila&quot;, c(TRUE, FALSE, FALSE), c(2, 5, 3.2)) str(x) #&gt; List of 4 #&gt; $ : int [1:3] 1 2 3 #&gt; $ : chr &quot;Mila&quot; #&gt; $ : logi [1:3] TRUE FALSE FALSE #&gt; $ : num [1:3] 2 5 3.2 Las listas son vectores recursivos debido a que pueden almacenar otras listas. y &lt;- list(list(list(list()))) str(y) #&gt; List of 1 #&gt; $ :List of 1 #&gt; ..$ :List of 1 #&gt; .. ..$ : list() Para construir subconjuntos a partir de listas usamo [] y [[]]. En el primer caso siempre obtenemos como resultado una lista: x_1 &lt;- x[1] x_1 #&gt; [[1]] #&gt; [1] 1 2 3 str(x_1) #&gt; List of 1 #&gt; $ : int [1:3] 1 2 3 Y en el caso de [[]] extraemos un componente de la lista, eliminando un nivel de la jerarquía de la lista. x_2 &lt;- x[[1]] x_2 #&gt; [1] 1 2 3 str(x_2) #&gt; int [1:3] 1 2 3 ¿Cómo se comparan y, y[1] y y[[1]]? Propiedades Todos los vectores (atómicos y listas) tienen las propiedades tipo y longitud, la función typeof() se usa para determinar el tipo, ======= Todos los vectores tienen las propiedades tipo y longitud, la función typeof() se usa para determinar el tipo, &gt;&gt;&gt;&gt;&gt;&gt;&gt; 27ef834b49e0e8bc1097a6234d308d5406dca20c typeof(a) #&gt; [1] &quot;logical&quot; typeof(b) #&gt; [1] &quot;double&quot; typeof(frutas) #&gt; [1] &quot;character&quot; typeof(x) #&gt; [1] &quot;list&quot; y length() la longitud: length(a) #&gt; [1] 3 length(frutas) #&gt; [1] 6 length(x) #&gt; [1] 4 length(y) #&gt; [1] 1 La flexibilidad de las listas las convierte en estructuras muy útiles y muy comunes, muchas funciones regresan resultados en forma de lista. Incluso podemos ver que un data.frame es una lista de vectores, donde todos los vectores son de la misma longitud. Adicionalmente, los vectores pueden tener atributo de nombres, que puede usarse para indexar. names(b) &lt;- c(&quot;momo&quot;, &quot;mila&quot;, &quot;duna&quot;, &quot;milu&quot;, &quot;moka&quot;) b #&gt; momo mila duna milu moka #&gt; 5.0 2.0 4.1 7.0 9.2 b[&quot;moka&quot;] #&gt; moka #&gt; 9.2 names(x) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) x #&gt; $a #&gt; [1] 1 2 3 #&gt; #&gt; $b #&gt; [1] &quot;Mila&quot; #&gt; #&gt; $c #&gt; [1] TRUE FALSE FALSE #&gt; #&gt; $d #&gt; [1] 2.0 5.0 3.2 x$a #&gt; [1] 1 2 3 x[[&quot;c&quot;]] #&gt; [1] TRUE FALSE FALSE "],
["iteracion.html", "4.3 Iteración", " 4.3 Iteración En analisis de datos es común implementar rutinas iterativas, esto es, cuando debemos aplicar los mismos pasos a distintas entradas. medias &lt;- numeric() for (i in 1:5){ medias[i] &lt;- wtd_mean(df[, i]) } medias #&gt; [1] 5.50000000 -0.18887415 -0.07711494 -0.38013438 0.12434325 Recordando la limpieza de datos de la clase pasada el vector paths contenía la ruta a distintos archivos csv. Crea la tabla de datos final usando un ciclo for. paths &lt;- dir(&quot;data/specdata&quot;, pattern = &quot;\\\\.csv$&quot;, full.names = TRUE) Es muy común tener que iterar sobre un vector, modificando cada entrada y guardando los resultados en una nueva estructura, es por ello que hay funciones para realizar esto en R de manera más clara. Por ejemplo, R base existen lapply(), apply(), sapply(). Por su parte, el paquete purrr del tidyverse provee una familia de funciones para esta misma función. map() devuelve una lista. map_lgl() devuelve un vector lógico. map_int() devuelve un vector entero. map_dbl() devuelve un vector double. map_chr() devuelve un vector caracter. Todas las funciones reciben un vector, aplican una función a cada parte y regresan un nuevo vector de la misma longitud que el vector entrada. library(purrr) names(paths) &lt;- basename(paths) specdata_us_vec &lt;- map(paths, ~readr::read_csv(., col_types = &quot;Tddi&quot;), .id = &quot;filename&quot;) specdata_us_vec[[10]] #&gt; # A tibble: 1,096 x 4 #&gt; Date sulfate nitrate ID #&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2002-01-01 00:00:00 NA NA 10 #&gt; 2 2002-01-02 00:00:00 NA NA 10 #&gt; 3 2002-01-03 00:00:00 NA NA 10 #&gt; 4 2002-01-04 00:00:00 NA NA 10 #&gt; 5 2002-01-05 00:00:00 NA NA 10 #&gt; 6 2002-01-06 00:00:00 NA NA 10 #&gt; 7 2002-01-07 00:00:00 NA NA 10 #&gt; 8 2002-01-08 00:00:00 NA NA 10 #&gt; 9 2002-01-09 00:00:00 NA NA 10 #&gt; 10 2002-01-10 00:00:00 NA NA 10 #&gt; # ... with 1,086 more rows class(specdata_us_vec) #&gt; [1] &quot;list&quot; En este caso es más apropiado usar map_df specdata_us &lt;- map_df(paths, ~readr::read_csv(., col_types = &quot;Tddi&quot;), .id = &quot;filename&quot;) Utiliza map_*** para crear un vector con la media de nitrato de cada estación de monitoreo, itera sobre el vector specdata_us_vec. "],
["rendimiento-en-r.html", "4.4 Rendimiento en R", " 4.4 Rendimiento en R “We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified.” -Donald Knuth Diseña primero, luego optimiza. La optimización del código es un proceso iterativo: 1. Encuentra el cuello de botella más importante. 2. Intenta eliminarlo (no siempre se puede). 3. Repie hasta que tu código sea lo suficientemente rápido. Diagnosticar Una vez que tienes código que se puede leer y funciona, el perfilamiento (profiling) del código es un método sistemático que nos permite conocer cuanto tiempo se esta usando en diferentes partes del programa. Comenzaremos con la función system.time (no es perfilamiento aún), esta calcula el tiempo en segundos que toma ejecutar una expresión (si hay un error, regresa el tiempo hasta que ocurre el error): batting &lt;- read.csv(&quot;data/batting.csv&quot;) system.time(lm(R ~ AB + teamID, batting)) #&gt; user system elapsed #&gt; 2.416 0.080 2.496 user time: Tiempo usado por el CPU(s) para evaluar esta expresión, tiempo que experimenta la computadora. elapsed time: tiempo en el reloj, tiempo que experimenta la persona. Notemos que el tiempo de usuario (user) puede ser mayor al tiempo transcurrido (elapsed), system.time(readLines(&quot;http://www.jhsph.edu&quot;)) #&gt; user system elapsed #&gt; 0.020 0.000 0.251 o al revés: library(parallel) system.time(mclapply(2000:2007, function(x){ sub &lt;- subset(batting, yearID == x) lm(R ~ AB + playerID, sub) }, mc.cores = 7)) #&gt; user system elapsed #&gt; 7.066 0.504 4.862 Comparemos la velocidad de dplyr con funciones que se encuentran en R estándar y plyr. # dplyr dplyr_st &lt;- system.time({ batting %&gt;% group_by(playerID) %&gt;% summarise(total = sum(R, na.rm = TRUE), n = n()) %&gt;% dplyr::arrange(desc(total)) }) # plyr plyr_st &lt;- system.time({ batting %&gt;% plyr::ddply(&quot;playerID&quot;, plyr::summarise, total = sum(R, na.rm = TRUE), n = length(R)) %&gt;% plyr::arrange(-total) }) # estándar lento est_l_st &lt;- system.time({ players &lt;- unique(batting$playerID) n_players &lt;- length(players) total &lt;- rep(NA, n_players) n &lt;- rep(NA, n_players) for(i in 1:n_players){ sub_batting &lt;- batting[batting$playerID == players[i], ] total[i] &lt;- sum(sub_batting$R, na.rm = TRUE) n[i] &lt;- nrow(sub_batting) } batting_2 &lt;- data.frame(playerID = players, total = total, n = n) batting_2[order(batting_2$total, decreasing = TRUE), ] }) # estándar rápido est_r_st &lt;- system.time({ batting_2 &lt;- aggregate(. ~ playerID, data = batting[, c(&quot;playerID&quot;, &quot;R&quot;)], sum) batting_ord &lt;- batting_2[order(batting_2$R, decreasing = TRUE), ] }) dplyr_st #&gt; user system elapsed #&gt; 0.038 0.000 0.038 plyr_st #&gt; user system elapsed #&gt; 6.696 0.008 6.704 est_l_st #&gt; user system elapsed #&gt; 236.097 3.111 239.214 est_r_st #&gt; user system elapsed #&gt; 0.203 0.000 0.203 La función system.time supone que sabes donde buscar, es decir, que expresiones debes evaluar, una función que puede ser más útil cuando uno desconoce cuál es la función que alenta un programa es profvis() del paquete con el mismo nombre. library(profvis) batting_recent &lt;- filter(batting, yearID &gt; 2006) profvis({ players &lt;- unique(batting_recent$playerID) n_players &lt;- length(players) total &lt;- rep(NA, n_players) n &lt;- rep(NA, n_players) for(i in 1:n_players){ sub_batting &lt;- batting_recent[batting_recent$playerID == players[i], ] total[i] &lt;- sum(sub_batting$R, na.rm = TRUE) n[i] &lt;- nrow(sub_batting) } batting_2 &lt;- data.frame(playerID = players, total = total, n = n) batting_2[order(batting_2$total, decreasing = TRUE), ] }) profvis() utiliza a su vez la función Rprof() de R base, este es un perfilador de muestreo que registra cambios en la pila de funciones, funciona tomando muestras a intervalos regulares y tabula cuánto tiempo se lleva en cada función. Estrategias para mejorar desempeño Algunas estrategias para mejorar desempeño: Utilizar apropiadamente funciones de R, o funciones de paquetes que muchas veces están mejor escritas de lo que nosotros podríamos hacer. Hacer lo menos posible. Usar funciones vectorizadas en R (casi siempre). No hacer crecer objetos (es preferible definir su tamaño antes de operar en ellos). Paralelizar. La más simple y muchas veces la más barata: conseguie una máquina más grande (por ejemplo Amazon web services). A continuación revisamos y ejemplificamos los puntos anteriores, los ejemplos de código se tomaron del taller EfficientR, impartido por Martin Morgan. Utilizar apropiadamente funciones de R Si el cuello de botella es la función de un paquete vale la pena buscar alternativas, CRAN task views es un buen lugar para buscar. Hacer lo menos posible Utiliza funciones más específicas, por ejemplo: * rowSums(), colSums(), rowMeans() y colMeans() son más rápidas que las invocaciones equivalentes de apply(). Si quieres checar si un vector contiene un valor any(x == 10) es más veloz que 10 %in% x, esto es porque examinar igualdad es más sencillo que examinar inclusión en un conjunto. Este conocimiento requiere que conozcas alternativas, para ello debes construir tu vocabulario, puedes comenzar por lo básico e ir incrementando conforme lees código. Otro caso es cuando las funciones son más rápidas cunado les das más información del problema, por ejemplo: read.csv(), especificar las clases de las columnas con colClasses. factor() especifica los niveles con el argumento levels. Usar funciones vectorizadas en R Es común escuchar que en R vectorizar es conveniente, el enfoque vectorizado va más allá que evitar ciclos for: Pensar en objetos, en lugar de enfocarse en las compoentes de un vector, se piensa únicamente en el vector completo. Los ciclos en las funciones vectorizadas de R están escritos en C, lo que los hace más veloces. Las funciones vectorizadas programadas en R pueden mejorar la interfaz de una función pero no necesariamente mejorar el desempeño. Usar vectorización para desempeño implica encontrar funciones de R implementadas en C. Al igual que en el punto anterior, vectorizar requiere encontrar las funciones apropiadas, algunos ejemplos incluyen: _rowSums(), colSums(), rowMeans() y colMeans(). Ejemplo: iteración (for, lapply(), sapply(), vapply(), mapply(), apply(), …) en un vector de n elementos llama a R base n veces compute_pi0 &lt;- function(m) { s = 0 sign = 1 for (n in 0:m) { s = s + sign / (2 * n + 1) sign = -sign } 4 * s } compute_pi1 &lt;- function(m) { even &lt;- seq(0, m, by = 2) odd &lt;- seq(1, m, by = 2) s &lt;- sum(1 / (2 * even + 1)) - sum(1 / (2 * odd + 1)) 4 * s } m &lt;- 1e6 Utilizamos el paquete microbenchmark para medir tiempos varias veces. library(microbenchmark) m &lt;- 1e4 result &lt;- microbenchmark( compute_pi0(m), compute_pi0(m * 10), compute_pi0(m * 100), compute_pi1(m), compute_pi1(m * 10), compute_pi1(m * 100), compute_pi1(m * 1000), times = 20 ) result #&gt; Unit: microseconds #&gt; expr min lq mean median #&gt; compute_pi0(m) 1030.826 1041.4395 5821.1386 1046.8370 #&gt; compute_pi0(m * 10) 10337.217 10371.7435 11422.5764 10422.3085 #&gt; compute_pi0(m * 100) 103373.450 103652.0030 112285.1709 103857.6760 #&gt; compute_pi1(m) 303.314 317.9275 393.9993 388.8885 #&gt; compute_pi1(m * 10) 2387.178 2438.3590 2623.1483 2471.3885 #&gt; compute_pi1(m * 100) 23659.275 23705.6570 28381.9494 28693.2215 #&gt; compute_pi1(m * 1000) 287631.964 360142.8845 397126.0649 405069.8890 #&gt; uq max neval #&gt; 1055.1640 95878.218 20 #&gt; 10534.1270 17144.650 20 #&gt; 109468.8605 171089.494 20 #&gt; 410.4705 573.719 20 #&gt; 2534.5145 3349.169 20 #&gt; 32171.4950 38477.939 20 #&gt; 443528.0625 475595.296 20 Evitar copias Otro aspecto importante es que generalmente conviene asignar objetos en lugar de hacerlos crecer (es más eficiente asignar toda la memoria necesaria antes del cálculo que asignarla sucesivamente). Esto es porque cuando se usan instrucciones para crear un objeto más grande (e.g. append(), cbind(), c(), rbind()) R debe primero asignar espacio a un nuevo objeto y luego copiar al nuevo lugar. Para leer más sobre esto Burns (2015) es una buena referencia. Ejemplo: crecer un vector puede causar que R copie de manera repetida el vector chico en el nuevo vector, aumentando el tiempo de ejecución. Solución: crear vector de tamaño final y llenarlo con valores. Las funciones como lapply() y map hacen esto de manera automática y son más sencillas que los ciclos for. memory_copy1 &lt;- function(n) { result &lt;- numeric() for (i in seq_len(n)) result &lt;- c(result, 1/i) result } memory_copy2 &lt;- function(n) { result &lt;- numeric() for (i in seq_len(n)) result[i] &lt;- 1 / i result } pre_allocate1 &lt;- function(n) { result &lt;- numeric(n) for (i in seq_len(n)) result[i] &lt;- 1 / i result } pre_allocate2 &lt;- function(n) { vapply(seq_len(n), function(i) 1 / i, numeric(1)) } vectorized &lt;- function(n) { 1 / seq_len(n) } n &lt;- 10000 microbenchmark( memory_copy1(n), memory_copy2(n), pre_allocate1(n), pre_allocate2(n), vectorized(n), times = 10, unit = &quot;relative&quot; ) #&gt; Unit: relative #&gt; expr min lq mean median uq #&gt; memory_copy1(n) 2904.03348 3080.93170 823.239229 3011.18916 2641.833598 #&gt; memory_copy2(n) 46.47161 45.06932 14.049626 46.60760 44.345317 #&gt; pre_allocate1(n) 11.82800 11.02883 4.431313 10.57052 9.656924 #&gt; pre_allocate2(n) 99.33693 95.09617 27.677349 95.01254 94.295753 #&gt; vectorized(n) 1.00000 1.00000 1.000000 1.00000 1.000000 #&gt; max neval #&gt; 144.096023 10 #&gt; 3.707451 10 #&gt; 2.434541 10 #&gt; 5.477067 10 #&gt; 1.000000 10 Un caso común donde se hacen copias sin necesidad es al trabajar con data.frames. Ejemplo: actualizar un data.frame copia el data.frame completo. Solución: operar en vectores y actualiza el data.frame al final. n &lt;- 1e4 df &lt;- data.frame(Index = 1:n, A = seq(10, by = 1, length.out = n)) f1 &lt;- function(df) { ## constants cost1 &lt;- 3 cost2 &lt;- 0.05 cost3 &lt;- 50 ## update data.frame -- copies entire data frame each time! df$S[1] &lt;- cost1 for (j in 2:(n)) df$S[j] &lt;- df$S[j - 1] - cost3 + df$S[j - 1] * cost2 / 12 ## return result df } .f2helper &lt;- function(cost1, cost2, cost3, n) { ## create the result vector separately cost2 &lt;- cost2 / 12 # &#39;hoist&#39; common operations result &lt;- numeric(n) result[1] &lt;- cost1 for (j in 2:(n)) result[j] &lt;- (1 + cost2) * result[j - 1] - cost3 result } f2 &lt;- function(df) { cost1 &lt;- 3 cost2 &lt;- 0.05 cost3 &lt;- 50 ## update the data.frame once df$S &lt;- .f2helper(cost1, cost2, cost3, n) df } microbenchmark( f1(df), f2(df), times = 5, unit = &quot;relative&quot; ) #&gt; Unit: relative #&gt; expr min lq mean median uq max neval #&gt; f1(df) 498.2217 467.7766 173.3658 418.6189 140.8606 70.0349 5 #&gt; f2(df) 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 5 Paralelizar Paralelizar usa varios cores para trabajar de manera simultánea en varias secciones de un problema, no reduce el tiempo computacional pero incrementa el tiempo del usuario pues aprovecha los recursos. Como referencia está [Parallel Computing for Data Science] de Norm Matloff. Referencias "],
["introduccion-a-probabilidad.html", "Sección 5 Introducción a probabilidad", " Sección 5 Introducción a probabilidad “Probabilidad es el lenguaje matemático para cuantificar incertidumbre.” -Wasserman En estas notas hacemos un repaso de conceptos de probabilidad con un enfoque computacional: Terminología de probabilidad: espacio de resultados, eventos, funciones de probabilidad. Interpretación frecuentista de probabilidad. Variables aleatorias: a qué se refieren. Las referencias para esta sección son Pitman (1992), Ross (1998) y Wasserman (2010). Referencias "],
["probabilidad-como-extension-a-proporcion.html", "5.1 Probabilidad como extensión a proporción", " 5.1 Probabilidad como extensión a proporción Espacio de resultados y eventos El espacio de resultados \\(\\Omega\\) es el conjunto de posibles resultados de un experimento aleatorio. A los puntos \\(\\omega \\in \\Omega\\) se les conoce como resultados muestrales, realizaciones o elementos. Ejemplo: Si lanzamos una moneda dos veces entonces el espacio de resultados es: \\[\\Omega = \\{AA, AS, SA, SS \\}\\] Un evento es un subconjunto del espacio muestral, los eventos usualmente se denotan por letras mayúsculas. El evento: que la primer lanzamiento resulte águila es \\[A=\\{AA, AS\\}\\] Eventos equiprobables Históricamente la primera aproximación a la probabilidad ocurrió con apuestas y juegos de azar, y se veía como una extensión de la idea de proporción, o cociente de una parte con respecto a un todo. Por ejemplo, si en la carrera de matemáticas del ITAM hay 300 estudiantes hombres y 700 mujeres, la proporción de hombres es: \\[\\frac{300}{700+300}=0.3\\] Ahora, supongamos que elegimos un estudiante al azar, la probabilidad de elegir una mujer es \\(0.7\\). En el ejemplo hay un supuesto implícito en elegir al azar (o aleatoriamente), en este caso estamos suponiendo que todos los estudiantes tienen la misma probabilidad de ser elegidos, que nos lleva al siguiente concepto: Eventos equiprobables. Si todos los elementos en el espacio de resultados tienen la misma oportunidad de ser elegidos entonces la probabilidad del evento A es el número de resultados en A dividido entre el número total de posibles resultados: \\[P(A)=\\frac{\\#(A)}{\\#(\\Omega)}\\] Por lo que solo hace falta contar. Por ejemplo, la probabilidad de obtener \\(AA\\) si lanzamos una moneda dos veces es \\(1/4 = 0.25\\), y la probabilidad del evento que la primer lanzamiento resulte águila es \\(2/4 = 0.5\\). Lanzamos un dado y anotamos el número de la cara superior, después lanzamos otro dado y anotamos el número de la cara superior. ¿Cuál es el espacio de resultados? ¿Cuál es la probabilidad de que la suma de los números sea 5? ¿Cuál es la probabilidad de que el segundo número sea mayor que el primero? Repite las preguntas anteriores cuando lanzas 2 dados con \\(n\\) caras (\\(n \\ge 4\\)). Ejemplo: combinaciones Un comité de 5 personas será seleccionado de un grupo de 6 hombres y 9 mujeres. Si la selección es aleatoria, ¿cuál es la probabilidad de que el comité este conformado por 3 hombres y 2 mujeres? Hay \\(\\dbinom{15}{5}\\) posibles comités, cada uno tiene la misma posibilidad de ser seleccionado. Por otra parte, hay \\(\\dbinom{6}{3} \\dbinom{9}{2}\\) posibles comités que incluyen 3 hombres y 2 mujeres, por lo tanto, la probabilidad que buscamos es: \\[\\frac{\\dbinom{6}{3} \\dbinom{9}{2}}{\\dbinom{15}{5}} \\] y la función para calcular combinaciones en R es choose(n, r) choose(6, 3) * choose(9, 2) / choose(15, 5) #&gt; [1] 0.24 Los solución a problemas derivados de juegos de azar se complica rápidamente y suele ser necesario conocer técnicas de conteo para resolverlos. Ahora, a pesar de que históricamente el desarrollo de estás técnicas surge de los juegos de azar, la realidad es que los jugadores en realidad estaban pensando en frecuencias relativas: ¿Si apuesto en un juego de dados de manera repetida, terminaré con ganancias o pérdidas? ¿Qué estrategia debo seguir para mejorar mis posibilidades de ganar? Es así que la interpretación frecuentista de la probabilidad estaba considerada desde un inicio. "],
["interpretacion-frecuentista-de-probabilidad.html", "5.2 Interpretación frecuentista de probabilidad", " 5.2 Interpretación frecuentista de probabilidad Ya tenemos una interpretación intuitiva de probabilidad pero nos deja abierta la pregunta de como interpretar probabilidades en aplicaciones. Abordamos ahora la interpretación frecuentista de la probabilidad en la cuál las probabilidades se entienden como una aproximación matemática de frecuencias relativas cuando la frecuencia total tiende a infinito. Una frecuencia relativa es una proporción que mide que tan seguido, o frecuente, ocurre una u otra cosa en una sucesión de observaciones. Pensemos en un experimento que se pueda repetir, por ejemplo, lanzar una moneda, lanzar un dado, el nacimiento de un bebé. Llamaremos ensayo a una repetición del experimento. Ahora, sea A un posible resultado del evento (obtener sol, obtener un 6, el bebé es niña), si A ocurre \\(m\\) veces en \\(n\\) ensayos, entonces la frecuencia relativa de A en \\(n\\) ensayos es \\(m/n\\). Supongamos que lanzamos una moneda 10 veces y obtenemos los siguientes resultados: lanzamientos_10 &lt;- sample(c(&quot;A&quot;, &quot;S&quot;), 10, replace = TRUE) lanzamientos_10 #&gt; [1] &quot;S&quot; &quot;A&quot; &quot;S&quot; &quot;S&quot; &quot;A&quot; &quot;S&quot; &quot;A&quot; &quot;S&quot; &quot;A&quot; &quot;S&quot; Podemos calcular las secuencia de frecuencias relativas de águila: cumsum(lanzamientos_10 == &quot;A&quot;) # suma acumulada de águilas #&gt; [1] 0 1 1 1 2 2 3 3 4 4 cumsum(lanzamientos_10 == &quot;A&quot;) / 1:10 #&gt; [1] 0.000 0.500 0.333 0.250 0.400 0.333 0.429 0.375 0.444 0.400 Una regla general, es que las frecuencias relativas basadas en un número mayor de observaciones son menos fluctuantes comparado con las frecuencias relativas basadas en pocas observaciones. Este fenómeno se conoce como la ley empírica de los promedios (y se formalizó después en las leyes de los grandes números): n &lt;- 1000 data_frame(num_lanzamiento = 1:n, lanzamiento = sample(c(&quot;A&quot;, &quot;S&quot;), n, replace = TRUE)) %&gt;% mutate(frec_rel = cummean(lanzamiento == &quot;A&quot;)) %&gt;% ggplot(aes(x = num_lanzamiento, y = frec_rel)) + geom_hline(yintercept = 0.5, color = &quot;red&quot;, alpha = 0.5) + geom_line(color = &quot;darkgray&quot;) + geom_point(size = 1.0) + labs(y = &quot;frecuencia relativa&quot;, title = &quot;1000 volados&quot;, x = &quot;lanzamiento&quot;) Veamos las frecuencias relativas para 3 series de 1000 lanzamientos. lanzar &lt;- function(n = 1000){ data_frame(num_lanzamiento = 1:n, lanzamiento = sample(c(&quot;A&quot;, &quot;S&quot;), n, replace = TRUE)) %&gt;% mutate(frec_rel = cummean(lanzamiento == &quot;A&quot;)) } head(lanzar()) #&gt; # A tibble: 6 x 3 #&gt; num_lanzamiento lanzamiento frec_rel #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 A 1 #&gt; 2 2 S 0.5 #&gt; 3 3 A 0.667 #&gt; 4 4 A 0.75 #&gt; 5 5 S 0.6 #&gt; 6 6 S 0.5 set.seed(31287931) # usamos la función map_df del paquete purrr map_df(1:3, ~lanzar(), .id = &quot;serie&quot;) %&gt;% ggplot(aes(x = log(num_lanzamiento), y = frec_rel, color = as.character(serie))) + geom_hline(yintercept = 0.5, color = &quot;darkgray&quot;) + geom_line() + scale_x_continuous(&quot;lanzamiento&quot;, labels = exp, breaks = log(sapply(0:10, function(i) 2 ^ i))) + labs(color = &quot;serie&quot;, y = &quot;frecuencia relativa&quot;, title = &quot;1000 volados&quot;) En la interpretación frecuentista, la probabilidad de un evento \\(A\\) es la estimación de la frecuencia relativa de \\(A\\) cuando el número de ensayos tiende a infinito. Si denotemos la proporción de veces que ocurre \\(A\\) en \\(n\\) ensayos por \\(P_n(A)\\), se espera que \\(P_n(A)\\) sea cercana a la probabilidad \\(P(A)\\) si \\(n\\) es grande: \\[P_n(A) \\approx P(A)\\] Veamos un ejemplo de calculo de una probabilidad como frecuencia relativa; el objetivo es entender cómo la interpretación frecuentista nos da el nivel de detalle correcto cuando suponemos resultados equiprobables. Ejemplo: Lanzamiento de dos monedas Supongamos que lanzamos dos monedas de manera simultánea. ¿Cuál es la probabilidad de que las dos monedas sean águila? Las dos son águila o no, así que la posibilidad es 1/2. Si definimos el resultado como el número de caras que se leen en las monedas, puede haber 0, 1 o 2. Si suponemos que estos tres resultados son igualmente probables, entonces la posibilidad es 1/3. A pesar de que las monedas son similares supongamos que se pueden distinguir, llamémoslas moneda 1 y moneda 2. Ahora tenemos cuatro posibles resultados: AA, AS, SA, SS, (la primer letra corresponde a la cara observada en la moneda 1 y la segunda en la moneda 2). Si estos 4 resultados son igualmente probables entonces el evento AA tiene posibilidad de 1/4. ¿Cuál es la respuesta correcta? En cuanto a teoría formal todas son correctas, cada escenario tiene supuestos de resultados equiprobables claramente enunciados y en base a éstos determina una probabilidad de manera correcta; sin embargo, los supuestos son diferentes y por tanto también las conclusiones. Únicamente una de las soluciones puede ser consistente con la interpretación frecuentista, ¿cuál es? La primer respuesta es incorrecta pues supone probabilidad cero para el evento águila y sol. La solución dos, por otra parte, no es fácil de desacreditar, así que realicemos el experimento para encontrar la respuesta: n &lt;- 10000 moneda_1 &lt;- sample(c(&quot;A&quot;, &quot;S&quot;), n, replace = TRUE) moneda_2 &lt;- sample(c(&quot;A&quot;, &quot;S&quot;), n, replace = TRUE) sum(moneda_1 == moneda_2 &amp; moneda_1 ==&quot;A&quot;) / n #&gt; [1] 0.257 La respuesta 3 es la correcta, y lo que vemos es que incluso cuando el supuesto de igualmente probables es apropiado a un cierto nivel de descripción determinado, este nivel no es algo que se pueda juzgar usando únicamente matemáticas, sino que se debe juzgar usando una interpretación de la probabilidad, como frecuencias relativas en ensayos. Más aún, hay ejemplos donde las monedas no son justas, o el sexo de un bebé recién nacido, donde el supuesto de equiprobabilidad no es adecuado. "],
["simulacion-para-el-calculo-de-probabilidades.html", "5.3 Simulación para el cálculo de probabilidades", " 5.3 Simulación para el cálculo de probabilidades En el ejemplo anterior vimos que puede ser sencillo usar simulación para calcular probabilidades, pues usando la interpretación de frecuencia relativa simplemente hace falta simular el experimento y contar los casos favorables entre el total de casos. Simulación para el cálculo de probabilidades: Definir el espacio de resultados. Describir el mecanismo que genera los resultados, esto incluye entender los pasos que involucran azar y los que no. Replicar el experimento con código, siguiendo el conocimiento elicitado en 1 y 2. Repetir el paso 3 \\(n\\) veces y calcular la frecuencia relativa de éxitos, estimando así la probabilidad. Para el paso 2, en R suelen ser de utilidad las funciones runif y sample(), revisa la ayuda de estas funciones. Ejemplo: comité Un comité de 5 personas será seleccionado de un grupo de 6 hombres y 9 mujeres. Si la selección es aleatoria, ¿cuál es la probabilidad de que el comité este conformado por 3 hombres y 2 mujeres? El espacio de resultados es \\(\\Omega = \\{M_1M_2M_3M_4M_5, M_2M_3M_4M_5M_6,... H_1,H_2H_3H_4H_5,H_2H_3H_4H_5H_6\\}\\). Se seleccionan 5 integrantes al azar del conjunto de hombres y mujeres, es claro que cada persona solo puee estar una vez. candidatos &lt;- c(paste(&quot;M&quot;, 1:9, sep = &quot;_&quot;), paste(&quot;H&quot;, 1:6, sep = &quot;_&quot;)) sample(candidatos, 5, replace = FALSE) #&gt; [1] &quot;H_1&quot; &quot;M_7&quot; &quot;H_5&quot; &quot;M_4&quot; &quot;M_3&quot; comite &lt;- function(){ candidatos &lt;- c(paste(&quot;M&quot;, 1:9, sep = &quot;_&quot;), paste(&quot;H&quot;, 1:6, sep = &quot;_&quot;)) comite &lt;- sample(candidatos, 5, replace = FALSE) n_mujeres &lt;- sum(substr(comite, 1, 1) == &quot;M&quot;) n_mujeres == 2 } rerun(1000, comite()) %&gt;% flatten_dbl() %&gt;% mean() #&gt; [1] 0.219 Ejemplo: La ruina del jugador Un jugador tiene $100, y va a apostar en un juego donde la probabilidad de ganar es p = 0.47 (e.g. una ruleta 18/38), si gana recibe el doble de lo que arriesgó, si no gana pierde todo lo que apostó. Cada vez que juega puede apostar cualquier cantidad siempre y cuando aún cuente con dinero. El jugador dejará de jugar cuando su capital sea $0 o cuando gane $200. El jugador busca una estrategia que le ayude a aumentar su probabilidad de ganar y te pregunta: ¿Cuál es la probabilidad de ganar si apuesto en incrementos de $5 cada vez que apuesto? Siguiendo los pasos enunciados: El espacio de resultados es \\(\\Omega = \\{GGGGGGGGGGGGGGGGGGGG, PGGGGGGGGGGGGGGGGGGGGGG, GPGGGGGGGGGGGGGGGGGGGGG, ...\\}\\). El jugador juega mientras tenga capital y este sea menor a $200, el monto de la apuesta está fijo en $5, no importa el capital en cada momento. La componente aleatoria involucra si gana cada uno de los juegos y esto ocurre con probabilidad 0.47. apostar &lt;- function(dinero = 100, apuesta = 5, tope = 200){ while(0 &lt; dinero &amp; dinero &lt; tope){ if(sample(1:38, 1) &lt;= 18){ dinero &lt;- dinero + apuesta } else{ dinero &lt;- dinero - apuesta } } dinero &gt; 0 } n_juegos &lt;- 5000 juegos &lt;- rerun(n_juegos, apostar()) %&gt;% flatten_dbl() mean(juegos) #&gt; [1] 0.114 # incrementos de 50? juegos &lt;- rerun(n_juegos, apostar(apuesta = 50)) %&gt;% flatten_dbl() mean(juegos) #&gt; [1] 0.443 La solución analítica la pueden leer en este documento de caminatas aleatorias: p = 0.47 1 - (1 - (p / (1 - p)) ^ (100 / 5)) / (1 - (p / (1 - p)) ^ (200 / 5)) # apostando de 5 en 5 #&gt; [1] 0.083 1 - (1 - (p / (1 - p)) ^ (100 / 50)) / (1 - (p / (1 - p)) ^ (200 / 50)) # apostando de 50 en 50 #&gt; [1] 0.44 Cumpleaños. ¿Cuántas personas debe haber en un salón para que la probabilidad de encontrar 2 con el mismo cumpleaños sea 0.5? Supuestos: Mismo cumpleaños implica mismo día y mes. No hay años bisiestos. La probabilidad de que alguien nazca un día dado es la misma para todos los días del año. Chabelo (Monty Hall) Supongamos que estamos jugando las catafixias de Chabelo, en este juego hay 3 catafixias: 2 de ellas están vacías y una tiene un premio: El juego comienza cuando escoges una catafixia. A continuación Chabelo abre una catafixia vacía de las dos catafixias restantes. Tu eliges si te mantienes con tu catafixia o cambias a la otra que continúa cerrada. Chabelo abre tu segunda elección de catafixia y se revela si ganaste. ¿Cuál es la probabilidad de que ganes si cambias de catafixia? Urna: 10 personas (con nombres distintos) escriben sus nombres y los ponen en una urna, después seleccionan un nombre (al azar). Sea A el evento en el que ninguna persona selecciona su nombre, ¿Cuál es la probabilidad del evento A? Supongamos que hay 3 personas con el mismo nombre, ¿Cómo calcularías la probabilidad del evento A en este nuevo experimento? El señor J. tiene 2 cachorros, el mayor es hembra. ¿Cuál es la probabilidad de que los dos sean hembra? La señora K. tiene 2 cachorros, al menos uno es macho. ¿Cuál es la probabilidad de que los dos sean macho? Podemos generalizar las definiciones de equiprobable al caso continuo, como ejemplo supongamos que se lanza un dado a un table cuadrandgular de lado 2, ¿cuál es la probabilidad de que el dado caiga en el círculo de radio 1 inscrito en un cuadrado de lado 2? tablero &lt;- ggplot() + ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 1)) + geom_rect(aes(xmin = -1, xmax = 1, ymin = -1, ymax = 1), fill = &quot;white&quot;, color = &quot;black&quot;, alpha = 0.5) + coord_equal() ggsave(&quot;imagenes/tablero.png&quot;, tablero, width = 3, height = 3) knitr::include_graphics(&quot;imagenes/tablero.png&quot;) En este caso usamos áreas relativas para calcular la probabilidad: denotemos C al evento tal que el dardo cae en el círculo, entonces: \\[P(B) = \\frac{Área(B)}{Área(\\Omega)}\\] ¿Y simulando? circunferencia &lt;- function(){ x &lt;- runif(1) * sample(c(-1, 1), 1) y &lt;- runif(1) * sample(c(-1, 1), 1) sqrt(x ^ 2 + y ^ 2) &lt; 1 } rerun(10000, circunferencia()) %&gt;% flatten_dbl() %&gt;% mean() #&gt; [1] 0.784 dardos &lt;- data_frame(x = runif(1000, -1, 1), y = runif(1000, -1, 1), en_circulo = sqrt(x ^ 2 + y ^ 2) &lt; 1) tablero_dardos &lt;- tablero + geom_point(data = dardos, aes(x, y, color = en_circulo), alpha = 0.5, show.legend = FALSE) ggsave(&quot;imagenes/tablero_dardos.png&quot;, tablero_dardos, width = 3, height = 3) knitr::include_graphics(&quot;imagenes/tablero_dardos.png&quot;) Ahora, en el ejemplo de los dardos es más realista pensar que la probabilidad de que el dardo caiga en un segmento de la zona central no es la misma a que caiga en un segmento de igual área en las orillas. tablero_zonas &lt;- tablero + geom_rect(aes(xmin = -1, xmax = -0.8, ymin = -1, ymax = -0.8), fill = &quot;red&quot;, alpha = 0.5) + geom_rect(aes(xmin = -.1, xmax = 0.1, ymin = -0.1, ymax = 0.1), fill = &quot;red&quot;, alpha = 0.5) ggsave(&quot;imagenes/tablero_zonas.png&quot;, tablero_zonas, width = 3, height = 3) knitr::include_graphics(&quot;imagenes/tablero_zonas.png&quot;) La definición de probabilidad como área relativa no se puede usar en estos casos, sin embargo, el enfoque de simulación se continúa manteniendo. Comencemos con el caso del dardo univariado. unif &lt;- ggplot() + geom_rect(aes(xmin = 0.3, xmax = 0.6, ymin = 0, ymax = 1), fill = &quot;red&quot;, alpha = 0.5) + xlim(0, 1) unif En este caso de área relativa, calculamos la probabilidad cómo el área sombreada \\[P([a, b]) = \\frac{b-a}{1} = \\int_a^b 1dx\\] Ahora, si el dardo cae en ciertas zonas con mayor probabilidad: ggplot(data_frame(x = c(0 , 1)), aes(x)) + stat_function(fun = dbeta, args = list(shape1 = 5, shape2 = 2)) + geom_rect(data = NULL, aes(xmin = 0, xmax = 1, ymin = 0, ymax = 1), fill = &quot;red&quot;, alpha = 0.2) \\[P([a,b])=\\int_a^bf(x)dx\\] Y lo podemos calcular con simulación, por ejemplo la probabilidad de x en [0.2, 0.5]: curva &lt;- function(){ x &lt;- runif(1) y &lt;- runif(1) * 2.5 while(dbeta(x, 5, 2) &lt; y){ x &lt;- runif(1) y &lt;- runif(1) * 2.5 } x } sims_x &lt;- rerun(5000, curva()) %&gt;% flatten_dbl() mean(sims_x &gt; 0.2 &amp; sims_x &lt; 0.5) #&gt; [1] 0.104 data_frame(x = runif(1000), y = runif(1000) * 2.5, dentro = dbeta(x, 5, 2) &gt; y, en_int = dentro * (x &gt; 0.2 &amp; x &lt; 0.5), cat = case_when(!dentro ~ &quot;a&quot;, dentro &amp; en_int ~ &quot;b&quot;, TRUE ~ &quot;c&quot;)) %&gt;% ggplot()+ stat_function(fun = dbeta, args = list(shape1 = 5, shape2 = 2)) + geom_point(aes(x, y, color = cat), alpha = 0.5, show.legend = FALSE) En el caso discreto. Supongamos que el proceso de selección del comité tiene sesgo, las mujeres se seleccionan con mayor probabilidad que los hombres comite &lt;- function(){ candidatos &lt;- c(paste(&quot;M&quot;, 1:9, sep = &quot;_&quot;), paste(&quot;H&quot;, 1:6, sep = &quot;_&quot;)) comite &lt;- sample(candidatos, 5, replace = FALSE, prob = c(rep(2, 9), rep(1, 6))) n_mujeres &lt;- sum(substr(comite, 1, 1) == &quot;M&quot;) n_mujeres == 2 } rerun(1000, comite()) %&gt;% flatten_dbl() %&gt;% mean() #&gt; [1] 0.094 Y para el caso continuo comencemos con un dardo en una sola dimensión en el caso de eventos equiprobables: "],
["variables-aleatorias.html", "5.4 Variables aleatorias", " 5.4 Variables aleatorias A partir de un experimento aleatorio se pueden definir muchas preguntas de probabilidad, por ejemplo, en el caso de la ruina del jugador podríamos preguntarnos: las ganancias después del tercer juego, probabilidad de ganar, duración del experimeto (cuántos juegos se jugaron antes de alcanzar las reglas de término). Sin embargo, muchas veces nos centramos en estudiar un solo aspecto del experimento. La variable aleatoria \\(X\\) es un mapeo entre el espacio de resultados y los números reales. Distribución de probabilidad La distribución de probabilidad de una variable aleatoria \\(X\\) es simplemente una lista de todos los posibles valores y sus probabilidades correspondientes (en el caso discreto). Podemos pensar en el término distribución como una masa distribuida sobre un área o volumen \\(\\Omega\\), y \\(P(A)\\) representa la proporción de esa masa en el subconjunto \\(A\\). Definimos \\(X\\) como la variable aleatoria del número de juegos antes de que termine el experimento de la ruina del jugador, grafica la distribución de probabilidad de \\(X\\) (calcula \\(P(X=1), P(X=2),...,P(X=50)\\)). La función de distribución acumulada contiene la misma información que la función de distribución y se define como \\[P(X \\le x)\\] con la ventaja de que la definición aplica tanto al caso discreto como en el caso continuo. Esperanza La esperanza (valor esperado o media) de una variable aleatoria \\(X\\), es la media de la distribución \\(X\\), esto es, \\[E(X)=\\sum_{x\\in \\Omega_x} x P(X=x)\\] el promedio de todos los posibles valores de \\(X\\) ponderados por sus probabilidades. Por ejemplo, si \\(X\\) toma únicamente dos posibles valores, \\(a,b\\) con probabilidad \\(P(a)\\) y \\(P(b)\\) entonces \\[E(X)=aP(a)+bP(b).\\] Ejemplo: Supongamos que \\(X\\) es el valor que se produce cuando tiro un dado justo. Entonces, \\[E(X)=1\\cdot P(X=1) +2\\cdot P(X=2) +3\\cdot P(X=3) +4\\cdot P(X=4) +5\\cdot P(X=5) +6\\cdot P(X=6) = 3.5\\] Lo que nos dice que si tiramos el dado muchas veces deberíamos esperar que el promedio de las tiradas sea cercano a 3.5. Esperanza como un promedio cuando n es grande. Si vemos las probabilidades de los valores de \\(X\\) como una aproximación de frecuencias relativas cuando n es grande, entonces \\(E(X)\\) es aproximadamente el valor promedio del valor de \\(X\\) cuando n es grande. x &lt;- rnorm(10000, mean = 10) mean(x) #&gt; [1] 9.99 La esperanza cumple las siguientes reglas: Constantes. La esperanza de una variable aleatoria constante es su valor constante, \\[E(c) = c\\] Indicadoras. Si \\(I_A\\) es la función indicadora del evento \\(A\\), \\[E(I_A) = P(A)\\] Funciones. Típicamente, \\(E[g(X)]\\ne g[E(X)]\\), pero \\[E[g(X)] = \\sum_{x \\in \\Omega_X} g(x) P(X=x)\\] Factores constantes. Para una constante c, \\[E(cX)=cE(X)\\] Adición. Para cualquier par de variables aleatorias \\(X\\), \\(Y\\), \\[E(X+Y) = E(X)+E(Y)\\] Multiplicación. Típicamente \\(E(XY) \\ne E(X)E(Y)\\), pero si \\(X\\) y \\(Y\\) son independientes, entonces \\[E(XY)=E(X)E(Y)\\] Varianza y desviación estándar Si intentamos predecir el valor de una variable aleatoria usando su media \\(E(X)=\\mu\\), vamos a fallar por una cantidad aleatoria \\(X-\\mu\\). Suele ser importante tener una idea de que tan grande será esta desviación. Debido a que \\[E(X-\\mu) = E(X)-\\mu=0\\] es necesario considerar la diferencia absoluta o la diferencia al cuadrado de \\(X-\\mu\\) con el fin de tener una idea del tamaño de la desviación sin importar el signo de esta. Varianza y desviación estándar. La varianza de \\(X\\), denotada \\(var(X)=\\sigma^2\\) es la media de la desviación cuadrada de \\(X\\) respecto a su valor esperado \\(\\mu=E(X)\\): \\[\\sigma^2(X)=var(X)=E(X-\\mu)^2\\] La desviación estándar de \\(X\\), es la raíz cuadrada de la varianza de X: \\[\\sigma(X)=sd(X)=\\sqrt{var(X)}\\] Intuitivamente, \\(sd(X)\\) es una medida de la dispersión de la distribución de \\(X\\) alrededor de su media. Debido a que la varianza es el valor central de la distribución de \\((X-\\mu)^2\\), su raíz cuadrada da una idea del tamaño típico de la desviación absoluta \\(|X-\\mu|\\). Notemos que \\(E(X)\\), \\(var(X)\\) y \\(sd(X)\\) están determinados por \\(X\\), de tal manera que si dos variables aleatorias tienen la misma distribución, también tienen la misma media, varianza y desviación estándar. "],
["bootstrap-no-parametrico.html", "Sección 6 Bootstrap no paramétrico", " Sección 6 Bootstrap no paramétrico Bootstrap: to pull oneself up by one’s bootstrap Estas notas se desarrollaron con base en Efron and Tibshirani (1993), adicionalmente se usaron ideas de Hesterberg (2015). Abordamos los siguientes temas: Muestras aleatorias El principio del plug-in Bootstrap no paramétrico Ejemplos: componentes principales, ajuste de curvas, muestreo. Ejemplo: aspirina y ataques cardiacos Como explican Efron y Tibshirani, las explicaciones del bootstrap y otros métodos computacionales involucran las ideas de inferencia estadistica tradicional. Las ideas báscias no han cambiado pero la implementación de estas sí. Los tres conceptos básicos de estadística son: Recolección de datos, resúmenes (o descriptivos) de datos y inferencia. Veamos un ejemplo de estos conceptos y como se introduce bootstrap. Usaremos datos de un estudio clínico de consumo de aspirina y ataques cardiacos cuyos resultados fueron publicados en el New York Times: Planteamiento: se diseñó un estudio para investigar si el consumo de dosis bajas de aspirina podía prevenir los ataques cardiacos en hombres sanos en edad media. Recolección de datos: Se hizo un diseño controlado, aleatorizado y doblemente ciego. La mitad de los participantes recibieron aspirina y la otra mitad un placebo. Descriptivos: Las estadísticas descriptivas del artículo son muy sencillas: grupo ataques cardiacos sujetos aspirina 104 11037 placebo 189 11034 De manera que la estimación del cociente de las tasas es \\[\\hat{\\theta}=\\frac{104/11037}{189/11034} = 0.55\\] En la muestra los individuos que toman aspirina tienen únicamente 55% de los ataques que los que toman placebo. Sin embargo, lo que realmente nos interesa es \\(\\theta\\): el cociente de tasas que observaríamos si pudieramos tratar a todos los hombres y no únicamente a una muestra. Inferencia: aquí es donde recurrimos a inferencia estadística: \\[0.43 &lt; \\theta &lt; 0.70\\] El verdadero valor de \\(\\theta\\) esta en el intervalo \\((0.43,0.70)\\) con una confianza del 95%. Ahora, el bootstrap es un método de simulación basado en datos para inferencia estadística. La idea detrás es que si una muestra es una aproximación de la población que la generó, entoces podemos hacer muestreos de la muestra para calcular una estadística de interés y medir la exactitud en la misma. En este caso tenemos los resultados del experimento en la variable trial. trial &lt;- data_frame(patient = 1:22071, group = ifelse(patient &lt;= 11037, &quot;aspirin&quot;, &quot;control&quot;), heart_attack = c(rep(TRUE, 104), rep(FALSE, 10933), rep(TRUE, 189), rep(FALSE, 10845))) trial #&gt; # A tibble: 22,071 x 3 #&gt; patient group heart_attack #&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; #&gt; 1 1 aspirin TRUE #&gt; 2 2 aspirin TRUE #&gt; 3 3 aspirin TRUE #&gt; 4 4 aspirin TRUE #&gt; 5 5 aspirin TRUE #&gt; 6 6 aspirin TRUE #&gt; 7 7 aspirin TRUE #&gt; 8 8 aspirin TRUE #&gt; 9 9 aspirin TRUE #&gt; 10 10 aspirin TRUE #&gt; # ... with 22,061 more rows Y calculamos el cociente de las tasas: summ_stats &lt;- trial %&gt;% group_by(group) %&gt;% summarise( n_attacks = sum(heart_attack), n_subjects = n(), rate_attacks = n_attacks / n_subjects * 100 ) summ_stats #&gt; # A tibble: 2 x 4 #&gt; group n_attacks n_subjects rate_attacks #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 aspirin 104 11037 0.942 #&gt; 2 control 189 11034 1.71 ratio_rates &lt;- summ_stats$rate_attacks[1] / summ_stats$rate_attacks[2] Después calculamos 1000 replicaciones bootstrap de \\(\\hat{\\theta*}\\) boot_ratio_rates &lt;- function(){ boot_sample &lt;- trial %&gt;% group_by(group) %&gt;% sample_frac(replace = TRUE) rates &lt;- boot_sample %&gt;% summarise(rate_attacks = sum(heart_attack) / n()) %&gt;% pull(rate_attacks) rates[1] / rates[2] } boot_ratio_rates &lt;- rerun(1000, boot_ratio_rates()) %&gt;% map_dbl(~.x) Las replicaciones se pueden utilizar para hacer inferencia de los datos. Por ejemplo, podemos estimar el error estándar de \\(\\theta\\): se &lt;- sd(boot_ratio_rates) comma(se) #&gt; [1] &quot;0.066&quot; Referencias "],
["el-principio-del-plug-in.html", "6.1 El principio del plug-in", " 6.1 El principio del plug-in Muestras aleatorias Supongamos que tenemos una población finita o universo \\(U\\), conformado por unidades individuales con propiedades que nos gustaría aprender (opinión política, nivel educativo, preferencias de consumo, …). Debido a que es muy difícil y caro examinar cada unidad en \\(U\\) seleccionamos una muestra aleatoria. Una muestra aleatoria de tamaño \\(n\\) se define como una colección de \\(n\\) unidades \\(u_1,...,u_n\\) seleccionadas aleatoriamente de una población \\(U\\). Una vez que se selecciona una muestra aleatoria, los datos observados son la colección de medidas \\(x_1,...,x_n\\), también denotadas \\(\\textbf{x} = (x_1,...,x_n)\\). En principio, el proceso de muestreo es como sigue: Seleccionamos \\(n\\) enteros de manera independiente (con probabilidad \\(1/N\\)), cada uno de ellos asociado a un número entre \\(1\\) y \\(N\\). Los enteros determinan las unidades que seleccionamos y tomamos medidas a cada unidad. En la práctica el proceso de selección suele ser más complicado y la definición de la población \\(U\\) suele ser deficiente; sin embargo, el marco conceptual sigue siendo útil para entender la inferencia estadística. Nuestra definición de muestra aleatoria comprende muestras con y sin reemplazo: muestra sin reemplazo: una unidad particular puede aparecer a lo más una vez. muestra con reemplazo: permite que una unidad aparezca más de una vez. Es más común tomar muestras sin remplazo, sin embargo, para hacer inferencia suele ser más sencillo permitir repeticiones (muestreo con remplazo) y si el tamaño de la muestra \\(n\\) es mucho más chico que la población \\(N\\), la probabilidad de muestrear la misma unidad más de una vez es chica. El caso particular en el que obtenemos las medidas de interés de cada unidad en la población se denomina censo, y denotamos al conjunto de datos observados de la población por \\(\\mathcal{X}\\). En general, no nos interesa simplemente describir la muestra que observamos sino que queremos aprender acerca de la población de donde se seleccionó la muestra: El objetivo de la inferencia estadística es expresar lo que hemos aprendido de la población \\(\\mathcal{X}\\) a partir de los datos observados \\(\\textbf{x}\\). Ejemplo: ENLACE Veamos un ejemplo donde tomamos una muestra de 300 escuelas primarias de la Ciudad de México, de un universo de 3,200 escuelas, # universo primaria &lt;- read_csv(&quot;data/primarias.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; clave = col_character(), #&gt; turno = col_character(), #&gt; tipo = col_character(), #&gt; mun = col_integer(), #&gt; esp_3 = col_double(), #&gt; esp_6 = col_double() #&gt; ) glimpse(primaria) #&gt; Observations: 3,202 #&gt; Variables: 6 #&gt; $ clave &lt;chr&gt; &quot;09DBN0006J&quot;, &quot;09DBN0007I&quot;, &quot;09DBN0008H&quot;, &quot;09DBN0015R&quot;, ... #&gt; $ turno &lt;chr&gt; &quot;NOCTURNO&quot;, &quot;NOCTURNO&quot;, &quot;NOCTURNO&quot;, &quot;NOCTURNO&quot;, &quot;NOCTURN... #&gt; $ tipo &lt;chr&gt; &quot;GENERAL&quot;, &quot;GENERAL&quot;, &quot;GENERAL&quot;, &quot;GENERAL&quot;, &quot;GENERAL&quot;, &quot;... #&gt; $ mun &lt;int&gt; 17, 28, 28, 14, 14, 17, 17, 14, 14, 14, 16, 16, 16, 16, ... #&gt; $ esp_3 &lt;dbl&gt; 483.14, 571.03, 418.50, 776.57, 714.09, 659.04, 703.04, ... #&gt; $ esp_6 &lt;dbl&gt; 513.98, 455.92, 561.27, 540.32, 670.00, 570.04, 606.64, ... set.seed(16021) n &lt;- 300 # muestra primaria_muestra &lt;- sample_n(primaria, n) %&gt;% mutate(clase = &quot;muestra&quot;) para cada escuela en la muestra tenemos la medida \\(x_i\\), conformada por el promedio de las calificaciones en español de los alumnos de tercero y sexto de primaria (prueba ENLACE 2010): \\[x_i=(esp_{3i}, esp_{6i})\\] En este ejemplo contamos con un censo de las escuelas y tomamos la muestra aleatoria de la tabla de datos general, sin embargo, es común contar únicamente con la muestra. Para español 3o de primaria la media observada es mean(primaria_muestra$esp_3) #&gt; [1] 577.27 La media muestral es una estadística descriptiva de la muestra, pero también la podemos usar para describir a la población de escuelas. Al usar la media observada para describir a la población estamos aplicando el principio del plug-in que dice que una característica dada de una distribución puede ser aproximada por la equivalente evaluada en la distribución empírica de una muestra aleatoria. Función de distribución empírica Dada una muestra aleatoria de tamaño \\(n\\) de una distribución de probabilidad \\(P\\), la función de distribución empírica \\(P_n\\) se define como la distribución que asigna probabilidad \\(1/n\\) a cada valor \\(x_i\\) con \\(i=1,2,...,n\\). En otras palabras, \\(P_n\\) asigna a un conjunto \\(A\\) en el espacio muestral de \\(x\\) la probabilidad empírica: \\[P_n(A)=\\#\\{x_i \\in A \\}/n\\] La función de distribución empírica \\(P_n\\) es una estimación de la distribución completa \\(P\\), por lo que una manera inmediata de estimar aspectos de \\(P\\) (e.g media o mediana) es calcular el aspecto correspondiente de \\(P_n\\). En cuanto a la teoría el principio del plug-in está soportado por el teorema de Glivenko Cantelli: Sea \\(X_1,...,X_n\\) una muestra aleatoria de una distribución \\(P\\), con distribución empírica \\(P_n\\) entonces \\[\\sup_{x \\in \\mathcal{R}}|P_n(x)-P(x)|\\to_p0\\] casi seguro. Regresando al ejemplo de las escuelas, comparemos la distribución poblacional y la distribución empírica. primaria_long &lt;- primaria %&gt;% mutate(clase = &quot;población&quot;) %&gt;% rbind(primaria_muestra) %&gt;% gather(grado, calif, esp_3:esp_6) ggplot(primaria_long, aes(x = calif)) + geom_histogram(aes(y = ..density..), binwidth = 20, fill = &quot;darkgray&quot;) + facet_grid(grado ~ clase) Podemos comparar la función de distribución acumulada empírica y la función de distribución acumulada poblacional: En la siguiente gráfica la curva roja representa la función de distribución acumulada empírica y la curva con relleno gris la función de distribución acumulada poblacional. ggplot() + stat_ecdf(data = filter(primaria_long, clase == &quot;población&quot;), aes(x = calif, ymin=0, ymax=..y..), geom = &quot;ribbon&quot;, pad = TRUE, alpha = 0.5, fill = &quot;gray&quot;, color = &quot;darkgray&quot;) + stat_ecdf(data = filter(primaria_long, clase == &quot;muestra&quot;), aes(x = calif), geom = &quot;step&quot;, color = &quot;red&quot;) + facet_grid(~ grado) + labs(color = &quot;&quot;) Cuando la variable de interés toma pocos valores es fácil ver la distribución empírica, supongamos que la medición de las unidades que nos interesa es la variable tipo de escuela, entonces la distribución empírica en la muestra es table(primaria_muestra$tipo) / n #&gt; #&gt; GENERAL PARTICULAR #&gt; 0.67667 0.32333 Vale la pena notar que pasar de la muestra desagregada a la distribución empírica (lista de valores y la proporción que ocurre cada una en la muestra) no conlleva ninguna pérdida de información: el vector de frecuencias observadas es un estadístico suficiente para la verdadera distribución. Esto quiere decir que toda la información de \\(P\\) contenida en el vector de observaciones \\(\\textbf{x}\\) está también contenida en \\(P_n\\). Nota: el teorema de suficiencia asume que las observaciones \\(\\textbf{x}\\) son una muestra aleatoria de la distribución \\(P\\), este no es siempre el caso (e.g. si tenemos una serie de tiempo). Parámetros y estadísticas Cuando aplicamos teoría estadística a problemas reales, es común que las respuestas estén dadas en términos de distribuciones de probabilidad. Por ejemplo, podemos preguntarnos que tan correlacionados están los resultados de las pruebas de español correspondientes a 3o y 6o. Si conocemos la distribución de probabilidad \\(P\\) contestar esta pregunta es simplemente cuestión de aritmética, el coeficiente de correlación poblacional esta dado por: \\[corr(y,z) = \\frac{\\sum_{j=1}^{N}(Y_j - \\mu_y)(Z_j-\\mu_z)} {[\\sum_{j=1}^{N}(Y_j - \\mu_y)^2\\sum_{j=1}^{N}(Z_j - \\mu_z)^2]^{1/2}}\\] en nuestro ejemplo \\((Y_j,Z_j)\\) son el j-ésimo punto en la población de escuelas primarias \\(\\mathcal{X}\\), \\(\\mu_y=\\sum Y_j/3311\\) y \\(\\mu_z=\\sum Z_j/3311\\). ggplot(primaria, aes(x = esp_3, y = esp_6)) + geom_point(alpha = 0.5) cor(primaria$esp_3, primaria$esp_6) %&gt;% round(2) #&gt; [1] 0.72 Si no tenemos un censo debemos inferir, podríamos estimar la correlación \\(corr(y,z)\\) a través del coeficiente de correlación muestral: \\[\\hat{corr}(y,z) = \\frac{\\sum_{j=1}^{n}(y_j - \\hat{\\mu}_y)(z_j-\\hat{\\mu}_z)} {[\\sum_{j=1}^{n}(y_j - \\hat{\\mu}_y)^2\\sum_{j=1}^{n}(z_j - \\hat{\\mu}_z)^2]^{1/2}}\\] recordando que la distribución empírica es una estimación de la distribución completa. cor(primaria_muestra$esp_3, primaria_muestra$esp_6) #&gt; [1] 0.6822 Al igual que la media esto es una estimación plug-in. Otros ejemplos son: Supongamos que nos interesa estimar la mediana de las calificaciones de español para 3^o de primaria: median(primaria_muestra$esp_3) #&gt; [1] 562.46 Supongamos que nos interesa estimar la probabilidad de que la calificación de español de una escuela sea mayor a 700: \\[\\theta=\\frac{1}{N}\\sum_{j=1}^N I_{\\{Y_i&gt;700\\}}\\] donde \\(I_{\\{\\cdot\\}}\\) es la función indicadora. La estimación plug-in de \\(\\hat{\\theta}\\) sería: sum(primaria_muestra$esp_3 &gt; 700) / n #&gt; [1] 0.056667 Ejemplo: dado Observamos 100 lanzamientos de un dado, obteniendo la siguiente distribución empírica: dado &lt;- read.table(&quot;data/dado.csv&quot;, header=TRUE, quote=&quot;\\&quot;&quot;) prop.table(table(dado$x)) #&gt; #&gt; 1 2 3 4 5 6 #&gt; 0.13 0.19 0.10 0.17 0.14 0.27 En este caso no tenemos un censo, solo contamos con la muestra. Una pregunta de inferencia que surge de manera natural es si el dado es justo, esto es, si la distribución que generó esta muestra tiene una distribución \\(P = (1/6, 1/6, 1/6,1/6, 1/6, 1/6)\\). Para resolver esta pregunta, debemos hacer inferencia de la distribución empírica. Antes de proseguir repasemos dos conceptos importantes: parámetros y estadísticos: Un parámetro es una función de la distribución de probabilidad \\(\\theta=t(P)\\), mientras que una estadística es una función de la muestra \\(\\textbf{x}\\). Por ejemplo, la \\(corr(x,y)\\) es un parámetro de \\(P\\) y \\(\\hat{corr}(x,y)\\) es una estadística con base en \\(\\textbf{x}\\) y \\(\\textbf{y}\\). Entonces: El principio del plug-in es un método para estimar parámetros a partir de muestras; la estimación plug-in de un parámetro \\(\\theta=t(P)\\) se define como: \\[\\hat{\\theta}=t(P_n).\\] Es decir, estimamos la función \\(\\theta = t(P)\\) de la distribución de probabilidad \\(P\\) con la misma función aplicada en la distribución empírica \\(\\hat{\\theta}=t(P_n)\\). ¿Qué tan bien funciona el principio del plug-in? Suele ser muy bueno cuando la única información disponible de \\(P\\) es la muestra \\(\\textbf{x}\\), bajo esta circunstancia \\(\\hat{\\theta}=t(P_n)\\) no puede ser superado como estimador de \\(\\theta=t(P)\\), al menos no en el sentido asintótico de teoría estadística \\((n\\to\\infty)\\). El principio del plug-in provee de una estimación más no habla de precisión: usaremos el bootstrap para estudiar el sesgo y el error estándar del estimador plug-in \\(\\hat{\\theta}=t(P_n)\\). Distribuciones muestrales y errores estándar La distribución muestral de una estadística es la distribución de probabilidad de la misma, considerada como una variable aleatoria. Es así que la distribución muestral depende de: 1) La distribución poblacional, 2) la estadística que se está considerando, y 3) la muestra aleatoria: cómo se seleccionan las unidades de la muestra y cuántas. En teoría para obtener la distribución muestral uno seguiría los siguientes pasos: Selecciona muestras de una población (todas las posibles o un número infinito de muestras). Calcula la estadística de interés para cada muestra. La distribución de la estadística es la distribución muestral. library(LaplacesDemon) library(patchwork) # En este ejemplo la población es una mezcla de normales pob_plot &lt;- ggplot(data_frame(x = -15:20), aes(x)) + stat_function(fun = dnormm, args = list(p = c(0.3, 0.7), mu = c(-2, 8), sigma = c(3.5, 3)), alpha = 0.8) + geom_vline(aes(color = &quot;mu&quot;, xintercept = 5), alpha = 0.5) + scale_colour_manual(values = c(&#39;mu&#39; = &#39;red&#39;), name = &#39;&#39;, labels = expression(mu)) + labs(x = &quot;&quot;, subtitle = &quot;Población&quot;, color = &quot;&quot;) samples &lt;- data_frame(sample = 1:3) %&gt;% mutate( sims = rerun(3, rnormm(30, p = c(0.3, 0.7), mu = c(-2, 8), sigma = c(3.5, 3))), x_bar = map_dbl(sims, mean)) muestras_plot &lt;- samples %&gt;% unnest() %&gt;% ggplot(aes(x = sims)) + geom_histogram(binwidth = 2, alpha = 0.5, fill = &quot;darkgray&quot;) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) + geom_segment(aes(x = x_bar, xend = x_bar, y = 0, yend = 0.8), color = &quot;blue&quot;) + xlim(-15, 20) + facet_wrap(~ sample) + geom_text(aes(x = x_bar, y = 0.95, label = &quot;bar(x)&quot;), parse = TRUE, color = &quot;blue&quot;, alpha = 0.2, hjust = 1) + labs(x = &quot;&quot;, subtitle = &quot;Muestras&quot;) samples_dist &lt;- data_frame(sample = 1:10000) %&gt;% mutate( sims = rerun(10000, rnormm(100, p = c(0.3, 0.7), mu = c(-2, 8), sigma = c(3.5, 3))), mu_hat = map_dbl(sims, mean)) dist_muestral_plot &lt;- ggplot(samples_dist, aes(x = mu_hat)) + geom_density(adjust = 2) + labs(x = &quot;&quot;, subtitle = expression(&quot;Distribución muestral de &quot;~hat(mu))) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) (pob_plot | plot_spacer()) / (muestras_plot | dist_muestral_plot) Para hacer inferencia necesitamos describir la forma de la distribución muestral, es natural pensar en la desviación estándar pues es una medida de la dispersión de la distribución de la estadística alrededor de su media: El error estándar es la desviación estándar de la distribución muestral de una estadística. Ejemplo: el error estándar de una media Supongamos que \\(x\\) es una variable aleatoria que toma valores en los reales con distribución de probabilidad \\(P\\). Denotamos por \\(\\mu_P\\) y \\(\\sigma_P^2\\) la media y varianza de \\(P\\), \\[\\mu_P = E_P(x),\\] \\[\\sigma_P^2=var_P(x)=E_P[(x-\\mu_P)^2]\\] en la notación enfatizamos la dependencia de la media y varianza en la distribución \\(P\\). Ahora, sea \\((x_1,...,x_n)\\) una muestra aleatoria de \\(P\\), de tamaño \\(n\\), la media de la muestra \\(\\bar{x}=\\sum_{i=1}^nx_i/n\\) tiene: esperanza \\(\\mu_P\\), varianza \\(\\sigma_P^2/n\\). En palabras: la esperanza de \\(\\bar{x}\\) es la misma que la esperanza de \\(x\\), pero la varianza de \\(\\bar{x}\\) es \\(1/n\\) veces la varianza de \\(x\\), así que entre mayor es la \\(n\\) tenemos una mejor estimación de \\(\\mu_P\\). En el caso de la media \\(\\bar{x}\\), el error estándar, que denotamos \\(se_P(\\bar{x})\\), es la raíz de la varianza de \\(\\bar{x}\\), \\[se_P(\\bar{x}) = [var_P(\\bar{x})]^{1/2}= \\sigma_P/ \\sqrt{n}.\\] En este punto podemos usar el principio del plug-in, simplemente sustituimos \\(P_n\\) por \\(P\\) y obtenemos, primero, una estimación de \\(\\sigma_P\\): \\[\\hat{\\sigma}=\\hat{\\sigma}_{P_n} = \\bigg\\{\\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar{x})^2\\bigg\\}^{1/2}\\] de donde se sigue la estimación del error estándar: \\[\\hat{se}(\\bar{x})=\\hat{\\sigma}_{P_n}/\\sqrt{n}=\\bigg\\{\\frac{1}{n^2}\\sum_{i=1}^n(x_i-\\bar{x})^2\\bigg\\}^{1/2}\\] Notemos que usamos el principio del plug-in en dos ocasiones, primero para estimar la esperanza \\(\\mu_P\\) mediante \\(\\mu_{P_n}\\) y luego para estimar el error estándar \\(se_P(\\bar{x})\\). Consideramos la base de datos primaria, y la columna de calificaciones de español 3o de primaria (esp_3). Seleccina una muestra de tamaño \\(n = 10, 100, 1000\\). Para cada muestra calcula media y el error estándar de la media usando el principio del plug-in: \\(\\hat{\\mu}=\\bar{x}\\), y \\(\\hat{se}(\\bar{x})=\\hat{\\sigma}_{P_n}/\\sqrt{n}\\). Ahora aproximareos la distribución muestral, para cada tamaño de muestra \\(n\\): simula 10,000 muestras aleatorias, ii) calcula la media en cada muestra, iii) Realiza un histograma de la distribución muestral de las medias (las medias del paso anterior) iv) aproxima el error estándar calculando la desviación estándar de las medias del paso ii. Calcula el error estándar de la media para cada tamaño de muestra usando la información poblacional (ésta no es una aproximación), usa la fórmula: \\(se_P(\\bar{x}) = \\sigma_P/ \\sqrt{n}\\). ¿Cómo se comparan los errores estándar correspondientes a los distintos tamaños de muestra? ¿Por qué bootstrap? En el caso de la media \\(\\hat{\\theta}=\\bar{x}\\) la aplicación del principio del plug-in para el cálculo de errores estándar es inmediata; sin embargo, hay estadísticas para las cuáles no es fácil aplicar este método. El método de aproximarlo con simulación, como lo hicimos en el ejercicio de arriba no es factible pues en la práctica no podemos seleccionar un número arbitrario de muestras de la población, sino que tenemos únicamente una muestra. La idea del bootstrap es replicar el método de simulación para aproximar el error estándar, esto es seleccionar muchas muestras y calcular la estadística de interés en cada una, con la diferencia que las muestras se seleccionan de la distribución empírica a falta de la distribución poblacional. "],
["el-estimador-bootstrap-del-error-estandar.html", "6.2 El estimador bootstrap del error estándar", " 6.2 El estimador bootstrap del error estándar Entonces, los pasos para calcular estimador bootstrap del error estándar son: Tenemos una muestra aleatoria \\(\\textbf{x}=(x_1,x_2,...,x_n)\\) proveniente de una distribución de probabilidad desconocida \\(P\\), Seleccionamos muestras aleatorias con reemplazo de la distribución empírica. Calculamos la estadística de interés para cada muestra: \\[\\hat{\\theta}=s(\\textbf{x})\\] la estimación puede ser la estimación plug-in \\(t(P_n)\\) pero también puede ser otra. La distribución de la estadística es la distribución bootstrap, y el estimador bootstrap del error estándar es la desviación estándar de la distribución bootstrap. dist_empirica &lt;- data_frame(id = 1:30, obs = samples$sims[[1]]) dist_empirica_plot &lt;- ggplot(dist_empirica, aes(x = obs)) + geom_histogram(binwidth = 2, alpha = 0.5, fill = &quot;darkgray&quot;) + geom_vline(aes(color = &quot;mu&quot;, xintercept = 5), alpha = 0.5) + geom_vline(aes(xintercept = samples$x_bar[1], color = &quot;x_bar&quot;), alpha = 0.8, linetype = &quot;dashed&quot;) + xlim(-15, 20) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) + labs(x = &quot;&quot;, subtitle = expression(&quot;Distribución empírica&quot;~P[n])) + scale_colour_manual(values = c(&#39;mu&#39; = &#39;red&#39;, &#39;x_bar&#39; = &#39;blue&#39;), name = &#39;&#39;, labels = c(expression(mu), expression(bar(x)))) samples_boot &lt;- data_frame(sample_boot = 1:3) %&gt;% mutate( sims_boot = rerun(3, sample(dist_empirica$obs, replace = TRUE)), x_bar_boot = map_dbl(sims_boot, mean)) muestras_boot_plot &lt;- samples_boot %&gt;% unnest() %&gt;% ggplot(aes(x = sims_boot)) + geom_histogram(binwidth = 2, alpha = 0.5, fill = &quot;darkgray&quot;) + geom_vline(aes(xintercept = samples$x_bar[1]), color = &quot;blue&quot;, linetype = &quot;dashed&quot;, alpha = 0.8) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) + geom_segment(aes(x = x_bar_boot, xend = x_bar_boot, y = 0, yend = 0.8), color = &quot;black&quot;) + xlim(-15, 20) + facet_wrap(~ sample_boot) + geom_text(aes(x = x_bar_boot, y = 0.95, label = &quot;bar(x)^&#39;*&#39;&quot;), parse = TRUE, color = &quot;black&quot;, alpha = 0.3, hjust = 1) + labs(x = &quot;&quot;, subtitle = &quot;Muestras bootstrap&quot;) boot_dist &lt;- data_frame(sample = 1:10000) %&gt;% mutate( sims_boot = rerun(10000, sample(dist_empirica$obs, replace = TRUE)), mu_hat_star = map_dbl(sims_boot, mean)) boot_muestral_plot &lt;- ggplot(boot_dist, aes(x = mu_hat_star)) + geom_histogram(alpha = 0.5, fill = &quot;darkgray&quot;) + labs(x = &quot;&quot;, subtitle = expression(&quot;Distribución bootstrap de &quot;~hat(mu)^&#39;*&#39;)) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) + geom_vline(aes(xintercept = samples$x_bar[1]), color = &quot;blue&quot;, linetype = &quot;dashed&quot;, alpha = 0.8) (dist_empirica_plot | plot_spacer()) / (muestras_boot_plot | boot_muestral_plot) Describamos la notación y conceptos: Definimos una muestra bootstrap como una muestra aleatoria de tamaño \\(n\\) que se obtiene de la distribución empírica \\(P_n\\) y la denotamos \\[\\textbf{x}^* = (x_1^*,...,x_n^*).\\] La notación de estrella indica que \\(\\textbf{x}^*\\) no son los datos \\(\\textbf{x}\\) sino una versión de remuestreo de \\(\\textbf{x}\\). Otra manera de frasearlo: Los datos bootsrtap \\(x_1^*,...,x_n^*\\) son una muestra aleatoria de tamaño \\(n\\) seleccionada con reemplazo de la población de \\(n\\) objetos \\((x_1,...,x_n)\\). A cada muestra bootstrap \\(\\textbf{x}^*\\) le corresponde una replicación \\(\\hat{\\theta}^*=s(\\textbf{x}^*).\\) el estimador bootstrap de \\(se_P(\\hat{\\theta})\\) se define como: \\[se_{P_n}(\\hat{\\theta}^*)\\] en otras palabras, la estimación bootstrap de \\(se_P(\\hat{\\theta})\\) es el error estándar de \\(\\hat{\\theta}\\) para conjuntos de datos de tamaño \\(n\\) seleccionados de manera aleatoria de \\(P_n\\). La fórmula \\(se_{P_n}(\\hat{\\theta}^*)\\) no existe para casi ninguna estimación diferente de la media, por lo que recurrimos a la técnica computacional bootstrap: Algoritmo bootstrap para estimar errores estándar Selecciona \\(B\\) muestras bootstrap independientes: \\[\\textbf{x}^{*1},..., \\textbf{x}^{*B}\\]. Evalúa la replicación bootstrap correspondiente a cada muestra bootstrap: \\[\\hat{\\theta}^{*b}=s(\\textbf{x}^{*b})\\] para \\(b=1,2,...,B.\\) Estima el error estándar \\(se_P(\\hat{\\theta})\\) usando la desviación estándar muestral de las \\(B\\) replicaciones: \\[\\hat{se}_B = \\bigg\\{\\frac{\\sum_{b=1}^B[\\hat{\\theta}^{*}(b)-\\hat{\\theta}^*(\\cdot)]^2 }{B-1}\\bigg\\}^{1/2}\\] donde \\[\\hat{\\theta}^*(\\cdot)=\\sum_{b=1}^B \\theta^{*}(b)/B \\]. Notemos que: La estimación bootstrap de \\(se_{P}(\\hat{\\theta})\\), el error estándar de una estadística \\(\\hat{\\theta}\\), es un estimador plug-in que usa la función de distribución empírica \\(P_n\\) en lugar de la distribución desconocida \\(P\\). Conforme el número de replicaciones \\(B\\) aumenta \\[\\hat{se}_B\\approx se_{P_n}(\\hat{\\theta})\\] este hecho equivale a decir que la desviación estándar empírica se acerca a la desviación estándar poblacional conforme crece el número de muestras. La población en este caso es la población de valores \\(\\hat{\\theta}^*=s(x^*)\\). Al estimador de bootstrap ideal \\(se_{P_n}(\\hat{\\theta})\\) y su aproximación \\(\\hat{se}_B\\) se les denota estimadores bootstrap no paramétricos ya que estan basados en \\(P_n\\), el estimador no paramétrico de la población \\(P\\). Ejemplo: Error estándar bootstrap de una media mediaBoot &lt;- function(x){ # x: variable de interés # n: número de replicaciones bootstrap n &lt;- length(x) muestra_boot &lt;- sample(x, size = n, replace = TRUE) mean(muestra_boot) # replicacion bootstrap de theta_gorro } thetas_boot &lt;- rerun(10000, mediaBoot(primaria_muestra$esp_3)) %&gt;% flatten_dbl() sd(thetas_boot) #&gt; [1] 3.8618 y se compara con \\(\\hat{se}(\\bar{x})\\) (estimador plug-in del error estándar): se &lt;- function(x) sqrt(sum((x - mean(x)) ^ 2)) / length(x) se(primaria_muestra$esp_3) #&gt; [1] 3.8466 Nota: Conforme \\(B\\) aumenta \\(\\hat{se}_{B}(\\bar{x})\\to \\{\\sum_{i=1}^n(x_i - \\bar{x})^2 / n \\}^{1/2}\\), se demuestra con la ley débil de los grandes números. Considera el coeficiente de correlación muestral entre la calificación de \\(y=\\)esp_3 y la de \\(z=\\)esp_6: \\(\\hat{corr}(y,z)=0.9\\). ¿Qué tan precisa es esta estimación? Variación en distribuciones bootstrap En el proceso de estimación bootstrap hay dos fuentes de variación pues: La muestra original se selecciona con aleatoriedad de una población. Las muestras bootstrap se seleccionan con aleatoriedad de la muestra original. Esto es: La estimación bootstrap ideal es un resultado asintótico \\(B=\\infty\\), en esta caso \\(\\hat{se}_B\\) iguala la estimación plug-in \\(se_{P_n}\\). En el proceso de bootstrap podemos controlar la variación del sgundo aspecto, conocida como implementación de muestreo Monte Carlo, y la variación Monte Carlo decrece conforme incrementamos el número de muestras. Podemos eliminar la variación Monte Carlo si seleccionamos todas las posibles muestras con reemplazo de tamaño \\(n\\), hay \\({2n-1}\\choose{n}\\) posibles muestras y si seleccionamos todas obtenemos \\(\\hat{se}_\\infty\\) (bootstrap ideal), sin embargo, en la mayor parte de los problemas no es factible proceder así. Entonces, ¿cuántas muestras bootstrap? Incluso un número chico de replicaciones bootstrap, digamos \\(B=25\\) es informativo, y \\(B=50\\) con frecuencia es suficiente para dar una buena estimación de \\(se_P(\\hat{\\theta})\\) (Efron and Tibshirani (1993)). Cuando se busca estimar error estándar Hesterberg (2015) recomienda \\(B=1000\\) muestras, o \\(B=10,000\\) muestras dependiendo la presición que se busque. seMediaBoot &lt;- function(x, B){ thetas_boot &lt;- rerun(B, mediaBoot(x)) %&gt;% flatten_dbl() sd(thetas_boot) } B_muestras &lt;- data_frame(n_sims = c(5, 25, 50, 100, 200, 400, 1000, 1500, 3000, 5000, 10000, 20000)) %&gt;% mutate(est = map_dbl(n_sims, ~seMediaBoot(x = primaria_muestra$esp_3, B = .))) B_muestras #&gt; # A tibble: 12 x 2 #&gt; n_sims est #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 5 2.44 #&gt; 2 25 3.41 #&gt; 3 50 3.51 #&gt; 4 100 3.91 #&gt; 5 200 3.51 #&gt; 6 400 3.95 #&gt; 7 1000 4.03 #&gt; 8 1500 3.79 #&gt; 9 3000 3.86 #&gt; 10 5000 3.88 #&gt; 11 10000 3.83 #&gt; 12 20000 3.86 Ejemplo componentes principales: calificaciones en exámenes Los datos marks (Mardia, Kent y Bibby, 1979) contienen los puntajes de 88 estudiantes en 5 pruebas: mecánica, vectores, álgebra, análisis y estadística. Cada renglón corresponde a la calificación de un estudiante en cada prueba. marks &lt;- read_csv(&quot;data/marks.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; id = col_integer(), #&gt; MECH = col_integer(), #&gt; VECT = col_integer(), #&gt; ALG = col_integer(), #&gt; ANL = col_integer(), #&gt; STAT = col_integer() #&gt; ) glimpse(marks) #&gt; Observations: 88 #&gt; Variables: 6 #&gt; $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17... #&gt; $ MECH &lt;int&gt; 77, 63, 75, 55, 63, 53, 51, 59, 62, 64, 52, 55, 50, 65, 3... #&gt; $ VECT &lt;int&gt; 82, 78, 73, 72, 63, 61, 67, 70, 60, 72, 64, 67, 50, 63, 5... #&gt; $ ALG &lt;int&gt; 67, 80, 71, 63, 65, 72, 65, 68, 58, 60, 60, 59, 64, 58, 6... #&gt; $ ANL &lt;int&gt; 67, 70, 66, 70, 70, 64, 65, 62, 62, 62, 63, 62, 55, 56, 5... #&gt; $ STAT &lt;int&gt; 81, 81, 81, 68, 63, 73, 68, 56, 70, 45, 54, 44, 63, 37, 7... marks &lt;- select(marks, -id) Entonces un análisis de componentes principales proseguiría como sigue: pc_marks &lt;- princomp(marks) summary(pc_marks) #&gt; Importance of components: #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; Standard deviation 26.06114 14.13557 10.127604 9.147061 5.638077 #&gt; Proportion of Variance 0.61912 0.18214 0.093497 0.076269 0.028977 #&gt; Cumulative Proportion 0.61912 0.80126 0.894755 0.971023 1.000000 loadings(pc_marks) #&gt; #&gt; Loadings: #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; MECH 0.505 0.749 0.300 0.296 #&gt; VECT 0.368 0.207 -0.416 -0.783 0.189 #&gt; ALG 0.346 -0.145 -0.924 #&gt; ANL 0.451 -0.301 -0.597 0.518 0.286 #&gt; STAT 0.535 -0.548 0.600 -0.176 0.151 #&gt; #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; SS loadings 1.0 1.0 1.0 1.0 1.0 #&gt; Proportion Var 0.2 0.2 0.2 0.2 0.2 #&gt; Cumulative Var 0.2 0.4 0.6 0.8 1.0 plot(pc_marks, type = &quot;lines&quot;) biplot(pc_marks) Los cálculos de un análisis de componentes principales involucran la matriz de covarianzas empírica \\(G\\) (estimaciones plug-in) \\[G_{jk} = \\frac{1}{88}\\sum_{i=1}^88(x_{ij}-\\bar{x_j})(x_{ik}-\\bar{x_k})\\] para \\(j,k=1,2,3,4,5\\), y donde \\(\\bar{x_j} = \\sum_{i=1}^88 x_{ij} / 88\\) (la media de la i-ésima columna). G &lt;- cov(marks) * 87 / 88 G #&gt; MECH VECT ALG ANL STAT #&gt; MECH 302.29 125.777 100.43 105.065 116.071 #&gt; VECT 125.78 170.878 84.19 93.597 97.887 #&gt; ALG 100.43 84.190 111.60 110.839 120.486 #&gt; ANL 105.07 93.597 110.84 217.876 153.768 #&gt; STAT 116.07 97.887 120.49 153.768 294.372 Los pesos y las componentes principales no son mas que los eigenvalores y eigenvectores de la matriz de covarianzas \\(G\\), estos se calculan a través de una serie de de manipulaciones algebraicas que requieren cálculos del orden de p^3 (cuando G es una matriz de tamaño p\\(\\times\\)p). eigen_G &lt;- eigen(G) lambda &lt;- eigen_G$values v &lt;- eigen_G$vectors lambda #&gt; [1] 679.183 199.814 102.568 83.669 31.788 v #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] -0.50545 0.748748 0.29979 0.2961843 -0.079394 #&gt; [2,] -0.36835 0.207403 -0.41559 -0.7828882 -0.188876 #&gt; [3,] -0.34566 -0.075908 -0.14532 -0.0032363 0.923920 #&gt; [4,] -0.45112 -0.300888 -0.59663 0.5181397 -0.285522 #&gt; [5,] -0.53465 -0.547782 0.60028 -0.1757320 -0.151232 Proponemos el siguiente modelo simple para puntajes correlacionados: \\[\\textbf{x}_i = Q_i \\textbf{v}\\] donde \\(\\textbf{x}_i\\) es la tupla de calificaciones del i-ésimo estudiante, \\(Q_i\\) es un número que representa la habilidad del estudiante y \\(\\textbf{v}\\) es un vector fijo con 5 números que aplica a todos los estudiantes. Si este modelo simple fuera cierto, entonces únicamente el \\(\\hat{\\lambda}_1\\) sería positivo y \\(\\textbf{v} = \\hat{v}_1\\). Sea \\[\\hat{\\theta}=\\sum_{i=1}^5\\hat{\\lambda}_i\\] el modelo propuesto es equivalente a \\(\\hat{\\theta}=1\\), inculso si el modelo es correcto, no esperamos que \\(\\hat{\\theta}\\) sea exactamente uno pues hay ruido en los datos. theta_hat &lt;- lambda[1]/sum(lambda) theta_hat #&gt; [1] 0.61912 El valor de \\(\\hat{\\theta}\\) mide el porcentaje de la varianza explicada en la primer componente principal, ¿qué tan preciso es \\(\\hat{\\theta}\\)? La complejidad matemática en el cálculo de \\(\\hat{\\theta}\\) es irrelevante siempre y cuando podamos calcular \\(\\hat{\\theta}^*\\) para una muestra bootstrap, en esta caso una muestra bootsrtap es una base de datos de 88$$5 \\(\\textbf{X}^*\\), donde las filas \\(\\textbf{x_i}^*\\) de \\(\\textbf{X}^*\\) son una muestra aleatoria de tamaño 88 de la verdadera matriz de datos. pc_boot &lt;- function(){ muestra_boot &lt;- sample_n(marks, size = 88, replace = TRUE) G &lt;- cov(muestra_boot) * 87 / 88 eigen_G &lt;- eigen(G) theta_hat &lt;- eigen_G$values[1] / sum(eigen_G$values) } B &lt;- 1000 thetas_boot &lt;- rerun(B, pc_boot()) %&gt;% flatten_dbl() Veamos un histograma de las replicaciones de \\(\\hat{\\theta}\\): ggplot(data_frame(theta = thetas_boot)) + geom_histogram(aes(x = theta, y = ..density..), binwidth = 0.02, fill = &quot;gray40&quot;) + geom_vline(aes(xintercept = mean(theta)), color = &quot;red&quot;) + labs(x = expression(hat(theta)^&quot;*&quot;), y = &quot;&quot;) Estas tienen un error estándar theta_se &lt;- sd(thetas_boot) theta_se #&gt; [1] 0.048405 y media mean(thetas_boot) #&gt; [1] 0.61904 la media de las replicaciones es muy similar a la estimación \\(\\hat{\\theta}\\), esto indica que \\(\\hat{\\theta}\\) es cercano a insesgado. El eigenvetor \\(\\hat{v}_1\\) correspondiente al mayor eigenvalor se conoce como primera componente de \\(G\\), supongamos que deseamos resumir la calificación de los estudiantes mediante un único número, entonces la mejor combinación lineal de los puntajes es \\[y_i = \\sum_{k = 1}^5 \\hat{v}_{1k}x_{ik}\\] esto es, la combinación lineal que utiliza las componentes de \\(\\hat{v}_1\\) como ponderadores. Si queremos un resumen compuesto por dos números \\((y_i,z_i)\\), la segunda combinación lineal debería ser: \\[z_i = \\sum_{k = 1}^5 \\hat{v}_{2k}x_{ik}\\] Las componentes principales \\(\\hat{v}_1\\) y \\(\\hat{v}_2\\) son estadísticos, usa bootstrap para dar una medición de su variabilidad calculando el error estándar de cada una. Más alla de muestras aleatorias simples Introdujimos el bootstrap en el contexto de muestras aleatorias, esto es, suponiendo que las observaciones son independientes; en este escenario basta con aproximar la distribución desconocida \\(P\\) usando la dsitribución empírica \\(P_n\\), y el cálculo de los estadísticos es inmediato. Hay casos en los que el mecanismo que generó los datos es más complicado, por ejemplo, cuando tenemos dos muestras, en diseños de encuestas complejas o en series de tiempo. Ejemplo: Dos muestras En el ejemplo de experimentos clínicos de aspirina y ataques de de corazón, podemos pensar el modelo probabilístico \\(P\\) como compuesto por dos distribuciones de probabilidad \\(G\\) y \\(Q\\) una correspondiente al grupo control y otra al grupo de tratamiento, entonces las observaciones de cada grupo provienen de distribuciones distintas y el método bootstrap debe tomar en cuenta esto al generar las muestras, en este caso implica seleccionar muesreas de manera independiente dentro de cada grupo. Ejemplo: Bootstrap en muestreo de encuestas La necesidad de estimaciones confiables junto con el uso eficiente de recursos conllevan a diseños de muestras complejas. Estos diseños típicamente usan las siguientes técnicas: muestreo sin reemplazo de una población finita, muestreo sistemático, estratificación, conglomerados, ajustes a no-respuesta, postestratificación. Como consecuencia, los valores de la muestra suelen no ser independientes. La complejidad de los diseños de encuestas conlleva a que el cálculo de errores estándar sea muy complicado, para atacar este problema hay dos técnicas básicas: 1) un enfoque analítico usando linearización, 2) métodos de remuestreo como bootstrap. El incremento en el poder de cómputo ha favorecido los métodos de remuestreo pues la linearización requiere del desarrollo de una fórmula para cada estimación y supuestos adicionales para simplificar. En 1988 Rao and Wu (1988) propusieron un método de bootstrap para diseños estratificados multietápicos con reemplazo de UPMs que describimos a continuación. ENIGH. Usaremos como ejemplo la Encuesta Nacional de Ingresos y Gastos de los Hogares, ENIGH 2014 (INEGI 2014), esta encuesta usa un diseño de conglomerados estratificado. Antes de proceder a bootstrap debemos entender como se seleccionaron los datos, esto es, el diseño de la muestra: Unidad primaria de muestreo (UPM). Las UPMs están constituidas por agrupaciones de viviendas. Se les denomina unidades primarias pues corresponden a la primera etapa de selección, las unidades secundarias (USMs) serían los hogares. Estratificación. Los estratos se construyen en base a estado, ámbito (urbano, complemento urbano, rural), características sociodemográficas de los habitantes de las viviendas, características físicas y equipamiento. El proceso de estratificación resulta en 888 subestratos en todo el ámbito nacional. La selección de la muestra es independiente para cada estrato, y una vez que se obtiene la muestra se calculan los factores de expansión que reflejan las distintas probabilidades de selección. Después se llevan a cabo ajustes por no respuesta y por proyección (calibración), esta última busca que distintos dominios de la muestra coincidan con la proyección de población de INEGI. concentrado_hogar &lt;- read_csv(&quot;data/concentradohogar.csv&quot;) concentrado_hogar #&gt; # A tibble: 19,479 x 132 #&gt; folioviv foliohog ubica_geo ageb tam_loc est_socio est_dis upm #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 0100008… 1 010010001 028-6 1 4 005 00670 #&gt; 2 0100008… 1 010010001 028-6 1 4 005 00670 #&gt; 3 0100008… 1 010010001 028-6 1 4 005 00670 #&gt; 4 0100008… 1 010010001 028-6 1 4 005 00670 #&gt; 5 0100010… 1 010010001 028-6 1 4 005 00600 #&gt; 6 0100010… 1 010010001 028-6 1 4 005 00600 #&gt; 7 0100010… 1 010010001 028-6 1 4 005 00600 #&gt; 8 0100010… 1 010010001 028-6 1 4 005 00600 #&gt; 9 0100010… 1 010010001 028-6 1 4 005 00600 #&gt; 10 0100018… 1 010010001 029-0 1 3 004 00570 #&gt; # ... with 19,469 more rows, and 124 more variables: factor_hog &lt;int&gt;, #&gt; # clase_hog &lt;int&gt;, sexo_jefe &lt;int&gt;, edad_jefe &lt;int&gt;, educa_jefe &lt;chr&gt;, #&gt; # tot_integ &lt;int&gt;, hombres &lt;int&gt;, mujeres &lt;int&gt;, mayores &lt;int&gt;, #&gt; # menores &lt;int&gt;, p12_64 &lt;int&gt;, p65mas &lt;int&gt;, ocupados &lt;int&gt;, #&gt; # percep_ing &lt;int&gt;, perc_ocupa &lt;int&gt;, ing_total &lt;dbl&gt;, ing_cor &lt;dbl&gt;, #&gt; # ing_mon &lt;dbl&gt;, trabajo &lt;dbl&gt;, sueldos &lt;dbl&gt;, horas_extr &lt;dbl&gt;, #&gt; # comisiones &lt;dbl&gt;, otra_rem &lt;dbl&gt;, negocio &lt;dbl&gt;, noagrop &lt;dbl&gt;, #&gt; # industria &lt;dbl&gt;, comercio &lt;dbl&gt;, servicios &lt;dbl&gt;, agrope &lt;dbl&gt;, #&gt; # agricolas &lt;dbl&gt;, pecuarios &lt;dbl&gt;, reproducc &lt;int&gt;, pesca &lt;int&gt;, #&gt; # otros_trab &lt;dbl&gt;, rentas &lt;dbl&gt;, utilidad &lt;dbl&gt;, arrenda &lt;dbl&gt;, #&gt; # transfer &lt;dbl&gt;, jubilacion &lt;dbl&gt;, becas &lt;dbl&gt;, donativos &lt;dbl&gt;, #&gt; # remesas &lt;dbl&gt;, bene_gob &lt;dbl&gt;, otros_ing &lt;dbl&gt;, gasto_nom &lt;dbl&gt;, #&gt; # autoconsum &lt;dbl&gt;, remu_espec &lt;dbl&gt;, transf_esp &lt;dbl&gt;, #&gt; # transf_hog &lt;dbl&gt;, trans_inst &lt;dbl&gt;, estim_alqu &lt;dbl&gt;, #&gt; # percep_tot &lt;dbl&gt;, percep_mon &lt;dbl&gt;, retiro_inv &lt;dbl&gt;, prestamos &lt;dbl&gt;, #&gt; # otras_perc &lt;dbl&gt;, erogac_nom &lt;dbl&gt;, gasto_tot &lt;dbl&gt;, gasto_cor &lt;dbl&gt;, #&gt; # gasto_mon &lt;dbl&gt;, alimentos &lt;dbl&gt;, ali_dentro &lt;dbl&gt;, cereales &lt;dbl&gt;, #&gt; # carnes &lt;dbl&gt;, pescado &lt;dbl&gt;, leche &lt;dbl&gt;, huevo &lt;dbl&gt;, aceites &lt;dbl&gt;, #&gt; # tuberculo &lt;dbl&gt;, verduras &lt;dbl&gt;, frutas &lt;dbl&gt;, azucar &lt;dbl&gt;, #&gt; # cafe &lt;dbl&gt;, especias &lt;dbl&gt;, otros_alim &lt;dbl&gt;, bebidas &lt;dbl&gt;, #&gt; # ali_fuera &lt;dbl&gt;, tabaco &lt;dbl&gt;, vesti_calz &lt;dbl&gt;, vestido &lt;dbl&gt;, #&gt; # calzado &lt;dbl&gt;, vivienda &lt;dbl&gt;, alquiler &lt;dbl&gt;, pred_cons &lt;dbl&gt;, #&gt; # agua &lt;dbl&gt;, energia &lt;dbl&gt;, limpieza &lt;dbl&gt;, cuidados &lt;dbl&gt;, #&gt; # utensilios &lt;dbl&gt;, enseres &lt;dbl&gt;, salud &lt;dbl&gt;, atenc_ambu &lt;dbl&gt;, #&gt; # hospital &lt;dbl&gt;, medicinas &lt;dbl&gt;, transporte &lt;dbl&gt;, publico &lt;dbl&gt;, #&gt; # foraneo &lt;dbl&gt;, adqui_vehi &lt;dbl&gt;, mantenim &lt;dbl&gt;, refaccion &lt;dbl&gt;, … # seleccionar variable de ingreso corriente hogar &lt;- concentrado_hogar %&gt;% mutate(jefe_hombre = sexo_jefe == 1) %&gt;% select(folioviv, foliohog, est_dis, upm, factor_hog, ing_cor, sexo_jefe, edad_jefe) %&gt;% group_by(est_dis) %&gt;% mutate( n = n_distinct(upm), # número de upms por estrato jefa_50 = (sexo_jefe == 2) &amp; (edad_jefe &gt; 50) ) %&gt;% ungroup() head(hogar) #&gt; # A tibble: 6 x 10 #&gt; folioviv foliohog est_dis upm factor_hog ing_cor sexo_jefe edad_jefe #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 0100008… 1 005 00670 694 39787. 2 77 #&gt; 2 0100008… 1 005 00670 694 19524. 1 64 #&gt; 3 0100008… 1 005 00670 694 99258. 1 60 #&gt; 4 0100008… 1 005 00670 694 87884. 1 79 #&gt; 5 0100010… 1 005 00600 660 84427. 1 72 #&gt; 6 0100010… 1 005 00600 660 232014. 1 67 #&gt; # ... with 2 more variables: n &lt;int&gt;, jefa_50 &lt;lgl&gt; Para el cálculo de estadísticos debemos usar los factores de expansión, por ejemplo el ingreso trimestral total sería: sum(hogar$factor_hog * hogar$ing_cor / 1000) #&gt; [1] 1257944071 y ingreso trimestral medio (miles pesos) sum(hogar$factor_hog * hogar$ing_cor / 1000) / sum(hogar$factor_hog) #&gt; [1] 39.719 Veamos ahora como calcular el error estándar siguiendo el bootstrap de Rao y Wu: En cada estrato se seleccionan con reemplazo \\(m_h\\) UPMs de las \\(n_h\\) de la muestra original. Denotamos por \\(m_{hi}^*\\) el número de veces que se seleccionó la UPM \\(i\\) en el estrato \\(h\\) (de tal manera que \\(\\sum m_{hi}^*=m_h\\)). Creamos una replicación del ponderador correspondiente a la \\(k\\)-ésima unidad (USM) como: \\[d_k^*=d_k \\bigg[\\bigg(1-\\sqrt{\\frac{m_h}{n_h - 1}}\\bigg) + \\bigg(\\sqrt{\\frac{m_h}{n_h - 1}}\\frac{n_h}{m_h}m_{h}^*\\bigg)\\bigg]\\] donde \\(d_k\\) es el inverso de la probabilidad de selección. Si \\(m_h&lt;(n_h -1)\\) todos los pesos definidos de esta manera serán no negativos. Calculamos el peso final \\(w_k^*\\) aplicando a \\(d_k^*\\) los mismos ajustes que se hicieron a los ponderadores originales. Calculamos el estadístico de interés \\(\\hat{\\theta}\\) usando los ponderadores \\(w_k^*\\) en lugar de los originales \\(w_k\\). Repetimos los pasos 1 y 2 \\(B\\) veces para obtener \\(\\hat{\\theta}^{*1},\\hat{\\theta}^{*2},...,\\hat{\\theta}^{*B}\\). Calculamos el error estándar como: \\[\\hat{se}_B = \\bigg\\{\\frac{\\sum_{b=1}^B[\\hat{\\theta}^*(b)-\\hat{\\theta}^*(\\cdot)]^2 }{B}\\bigg\\}^{1/2}\\] Podemos elegir cualquier valor de \\(m_h \\geq 1\\), el más sencillo es elegir \\(m_h=n_h-1\\), en este caso: \\[d_k^*=d_k \\frac{n_h}{n_h-1}m_{hi}^*\\] en este escenario las unidades que no se incluyen en la muestra tienen un valor de cero como ponderador. Si elegimos \\(n_h \\ne n_h-1\\) las unidades que no están en la muestra tienen ponderador distinto a cero, si \\(m_h=n_h\\) el ponderador podría tomar valores negativos. Implementemos el bootstrap de Rao y Wu a la ENIGH, usaremos \\(m_h=n_h-1\\) # creamos una tabla con los estratos y upms est_upm &lt;- hogar %&gt;% distinct(est_dis, upm, n) hogar_factor &lt;- est_upm %&gt;% split(.$est_dis) %&gt;% # dentro de cada estrato tomamos muestra (n_h-1) map_df(~sample_n(., size = first(.$n) - 1, replace = TRUE)) %&gt;% add_count(upm) %&gt;% # calculamos m_hi* left_join(hogar, by = c(&quot;est_dis&quot;, &quot;upm&quot;, &quot;n&quot;)) %&gt;% mutate(factor_b = factor_hog * nn * n / (n - 1)) # unimos los pasos anteriores en una función para replicar en cada muestra bootstrap svy_boot &lt;- function(est_upm, hogar){ m_hi &lt;- est_upm %&gt;% split(.$est_dis) %&gt;% map(~sample(.$upm, size = first(.$n) - 1, replace = TRUE)) %&gt;% flatten_chr() %&gt;% plyr::count() %&gt;% select(upm = x, m_h = freq) m_hi %&gt;% mutate(upm = as.character(upm)) %&gt;% left_join(hogar, by = c(&quot;upm&quot;)) %&gt;% mutate(factor_b = factor_hog * m_h * n / (n - 1)) } set.seed(1038984) boot_rep &lt;- rerun(500, svy_boot(est_upm, hogar)) # Aplicación a ingreso medio media &lt;- function(w, x) sum(w * x) / sum(w) # La media es: hogar %&gt;% summarise(media = media(factor_hog, ing_cor)) #&gt; # A tibble: 1 x 1 #&gt; media #&gt; &lt;dbl&gt; #&gt; 1 39719. Y el error estándar: map_dbl(boot_rep, ~media(w = .$factor_b, x = .$ing_cor)) %&gt;% sd() #&gt; [1] 946.01 El método bootstrap está implementado en el paquete survey y más recientemente en srvyr que es una versión tidy que utiliza las funciones en survey. Podemos comparar nuestros resultados con la implementación en survey. # 1. Definimos el diseño de la encuesta library(survey) library(srvyr) enigh_design &lt;- hogar %&gt;% as_survey_design(ids = upm, weights = factor_hog, strata = est_dis) # 2. Elegimos bootstrap como el método para el cálculo de errores estándar set.seed(7398731) enigh_boot &lt;- enigh_design %&gt;% as_survey_rep(type = &quot;subbootstrap&quot;, replicates = 500) # 3. Así calculamos la media enigh_boot %&gt;% srvyr::summarise(mean_ingcor = survey_mean(ing_cor)) #&gt; # A tibble: 1 x 2 #&gt; mean_ingcor mean_ingcor_se #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 39719. 1008. # cuantiles svyquantile(~ing_cor, enigh_boot, quantiles = seq(0.1, 1, 0.1), interval.type = &quot;quantile&quot;) #&gt; Statistic: #&gt; ing_cor #&gt; q0.1 10622 #&gt; q0.2 14775 #&gt; q0.3 18597 #&gt; q0.4 22682 #&gt; q0.5 27186 #&gt; q0.6 32726 #&gt; q0.7 40057 #&gt; q0.8 51990 #&gt; q0.9 76285 #&gt; q1 4150377 #&gt; SE: #&gt; ing_cor #&gt; q0.1 143.90 #&gt; q0.2 165.40 #&gt; q0.3 193.00 #&gt; q0.4 218.38 #&gt; q0.5 239.42 #&gt; q0.6 347.55 #&gt; q0.7 483.52 #&gt; q0.8 768.62 #&gt; q0.9 1322.12 #&gt; q1 1318629.10 Supongamos que queremos calcular la media para los hogares con jefe de familia mujer mayor a 50 años. # Creamos datos con filter y repetimos lo de arriba hogar_mujer &lt;- filter(hogar, jefa_50) est_upm_mujer &lt;- hogar_mujer %&gt;% distinct(est_dis, upm, n) # bootstrap boot_rep_mujer &lt;- rerun(500, svy_boot(est_upm_mujer, hogar_mujer)) # media y error estándar hogar_mujer %&gt;% summarise(media = media(factor_hog, ing_cor)) #&gt; # A tibble: 1 x 1 #&gt; media #&gt; &lt;dbl&gt; #&gt; 1 35259. # usamos bootstrap para calcular los errores estándar map_dbl(boot_rep_mujer, ~media(w = .$factor_b, x = .$ing_cor)) %&gt;% sd() #&gt; [1] 1788.6 Comparemos con los resultados de srvyr. ¿qué pasa? library(srvyr) enigh_boot %&gt;% srvyr::group_by(jefa_50) %&gt;% srvyr::summarise(mean_ingcor = survey_mean(ing_cor)) #&gt; # A tibble: 2 x 3 #&gt; jefa_50 mean_ingcor mean_ingcor_se #&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 FALSE 40412. 949. #&gt; 2 TRUE 35259. 1988. Sub-poblaciones como “jefas de familia mayores a 50” se conocen como un dominio, esto es un subgrupo cuyo tamaño de muestra es aleatorio. Este ejemplo nos recalca la importancia de considerar el proceso en que se generó la muestra para calcular los errores estándar bootstrap. map_dbl(boot_rep, function(x){hm &lt;- filter(x, jefa_50); media(w = hm$factor_b, x = hm$ing_cor)}) %&gt;% sd() #&gt; [1] 1965.4 Resumiendo: El bootstrap de Rao y Wu genera un estimador consistente y aproximadamente insesgado de la varianza de estadísticos no lineales y para la varianza de un cuantil. Este método supone que la seleccion de UPMs es con reemplazo; hay variaciones del estimador bootstrap de Rao y Wu que extienden el método que acabamos de estudiar; sin embargo, es común ignorar este aspecto, por ejemplo Mach et al estudian las propiedades del estimador de varianza bootstrap de Rao y Wu cuando la muestra se seleccionó sin reemplazo. Referencias "],
["intervalos-de-confianza.html", "6.3 Intervalos de confianza", " 6.3 Intervalos de confianza Hasta ahora hemos discutido la idea detrás del bootstrap y como se puede usar para estimar errores estándar. Comenzamos con el error estándar pues es la manera más común para describir la precisión de una estadística. En términos generales, esperamos que \\(\\bar{x}\\) este a una distancia de \\(\\mu_P\\) menor a un error estándar el 68% del tiempo, y a menos de 2 errores estándar el 95% del tiempo. Estos porcentajes están basados el teorema central del límite que nos dice que bajo ciertas condiciones (bastante generales) de \\(P\\) la distribución de \\(\\bar{x}\\) se aproximará a una distribución normal: \\[\\bar{x} \\overset{\\cdot}{\\sim} N(\\mu_P,\\sigma_P^2/n)\\] Veamos algunos ejemplos de como funciona el Teorema del Límite Central, buscamos ver como se aproxima la distribución muestral de la media (cuando las observaciones provienen de distintas distribuciones) a una Normal conforme aumenta el tamaño de muestra. Para esto, aproximamos la distribución muestral de la media usando simulación de la población. Vale la pena observar que hay distribuciones que requieren un mayor tamaño de muestra \\(n\\) para lograr una buena aproximación (por ejemplo la log-normal), ¿a qué se debe esto? Para la opción de Elecciones tenemos una población de tamaño \\(N=143,437\\) y el objetivo es estimar la media del tamaño de la lista nominal de las casillas (datos de las elecciones presidenciales de 2012). Podemos ver como mejora la aproximación Normal de la distribución muestral conforme aumenta el tamaño de muestra \\(n\\); sin embargo, también sobresale que no es necesario tomar una muestra demasiado grande (\\(n = 60\\) ya es razonable). knitr::include_app(&quot;https://tereom.shinyapps.io/15-TLC/&quot;, height = &quot;1000px&quot;) En lo que sigue veremos distintas maneras de construir intervalos de confianza usando bootstrap. Un intervalo de confianza \\((1-2\\alpha)\\)% para un parámetro \\(\\theta\\) es un intervalo \\((a,b)\\) tal que \\(P(a \\le \\theta \\le b) = 1-2\\alpha\\) para todo \\(\\theta \\in \\Theta\\). Y comenzamos con la versión bootstrap del intervalo más popular. Intervalo Normal con error estándar bootstrap. El intervalo para \\(\\hat{\\theta}\\) con un nivel de confianza de \\(100\\cdot(1-2\\alpha)\\%\\) se define como: \\[(\\hat{\\theta}-z^{(1-\\alpha)}\\cdot \\hat{se}_B, \\hat{\\theta}+z^{(1-\\alpha)}\\cdot \\hat{se})\\]. donde \\(z^{(\\alpha)}\\) denota el percentil \\(100\\cdot \\alpha\\) de una distribución \\(N(0,1)\\). este intervalo está soportado por el Teorema Central del Límite, sin embargo, no es adecuado cuando \\(\\hat{\\theta}\\) no se distribuye aproximadamente Normal. 6.3.0.1 Ejemplo: kurtosis Supongamos que queremos estimar la kurtosis de una base de datos que consta de 799 tiempos de espera entre pulsasiones de un nervio (Cox, Lewis 1976). \\[\\hat{\\theta} = t(P_n) =\\frac{1/n \\sum_{i=1}^n(x_i-\\hat{\\mu})^3}{\\hat{\\sigma}^3}\\] nerve &lt;- read_delim(&quot;data/nerve.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, col_names = FALSE, trim_ws = TRUE) #&gt; Parsed with column specification: #&gt; cols( #&gt; X1 = col_double(), #&gt; X2 = col_double(), #&gt; X3 = col_double(), #&gt; X4 = col_double(), #&gt; X5 = col_double(), #&gt; X6 = col_double() #&gt; ) nerve_long &lt;- tidyr::gather(nerve, col, val, X1:X6) %&gt;% filter(!is.na(val)) kurtosis &lt;- function(x){ n &lt;- length(x) 1 / n * sum((x - mean(x)) ^ 3) / sd(x) ^ 3 } theta_hat &lt;- kurtosis(nerve_long$val) theta_hat #&gt; [1] 1.7579 kurtosis_boot &lt;- function(x, n){ x_boot &lt;- sample(x, n, replace = TRUE) kurtosis(x_boot) } B &lt;- 10000 kurtosis &lt;- rerun(B, kurtosis_boot(nerve_long$val, length(nerve_long$val))) %&gt;% flatten_dbl() Usando el intervalo normal tenemos: li_normal &lt;- round(theta_hat - 1.96 * sd(kurtosis), 2) ls_normal &lt;- round(theta_hat + 1.96 * sd(kurtosis), 2) c(li_normal, ls_normal) #&gt; [1] 1.44 2.08 Una modificación común del intervalo normal es el intervalo t, estos intervalos son mejores en caso de muestras pequeñas (\\(n\\) chica). Intervalo \\(t\\) con error estándar bootstrap. Para una muestra de tamaño \\(n\\) el intervalo \\(t\\) con un nivel de confianza de \\(100\\cdot(1-2\\alpha)\\%\\) se define como: \\[(\\hat{\\theta}-t^{(1-\\alpha)}_{n-1}\\cdot \\hat{se}_B, \\hat{\\theta}+t^{(1-\\alpha)}_{n-1}\\cdot \\hat{se}_B)\\]. donde \\(t^{(\\alpha)}_{n-1}\\) denota denota el percentil \\(100\\cdot \\alpha\\) de una distribución \\(t\\) con \\(n-1\\) grados de libertad. n_nerve &lt;- nrow(nerve_long) li_t &lt;- round(theta_hat + qt(0.025, n_nerve - 1) * sd(kurtosis), 2) ls_t &lt;- round(theta_hat - qt(0.025, n_nerve - 1) * sd(kurtosis), 2) c(li_t, ls_t) #&gt; [1] 1.44 2.08 Los intervalos normales y \\(t\\) se valen de la estimación bootstrap del error estándar; sin embargo, el bootstrap se puede usar para estimar la función de distribución de \\(\\hat{\\theta}\\) por lo que no es necesario hacer supuestos distribucionales para \\(\\hat{\\theta}\\) sino que podemos estimarla como parte del proceso de construir intervalos de confianza. Veamos un histograma de las replicaciones bootstrap de \\(\\hat{\\theta}^*\\) nerve_kurtosis &lt;- data_frame(kurtosis) hist_nerve &lt;- ggplot(nerve_kurtosis, aes(x = kurtosis)) + geom_histogram(binwidth = 0.05, fill = &quot;gray30&quot;) + geom_vline(xintercept = c(li_normal, ls_normal, theta_hat), color = c(&quot;black&quot;, &quot;black&quot;, &quot;red&quot;), alpha = 0.5) qq_nerve &lt;- ggplot(nerve_kurtosis) + geom_abline(color = &quot;red&quot;, alpha = 0.5) + stat_qq(aes(sample = kurtosis), dparams = list(mean = mean(kurtosis), sd = sd(kurtosis))) grid.arrange(hist_nerve, qq_nerve, ncol = 2, newpage = FALSE) En el ejemplo anterior el supuesto de normalidad parece razonable, veamos como se comparan los cuantiles de la estimación de la distribución de \\(\\hat{\\theta}\\) con los cuantiles de una normal: comma(q_kurt &lt;- quantile(kurtosis, probs = c(0.025, 0.05, 0.1, 0.9, 0.95, 0.975))) comma(qnorm(p = c(0.025, 0.05, 0.1, 0.9, 0.95, 0.975), mean = theta_hat, sd = sd(kurtosis))) #&gt; 2.5% 5% 10% 90% 95% 97.5% #&gt; &quot;1.4&quot; &quot;1.5&quot; &quot;1.5&quot; &quot;2.0&quot; &quot;2.0&quot; &quot;2.1&quot; #&gt; [1] &quot;1.4&quot; &quot;1.5&quot; &quot;1.5&quot; &quot;2.0&quot; &quot;2.0&quot; &quot;2.1&quot; Esto sugiere usar los cuantiles del histograma bootstrap para definir los límites de los intervalos de confianza: Percentiles. Denotemos por \\(G\\) la función de distribución acumulada de \\(\\hat{\\theta}^*\\) el intervalo percentil de \\(1-2\\alpha\\) se define por los percentiles \\(\\alpha\\) y \\(1-\\alpha\\) de \\(G\\) \\[(\\theta^*_{\\%,inf}, \\theta^*_{\\%,sup}) = (G^{-1}(\\alpha), G^{-1}(1-\\alpha))\\] Por definición \\(G^{-1}(\\alpha)=\\hat{\\theta}^*(\\alpha)\\), esto es, el percentil \\(100\\cdot \\alpha\\) de la distribución bootstrap, por lo que podemos escribir el intervalo bootstrap como \\[(\\theta^*_{\\%,inf}, \\theta^*_{\\%,sup})=(\\hat{\\theta}^*(\\alpha),\\hat{\\theta}^*(1-\\alpha))\\] ggplot(arrange(nerve_kurtosis, kurtosis)) + stat_ecdf(aes(x = kurtosis)) + geom_segment(data = data_frame(x = c(-Inf, -Inf, q_kurt[c(1, 6)]), xend = q_kurt[c(1, 6, 1, 6)], y = c(0.025, 0.975, 0, 0), yend = c(0.025, 0.975, 0.025, 0.975)), aes(x = x, xend = xend, y = y, yend = yend), color = &quot;red&quot;, size = 0.4, alpha = 0.5) + labs(x = &quot;Cuantiles muestrales&quot;, y = &quot;ecdf&quot;) Las expresiones de arriba hacen referencia a la situación bootstrap ideal donde el número de replicaciones bootstrap es infinito, en la práctica usamos aproximaciones. Y se procede como sigue: Intervalo percentil: Generamos B muestras bootstrap independientes \\(\\textbf{x}^{*1},..., \\textbf{x}^{*B}\\) y calculamos las replicaciones \\(\\hat{\\theta}^{*b}=s(x^{*b}).\\) Sea \\(\\hat{\\theta}^{*}_B(\\alpha)\\) el percentil \\(100\\cdot\\alpha\\) de la distribución empírica de \\(\\hat{\\theta}^{*}\\), y \\(\\hat{\\theta}^{*}_B(1-\\alpha)\\) el correspondiente al percentil \\(100\\cdot (1-\\alpha)\\), escribimos el intervalo de percentil \\(1-2\\alpha\\) como \\[(\\theta^*_{\\%,inf}, \\theta^*_{\\%,sup})\\approx(\\hat{\\theta}^*_B(\\alpha),\\hat{\\theta}^*_B(1-\\alpha))\\] ls_per &lt;- round(quantile(kurtosis, prob = 0.975), 2) li_per &lt;- round(quantile(kurtosis, prob = 0.025), 2) stringr::str_c(li_normal, ls_normal, sep = &quot;,&quot;) stringr::str_c(li_per, ls_per, sep = &quot;,&quot;) #&gt; [1] &quot;1.44,2.08&quot; #&gt; [1] &quot;1.43,2.06&quot; Si la distribución de \\(\\hat{\\theta}^*\\) es aproximadamente normal, entonces los intervalos normales y de percentiles serán similares. Con el fin de comparar los intervalos creamos un ejemplo de simulación (ejemplo tomado de Efron and Tibshirani (1993)), generamos una muestra de tamaño 10 de una distribución normal estándar, supongamos que el parámetro de interés es \\(e^{\\mu}\\) donde \\(\\mu\\) es la media poblacional. set.seed(137612) x &lt;- rnorm(10) boot_sim_exp &lt;- function(){ x_boot &lt;- sample(x, size = 10, replace = TRUE) exp(mean(x_boot)) } theta_boot &lt;- rerun(1000, boot_sim_exp()) %&gt;% flatten_dbl() theta_boot_df &lt;- data_frame(theta_boot) hist_emu &lt;- ggplot(theta_boot_df, aes(x = theta_boot)) + geom_histogram(fill = &quot;gray30&quot;, binwidth = 0.08) qq_emu &lt;- ggplot(theta_boot_df) + geom_abline(color = &quot;red&quot;, alpha = 0.5) + stat_qq(aes(sample = theta_boot), dparams = list(mean = mean(theta_boot), sd = sd(theta_boot))) grid.arrange(hist_emu, qq_emu, ncol = 2, newpage = FALSE) La distribución empírica de \\(\\hat{\\theta}^*\\) es asimétrica, por lo que no esperamos que coincidan los intervalos. # Normal round(exp(mean(x)) - 1.96 * sd(theta_boot), 2) #&gt; [1] 0.33 round(exp(mean(x)) + 1.96 * sd(theta_boot), 2) #&gt; [1] 1.62 #Percentil round(quantile(theta_boot, prob = 0.025), 2) #&gt; 2.5% #&gt; 0.54 round(quantile(theta_boot, prob = 0.975), 2) #&gt; 97.5% #&gt; 1.78 La inspección del histograma deja claro que la aproximación normal no es conveniente en este caso, veamos que ocurre cuando aplicamos la transformación logarítmica. hist_log &lt;- ggplot(data_frame(theta_boot), aes(x = log(theta_boot))) + geom_histogram(fill = &quot;gray30&quot;, binwidth = 0.08) qq_log &lt;- ggplot(data_frame(theta_boot)) + geom_abline(color = &quot;red&quot;, alpha = 0.5) + stat_qq(aes(sample = log(theta_boot)), dparams = list(mean = mean(log(theta_boot)), sd = sd(log(theta_boot)))) grid.arrange(hist_log, qq_log, ncol = 2, newpage = FALSE) Y los intervalos se comparan: # Normal round(mean(x) - 1.96 * sd(log(theta_boot)), 2) #&gt; [1] -0.64 round(mean(x) + 1.96 * sd(log(theta_boot)), 2) #&gt; [1] 0.59 #Percentil round(quantile(log(theta_boot), prob = 0.025), 2) #&gt; 2.5% #&gt; -0.62 round(quantile(log(theta_boot), prob = 0.975), 2) #&gt; 97.5% #&gt; 0.58 La transformación logarítmica convierte la distribución de \\(\\hat{\\theta}\\) en normal y por tanto los intervalos de \\(\\hat{\\phi}^*=log(\\hat{\\theta}^*)\\) son similares. La forma normal no es sorprendente pues \\(\\hat{\\phi}^*=\\bar{x}^*\\). Si mapeamos los intervalos normales calculados para \\(log(\\hat{\\theta}^*)\\) de regreso a la escala de \\(\\theta\\) obtenemos intervalos similares a los calculados para \\(\\hat{\\theta}^*\\) usando percentiles: exp(round(mean(x) - 1.96 * sd(log(theta_boot)), 2)) #&gt; [1] 0.52729 exp(round(mean(x) + 1.96 * sd(log(theta_boot)), 2)) #&gt; [1] 1.804 Podemos ver que el método de aplicar una transformación, calcular intervalos usando la normal y aplicar la transformación inversa para volver a la escala original genera intervalos de confianza atractivos, el problema con este método es que requiere que conozcamos la transformación adecuada para cada parámetro. Por otra parte, podemos pensar en el método del percentil como un algoritmo que incorpora la transformación de manera automática: Intervalos acelerados y corregidos por sesgo. Esta es una versión mejorada del intervalo de percentil, la denotamos \\(BC_{a}\\) (bias-corrected and accelerated). Usaremos un ejemplo de Efron and Tibshirani (1993), los datos constan de los resultados en dos pruebas espaciales de 26 niños con algún problema neurológico. Supongamos que queremos calcular un intervalo de confianza de 90% para \\(\\theta=var(A)\\). El estimador plugin es: \\[\\hat{\\theta}=\\sum_{i=1}^n(A_i-\\bar{A})^2/n\\] notemos que el estimador plug-in es ligeramente menor que el estimador usual insesgado: \\[\\hat{\\theta}=\\sum_{i=1}^n(A_i-\\bar{A})^2/(n-1)\\] library(bootstrap) ggplot(spatial) + geom_point(aes(A, B)) sum((spatial$A - mean(spatial$A)) ^ 2) / nrow(spatial) #&gt; [1] 171.53 sum((spatial$A - mean(spatial$A)) ^ 2) / (nrow(spatial) - 1) #&gt; [1] 178.4 El método \\(BC_{a}\\) corrige el sesgo de manera automática, lo cuál es una de sus prinicipales ventajas comparado con el método del percentil. &lt;div class=“caja&gt; Los extremos en los intervalos \\(BC_{a}\\) están dados por percentiles de la distribución bootstrap, los percentiles usados dependen de dos números \\(\\hat{a}\\) y \\(\\hat{z}_0\\), que se denominan la aceleración y la corrección del sesgo: \\[BC_a : (\\hat{\\theta}_{inf}, \\hat{\\theta}_{sup})=(\\hat{\\theta}^*(\\alpha_1), \\hat{\\theta}^*(\\alpha_2))\\] donde \\[\\alpha_1= \\Phi\\bigg(\\hat{z}_0 + \\frac{\\hat{z}_0 + z^{(\\alpha)}}{1- \\hat{a}(\\hat{z}_0 + z^{(\\alpha)})}\\bigg)\\] \\[\\alpha_2= \\Phi\\bigg(\\hat{z}_0 + \\frac{\\hat{z}_0 + z^{(1-\\alpha)}}{1- \\hat{a}(\\hat{z}_0 + z^{(1-\\alpha)})}\\bigg)\\] y \\(\\Phi\\) es la función de distribución acumulada de la distribución normal estándar y \\(z^{\\alpha}\\) es el percentil \\(100 \\cdot \\alpha\\) de una distribución normal estándar. Notemos que si \\(\\hat{a}\\) y \\(\\hat{z}_0\\) son cero entonces \\(\\alpha_1=\\alpha\\) y \\(\\alpha_2=1-\\alpha\\), obteniendo así los intervalos de percentiles. El valor de la corrección por sesgo \\(\\hat{z}_0\\) se obtiene de la propoción de de replicaciones bootstrap menores a la estimación original \\(\\hat{\\theta}\\), \\[z_0=\\Phi^{-1}\\bigg(\\frac{\\#\\{\\hat{\\theta}^*(b) &lt; \\hat{\\theta} \\} }{B} \\bigg)\\] a grandes razgos \\(\\hat{z}_0\\) mide la mediana del sesgo de \\(\\hat{\\theta}^*\\), esto es, la discrepancia entre la mediana de \\(\\hat{\\theta}^*\\) y \\(\\hat{\\theta}\\) en unidades normales. Por su parte la aceleración \\(\\hat{a}\\) se refiere a la tasa de cambio del error estándar de \\(\\hat{\\theta}\\) respecto al verdadero valor del parámetro \\(\\theta\\). La aproximación estándar usual \\(\\hat{\\theta} \\approx N(\\theta, se^2)\\) supone que el error estándar de \\(\\hat{\\theta}\\) es el mismo para toda \\(\\hat{\\theta}\\), esto puede ser poco realista, en nuestro ejemplo, donde \\(\\hat{\\theta}\\) es la varianza si los datos provienen de una normal \\(se(\\hat{\\theta})\\) depende de \\(\\theta\\). Una manera de calcular \\(\\hat{a}\\) es \\[\\hat{a}=\\frac{\\sum_{i=1}^n (\\hat{\\theta}(\\cdot) - \\hat{\\theta}(i))^3}{6\\{\\sum_{i=1}^n (\\hat{\\theta}(\\cdot) - \\hat{\\theta}(i))^2\\}^{3/2}}\\] Los intervalos \\(BC_{a}\\) tienen 2 ventajas teóricas: Respetan transformaciones, esto nos dice que los extremos del intervalo se transforman de manera adecuada si cambiamos el parámetro de interés por una función del mismo. Su exactitud, los intervalos \\(BC_{a}\\) tienen precisión de segundo orden, esto es, los errores de cobertura se van a cero a una tasa de 1/n. Los intervalos \\(BC_{a}\\) están implementados en el paquete boot (boot.ci()) y en el paquete bootstrap (bcanon()). La desventaja de los intervalos \\(BC_{a}\\) es que requieren intenso cómputo estadístico, de acuerdo a Efron and Tibshirani (1993) al menos \\(B= 1000\\) replicaciones son necesairas para reducir el error de muestreo. Ante esto surgen los intervalos ABC (approximate bootstrap confidence intervals), que es un método para aproximar \\(BC_{a}\\) analíticamente (usando expansiones de Taylor). Usando la implementación del paquete bootstrap: library(bootstrap) var_sesgada &lt;- function(x) sum((x - mean(x)) ^ 2) / length(x) bcanon(x = spatial[, 1], nboot = 2000, theta = var_sesgada, alpha = c(0.025, 0.975)) #&gt; $confpoints #&gt; alpha bca point #&gt; [1,] 0.025 104.08 #&gt; [2,] 0.975 281.70 #&gt; #&gt; $z0 #&gt; [1] 0.14717 #&gt; #&gt; $acc #&gt; [1] 0.06124 #&gt; #&gt; $u #&gt; [1] 164.39 176.72 174.52 178.38 172.05 172.05 174.52 172.05 175.96 173.04 #&gt; [11] 168.60 168.20 155.12 141.81 177.93 178.28 177.61 151.02 178.17 177.07 #&gt; [21] 165.88 173.04 177.07 177.84 178.39 173.04 #&gt; #&gt; $call #&gt; bcanon(x = spatial[, 1], nboot = 2000, theta = var_sesgada, alpha = c(0.025, #&gt; 0.975)) Comapara el intervalo anterior con los intervalos normal y de percentiles. Otros intervalos basados en bootstrap incluyen los intervalos pivotales y los intervalos bootstrap-t. Sin embargo, BC y ABC son mejores alternativas. Intervalos pivotales. Sea \\(\\theta=s(P)\\) y \\(\\hat{\\theta}=s(P_n)\\) definimos el pivote \\(R=\\hat{\\theta}-\\theta\\). Sea \\(H(r)\\) la función de distribución acumulada del pivote: \\[H(r) = P(R&lt;r)\\] Definimos \\(C_n^*=(a,b)\\) donde: \\[a=\\hat{\\theta}-H^{-1}(1-\\alpha), b=\\hat{\\theta}-H^{-1}(\\alpha)\\] \\(C_n^*\\) es un intervalo de confianza de \\(1-2\\alpha\\) para \\(\\theta\\); sin embargo, \\(a\\) y \\(b\\) dependen de la distribución desconocida \\(H\\), la podemos estimar usando bootstrap: \\[\\hat{H}(r)=\\frac{1}{B}\\sum_{b=1}^B I(R^*_b \\le r)\\] y obtenemos \\[C_n=(2\\hat{\\theta} - \\hat{\\theta}^*_{1-\\alpha}, 2\\hat{\\theta} + \\hat{\\theta}^*_{1-\\alpha})\\] Exactitud en intervalos de confianza. Un intervalo de \\(95%\\) de confianza exacto no captura el verdadero valor \\(2.5%\\) de las veces, en cada lado. Un intervalo que sub-cubre un lado y sobre-cubre el otro es sesgado. Los intervalos estándar y de percentiles tienen exactitud de primer orden: los errores de cobertura se van a cero a una tasa de \\(1/\\sqrt{n}\\). Los intervalos \\(BC_a\\)normales y de percentiles tienen exactitud de segundo orden: los errores de cobertura se van a cero a una tasa de \\(1/n\\). Referencias "],
["bootstrap-en-r.html", "6.4 Bootstrap en R", " 6.4 Bootstrap en R Es común crear nuestras porpias funciones cuando usamos bootstrap, sin embargo, en R también hay alternativas que pueden resultar convenientes, mencionamos 3: El paquete rsample (forma parte de la colección tidymodels) y tiene una función bootsrtraps(): bootsrtaps() regresa un arreglo cuadrangular (tibble, data.frame) que incluye una columna con las muestras bootstrap y un identificador del número y tipo de muestra. Veamos un ejemplo donde seleccionamos muestras del conjunto de datos muestra_computos que contiene 10,000 observaciones. library(rsample) #&gt; Loading required package: broom #&gt; #&gt; Attaching package: &#39;broom&#39; #&gt; The following object is masked from &#39;package:bootstrap&#39;: #&gt; #&gt; bootstrap #&gt; #&gt; Attaching package: &#39;rsample&#39; #&gt; The following object is masked from &#39;package:tidyr&#39;: #&gt; #&gt; fill muestra_computos &lt;- read_csv(&quot;data/muestra_computos_2012.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; .default = col_integer(), #&gt; TIPO_CASILLA = col_character() #&gt; ) #&gt; See spec(...) for full column specifications. muestra_computos #&gt; # A tibble: 10,000 x 36 #&gt; ID_ESTADO D_DISTRITO SECCION ID_CASILLA TIPO_CASILLA EXT_CONTIGUA #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 15 14 324 1 C 0 #&gt; 2 20 3 207 3 C 0 #&gt; 3 19 9 275 1 B 0 #&gt; 4 8 7 2422 1 B 0 #&gt; 5 30 11 4665 1 C 0 #&gt; 6 13 3 199 1 E 0 #&gt; 7 20 11 2014 1 B 0 #&gt; 8 2 6 739 1 C 0 #&gt; 9 30 12 4348 1 C 0 #&gt; 10 21 11 1121 1 B 0 #&gt; # ... with 9,990 more rows, and 30 more variables: TIPO_CANDIDATURA &lt;int&gt;, #&gt; # CASILLA &lt;int&gt;, ESTATUS_ACTA &lt;int&gt;, ORDEN &lt;int&gt;, #&gt; # LISTA_NOMINAL_CASILLA &lt;int&gt;, ID_GRUPO &lt;int&gt;, TIPO_RECUENTO &lt;int&gt;, #&gt; # NUM_VOTOS_NULOS &lt;int&gt;, NUM_VOTOS_CAN_NREG &lt;int&gt;, #&gt; # NUMERO_VOTOS_VALIDOS &lt;int&gt;, TOTAL_VOTOS &lt;int&gt;, #&gt; # BOLETAS_INUTILIZADAS &lt;int&gt;, PAN &lt;int&gt;, PRI &lt;int&gt;, PRD &lt;int&gt;, #&gt; # PVEM &lt;int&gt;, PT &lt;int&gt;, MC &lt;int&gt;, PANAL &lt;int&gt;, PRI_PVEM &lt;int&gt;, #&gt; # PRD_PT_MC &lt;int&gt;, PRD_PT &lt;int&gt;, PRD_MC &lt;int&gt;, PT_MC &lt;int&gt;, #&gt; # ID_MUNICIPIO &lt;int&gt;, LISTA_NOMINAL &lt;int&gt;, VOTOS_RESERVADOS &lt;int&gt;, #&gt; # pan &lt;int&gt;, pri &lt;int&gt;, prd &lt;int&gt; Generamos 100 muestras bootstrap, y la función nos regresa un arreglo con 100 renglones, cada uno corresponde a una muestra bootstrap. set.seed(839287482) computos_boot &lt;- bootstraps(muestra_computos, times = 100) computos_boot #&gt; # Bootstrap sampling #&gt; # A tibble: 100 x 2 #&gt; splits id #&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 &lt;S3: rsplit&gt; Bootstrap001 #&gt; 2 &lt;S3: rsplit&gt; Bootstrap002 #&gt; 3 &lt;S3: rsplit&gt; Bootstrap003 #&gt; 4 &lt;S3: rsplit&gt; Bootstrap004 #&gt; 5 &lt;S3: rsplit&gt; Bootstrap005 #&gt; 6 &lt;S3: rsplit&gt; Bootstrap006 #&gt; 7 &lt;S3: rsplit&gt; Bootstrap007 #&gt; 8 &lt;S3: rsplit&gt; Bootstrap008 #&gt; 9 &lt;S3: rsplit&gt; Bootstrap009 #&gt; 10 &lt;S3: rsplit&gt; Bootstrap010 #&gt; # ... with 90 more rows La columna splits tiene información de las muestras seleccionadas, para la primera vemos que de 10,000 observaciones en la muestra original la primera muestra bootstrap contiene 10000-3709=6291. first_computos_boot &lt;- computos_boot$splits[[1]] first_computos_boot #&gt; &lt;10000/3709/10000&gt; Y podemos obtener los datos de la muestra bootstrap con la función as.data.frame() as.data.frame(first_computos_boot) #&gt; # A tibble: 10,000 x 36 #&gt; ID_ESTADO D_DISTRITO SECCION ID_CASILLA TIPO_CASILLA EXT_CONTIGUA #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 20 3 1246 3 C 0 #&gt; 2 21 7 9 1 B 0 #&gt; 3 30 2 3609 1 B 0 #&gt; 4 15 3 433 2 C 0 #&gt; 5 7 3 861 1 B 0 #&gt; 6 32 2 565 1 B 0 #&gt; 7 15 36 4402 1 B 0 #&gt; 8 5 5 542 2 C 0 #&gt; 9 16 9 2213 1 C 0 #&gt; 10 5 4 1006 1 E 2 #&gt; # ... with 9,990 more rows, and 30 more variables: TIPO_CANDIDATURA &lt;int&gt;, #&gt; # CASILLA &lt;int&gt;, ESTATUS_ACTA &lt;int&gt;, ORDEN &lt;int&gt;, #&gt; # LISTA_NOMINAL_CASILLA &lt;int&gt;, ID_GRUPO &lt;int&gt;, TIPO_RECUENTO &lt;int&gt;, #&gt; # NUM_VOTOS_NULOS &lt;int&gt;, NUM_VOTOS_CAN_NREG &lt;int&gt;, #&gt; # NUMERO_VOTOS_VALIDOS &lt;int&gt;, TOTAL_VOTOS &lt;int&gt;, #&gt; # BOLETAS_INUTILIZADAS &lt;int&gt;, PAN &lt;int&gt;, PRI &lt;int&gt;, PRD &lt;int&gt;, #&gt; # PVEM &lt;int&gt;, PT &lt;int&gt;, MC &lt;int&gt;, PANAL &lt;int&gt;, PRI_PVEM &lt;int&gt;, #&gt; # PRD_PT_MC &lt;int&gt;, PRD_PT &lt;int&gt;, PRD_MC &lt;int&gt;, PT_MC &lt;int&gt;, #&gt; # ID_MUNICIPIO &lt;int&gt;, LISTA_NOMINAL &lt;int&gt;, VOTOS_RESERVADOS &lt;int&gt;, #&gt; # pan &lt;int&gt;, pri &lt;int&gt;, prd &lt;int&gt; Una de las principales ventajas de usar este paquete es que es eficiente en el uso de memoria. library(pryr) #&gt; #&gt; Attaching package: &#39;pryr&#39; #&gt; The following objects are masked from &#39;package:purrr&#39;: #&gt; #&gt; compose, partial object_size(muestra_computos) #&gt; 1.49 MB object_size(computos_boot) #&gt; 5.58 MB # tamaño por muestra object_size(computos_boot)/nrow(computos_boot) #&gt; 55.8 kB # el incremento en tamaño es &lt;&lt; 100 as.numeric(object_size(computos_boot)/object_size(muestra_computos)) #&gt; [1] 3.7385 El paquete boot está asociado al libro Bootstrap Methods and Their Applications (Davison and Hinkley (1997)) y tiene, entre otras, funciones para calcular replicaciones bootstrap y para construir intervalos de confianza usando bootstrap: calculo de replicaciones bootstrap con la función boot(), intervalos normales, de percentiles y \\(BC_a\\) con la función boot.ci(), intevalos ABC con la función `abc.ci(). El paquete bootsrtap contiene datos usados en Efron and Tibshirani (1993), y la implementación de funciones para calcular replicaciones y construir intervalos de confianza: calculo de replicaciones bootstrap con la función bootstrap(), intervalos \\(BC_a\\) con la función bcanon(), intevalos ABC con la función `abcnon(). Referencias "],
["conclusiones-y-observaciones.html", "6.5 Conclusiones y observaciones", " 6.5 Conclusiones y observaciones El principio fundamental del Bootstrap no paramétrico es que podemos estimar la distribución poblacional con la distribución empírica. Por tanto para hacer inferencia tomamos muestras con reemplazo de la distribución empírica y analizamos la variación de la estadística de interés a lo largo de las muestras. El bootstrap nos da la posibilidad de crear intervalos de confianza cuando no contamos con fórmulas para hacerlo de manera analítica y sin supuestos distribucionales de la población. Hay muchas opciones para construir intervalos bootstrap, los que tienen mejores propiedades son los intervalos \\(BC_a\\), sin embargo los más comunes son los intervalos normales con error estándar bootstrap y los intervalos de percentiles de la distribución bootstrap. Antes de hacer intervalos normales vale la pena graficar la distribución bootstrap y evaluar si el supuesto de normalidad es razonable. En cuanto al número de muestras bootstrap se recomienda al menos \\(1,000\\) al hacer pruebas, y \\(10,000\\) o \\(15,000\\) para los resultados finales, sobre todo cuando se hacen intervalos de confianza de percentiles. La función de distribución empírica es una mala estimación en las colas de las distribuciones, por lo que es difícil construir intervalos de confianza (usando bootstrap no paramétrico) para estadísticas que dependen mucho de las colas. "],
["teoria-basica-de-simulacion.html", "Sección 7 Teoría básica de simulación", " Sección 7 Teoría básica de simulación “Any one who considers arithmetical methods of producing random digits is, of course, in a state of sin.” -Von Neuman La simulación de un modelo consiste en la construcción de un programa computacional que permite obtener los valores de las varibles de salida para distintos valores de las variables de entrada con el objetivo de obtener conclusiones del sistema que apoyen la toma de decisiones (explicar y/o predecir el comportamiento del sistema). Requerimientos prácticos de un proyecto de simulación: Fuente de números aleatorios \\(U(0,1)\\) (números pseudoaleatorios). Transformar los números aleatorios en variables de entrada del modelo (e.g. generación de muestras con cierta distribución). Construir el programa computacional de simulación. Analizar las distintas simulaciones del modelo para obtener conclusiones acerca del sistema. "],
["numeros-pseudoaleatorios.html", "7.1 Números pseudoaleatorios", " 7.1 Números pseudoaleatorios El objetivo es generar sucesiones \\(\\{u_i\\}_{i=1}^N\\) de números independientes que se puedan considerar como observaciones de una distribución uniforme en el intervalo \\((0, 1)\\). Verdaderos números aleatorios. Los números completamente aleatorios (no determinísticos) son fáciles de imaginar conceptualmente, por ejemplo podemos imaginar lanzar una moneda, lanzar un dadoo una lotería. En general los números aleatorios se basan en alguna fuente de aleatoreidad física que puede ser teóricamente impredecible (cuántica) o prácticamente impredecible (caótica). Por ejemplo: random.org genera aleatoreidad a través de ruido atmosférico (el paquete random contiene funciones para obtener números de random.org), ERNIE, usa ruido térmico en transistores y se utiliza en la lotería de bonos de Reino Unido. RAND Corporation En 1955 publicó una tabla de un millón de números aleatorios que fue ampliamente utilizada. Los números en la tabla se obtuvieron de una ruleta electrónica. La desventaja de éstos métodos es que son costosos, tardados y no reproducibles. Números pseudoaleatorios. Los números pseudoaleatorios se generan de manera secuencial con un algoritmo determinístico, formalmente se definen por: Función de inicialización. Recibe un número (la semilla) y pone al generador en su estado inicial. Función de transición. Transforma el estado del generador. Función de salidas. Transforma el estado para producir un número fijo de bits (0 ó 1). Una sucesión de bits pseudoaleatorios se obtiene definiendo la semilla y llamando repetidamente la función de transición y la función de salidas. Esto implica, entre otras cosas, que una sucesión de números pseudoaletorios esta completamente determinada por la semilla. Buscamos que una secuencia de números pseudoaleatorios: no muestre ningún patrón o regularidad aparente desde un punto de vista estadístico, y dada una semilla inicial, se puedan generar muchos valores antes de repetir el ciclo. Construir un buen algoritmo de números pseudo aleatorios es complicado, como veremos en los siguientes ejemplos. 7.1.0.1 Ejemplo: método de la parte media del cuadrado En 1946 Jon Von Neuman sugirió usar las operaciones aritméticas de una computadora para generar secuencias de número pseudoaleatorios. Sugirió el método middle square, para generar secuencias de dígitos pseudoaleatorios de 4 dígitos propuso: Se inicia con una semilla de 4 dígitos. seed = 9731 La semilla se eleva al cuadrado, produciendo un número de 8 dígitos (si el resultado tiene menos de 8 dígitos se añaden ceros al inicio). value = 94692361 Los 4 números del centro serán el siguiente número en la secuencia, y se devuelven como resultado. seed = 6923 mid_square &lt;- function(seed, n) { seeds &lt;- numeric(n) values &lt;- numeric(n) for(i in 1:n) { x &lt;- seed ^ 2 seed = case_when( nchar(x) &gt; 2 ~ (x %/% 1e2) %% 1e4, TRUE ~ 0) values[i] &lt;- x seeds[i] &lt;- seed } cbind(seeds, values) } x &lt;- mid_square(1931, 10) print(x, digits = 4) #&gt; seeds values #&gt; [1,] 7287 3728761 #&gt; [2,] 1003 53100369 #&gt; [3,] 60 1006009 #&gt; [4,] 36 3600 #&gt; [5,] 12 1296 #&gt; [6,] 1 144 #&gt; [7,] 0 1 #&gt; [8,] 0 0 #&gt; [9,] 0 0 #&gt; [10,] 0 0 x &lt;- mid_square(9731, 100) print(x, digits = 4) #&gt; seeds values #&gt; [1,] 6923 94692361 #&gt; [2,] 9279 47927929 #&gt; [3,] 998 86099841 #&gt; [4,] 9960 996004 #&gt; [5,] 2016 99201600 #&gt; [6,] 642 4064256 #&gt; [7,] 4121 412164 #&gt; [8,] 9826 16982641 #&gt; [9,] 5502 96550276 #&gt; [10,] 2720 30272004 #&gt; [11,] 3984 7398400 #&gt; [12,] 8722 15872256 #&gt; [13,] 732 76073284 #&gt; [14,] 5358 535824 #&gt; [15,] 7081 28708164 #&gt; [16,] 1405 50140561 #&gt; [17,] 9740 1974025 #&gt; [18,] 8676 94867600 #&gt; [19,] 2729 75272976 #&gt; [20,] 4474 7447441 #&gt; [21,] 166 20016676 #&gt; [22,] 275 27556 #&gt; [23,] 756 75625 #&gt; [24,] 5715 571536 #&gt; [25,] 6612 32661225 #&gt; [26,] 7185 43718544 #&gt; [27,] 6242 51624225 #&gt; [28,] 9625 38962564 #&gt; [29,] 6406 92640625 #&gt; [30,] 368 41036836 #&gt; [31,] 1354 135424 #&gt; [32,] 8333 1833316 #&gt; [33,] 4388 69438889 #&gt; [34,] 2545 19254544 #&gt; [35,] 4770 6477025 #&gt; [36,] 7529 22752900 #&gt; [37,] 6858 56685841 #&gt; [38,] 321 47032164 #&gt; [39,] 1030 103041 #&gt; [40,] 609 1060900 #&gt; [41,] 3708 370881 #&gt; [42,] 7492 13749264 #&gt; [43,] 1300 56130064 #&gt; [44,] 6900 1690000 #&gt; [45,] 6100 47610000 #&gt; [46,] 2100 37210000 #&gt; [47,] 4100 4410000 #&gt; [48,] 8100 16810000 #&gt; [49,] 6100 65610000 #&gt; [50,] 2100 37210000 #&gt; [51,] 4100 4410000 #&gt; [52,] 8100 16810000 #&gt; [53,] 6100 65610000 #&gt; [54,] 2100 37210000 #&gt; [55,] 4100 4410000 #&gt; [56,] 8100 16810000 #&gt; [57,] 6100 65610000 #&gt; [58,] 2100 37210000 #&gt; [59,] 4100 4410000 #&gt; [60,] 8100 16810000 #&gt; [61,] 6100 65610000 #&gt; [62,] 2100 37210000 #&gt; [63,] 4100 4410000 #&gt; [64,] 8100 16810000 #&gt; [65,] 6100 65610000 #&gt; [66,] 2100 37210000 #&gt; [67,] 4100 4410000 #&gt; [68,] 8100 16810000 #&gt; [69,] 6100 65610000 #&gt; [70,] 2100 37210000 #&gt; [71,] 4100 4410000 #&gt; [72,] 8100 16810000 #&gt; [73,] 6100 65610000 #&gt; [74,] 2100 37210000 #&gt; [75,] 4100 4410000 #&gt; [76,] 8100 16810000 #&gt; [77,] 6100 65610000 #&gt; [78,] 2100 37210000 #&gt; [79,] 4100 4410000 #&gt; [80,] 8100 16810000 #&gt; [81,] 6100 65610000 #&gt; [82,] 2100 37210000 #&gt; [83,] 4100 4410000 #&gt; [84,] 8100 16810000 #&gt; [85,] 6100 65610000 #&gt; [86,] 2100 37210000 #&gt; [87,] 4100 4410000 #&gt; [88,] 8100 16810000 #&gt; [89,] 6100 65610000 #&gt; [90,] 2100 37210000 #&gt; [91,] 4100 4410000 #&gt; [92,] 8100 16810000 #&gt; [93,] 6100 65610000 #&gt; [94,] 2100 37210000 #&gt; [95,] 4100 4410000 #&gt; [96,] 8100 16810000 #&gt; [97,] 6100 65610000 #&gt; [98,] 2100 37210000 #&gt; [99,] 4100 4410000 #&gt; [100,] 8100 16810000 Este generador cae rápidamente en cilcos cortos, por ejemplo, si aparece un cero se propagará por siempre. A inicios de 1950s se exploró el método y se propusieron mejoras, por ejemplo para evitar caer en cero. Metrópolis logró obtener una secuencia de 750,000 números distintos al usar semillas de 38 bits (usaba sistema binario), además la secuencia de Metrópolis mostraba propiedades deseables. No obstante, el método del valor medio no es considerado un método bueno por lo común de los ciclos cortos. Ejemplo: rand Por muchos años (antes de 1995) el generador de la función rand en Matlab fue el generador congruencial: \\[X_{n+1} = (7^5)X_n mod(2^{31}-1)\\] Construyamos sucesiones de longitud \\(1,500\\) usando el algoritmo de rand: sucesion &lt;- function(n = 1500, semilla = runif(1, 0, 2 ^ 31 - 1)){ x &lt;- rep(NA, n) u &lt;- rep(NA, n) x[1] &lt;- semilla u[1] &lt;- x[1] / (2 ^ 31 - 1) # transformamos al (0, 1) for(i in 2:n){ x[i] &lt;- (7 ^ 5 * x[i - 1]) %% (2 ^ 31 - 1) u[i] &lt;- x[i] / (2 ^ 31 - 1) } u } u_rand &lt;- sucesion(n = 150000) sucesiones &lt;- map_df(1:12, ~data_frame(serie = ., sim = sucesion(), ind = 1:length(sim))) sucesiones #&gt; # A tibble: 18,000 x 3 #&gt; serie sim ind #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1 0.582 1 #&gt; 2 1 0.807 2 #&gt; 3 1 0.450 3 #&gt; 4 1 0.257 4 #&gt; 5 1 0.372 5 #&gt; 6 1 0.685 6 #&gt; 7 1 0.465 7 #&gt; 8 1 0.492 8 #&gt; 9 1 0.566 9 #&gt; 10 1 0.602 10 #&gt; # ... with 17,990 more rows Una propiedad deseable es que la sucesión de \\(u_i\\) parezca una sucesión de observaciones independientes de una \\(Uniforme(0,1)\\). Veamos una gráfica del índice de simulación contra el valor obtenido ggplot(sucesiones, aes(x = ind, y = sim)) + geom_point(alpha = 0.5, size = 1.5) + # alpha controla la transparencia facet_wrap(~ serie) + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;white&quot;, size = 0.7) Comparemos con los cuantiles de una uniforme: ggplot(sucesiones) + stat_qq(aes(sample = sim), distribution = qunif) + geom_abline(color = &quot;white&quot;, size = 0.6, alpha = 0.6) + facet_wrap(~ serie) Ejemplo: RANDU RANDU fue generador de números aleatorios ampliamente utilizado en los 60´s y 70´s, se define como: \\[X_{n + 1}= (2 ^ {16} + 3)X_n mod(2^{31})\\] A primera vista las sucesiones se asemejan a una uniforme, sin embargo, cuando se grafican ternas emergen patrones no deseados. library(tourr) library(plotly) n &lt;- 1500 # longitud de la sucesión x &lt;- rep(NA, n) u &lt;- rep(NA, n) x[1] &lt;- 4798373 # semilla u[1] &lt;- x[1] / (2 ^ 31 - 1) # transformamos al (0, 1) for(i in 2:n){ x[i] &lt;- ((2 ^ 16 + 3) * x[i - 1]) %% (2 ^ 31) u[i] &lt;- x[i] / (2 ^ 31) } u_randu &lt;- u set.seed(8111938) mat &lt;- matrix(u, ncol = 3, byrow = TRUE) tour &lt;- new_tour(mat, grand_tour(), NULL) steps &lt;- seq(0, 1, 0.01) names(steps) &lt;- steps mat_xy &lt;- map_dfr(steps, ~data.frame(center(mat %*% tour(.)$proj)), .id = &quot;steps&quot;) # step 0.72 mat_xy %&gt;% mutate(steps = as.numeric(steps)) %&gt;% plot_ly(x = ~X1, y = ~X2, frame = ~steps, type = &#39;scatter&#39;, mode = &#39;markers&#39;, showlegend = F, marker = list(size = 5, color = &quot;black&quot;), opacity=0.5) %&gt;% animation_opts(frame = 250) Veamos los resultados enteros del generador, ¿qué notas? options(digits=5) n &lt;- 50 x &lt;- rep(NA, n) x[1] &lt;- 1 # semilla u[1] &lt;- x[1] # transformamos al (0, 1) for(i in 2:n){ x[i] &lt;- ((2 ^ 16 + 3) * x[i - 1]) %% (2 ^ 31) } x #&gt; [1] 1 65539 393225 1769499 7077969 26542323 #&gt; [7] 95552217 334432395 1146624417 1722371299 14608041 1766175739 #&gt; [13] 1875647473 1800754131 366148473 1022489195 692115265 1392739779 #&gt; [19] 2127401289 229749723 1559239569 845238963 1775695897 899541067 #&gt; [25] 153401569 1414474403 663781353 1989836731 1670020913 701529491 #&gt; [31] 2063890617 1774610987 662584961 888912771 1517695625 1105958811 #&gt; [37] 1566426833 1592415347 1899101529 1357838347 1792534561 682145891 #&gt; [43] 844966185 1077967739 1010594417 656824147 1288046073 1816859115 #&gt; [49] 1456223681 975544643 Generadores congruenciales y Mersenne-Twister Los generadores como rand y RANDU (\\(X_{n+1} = (7^5)X_n mod(2^{31}-1)\\) y \\(X_{n+1}= (2 ^ {16} + 3)X_n mod(2^{31})\\)) se denominan generadores congruenciales. Los Generadores Congruenciales Lineales (GCL) tienen la forma \\[X_{n+1} = (aX_n + c)mod(m)\\] Están determinados por los parámetros: * Módulo: \\(m &gt; 0\\) * Multiplicador \\(0\\le a &lt; m\\) * Incremento \\(c \\le m\\) * Semilla \\(0\\le X_0 &lt; m\\) Los GCL se introdujeron en 1949 por D.H. Lehemer y son muy populares. La elección de los parámetros determina la calidad del generador: Queremos \\(m\\) grande pues el periodo (longitud del ciclo) del generador no puede tener más de \\(m\\) elementos. Queremos velocidad, en este caso, un valor conveniente para \\(m\\) es el tamaño de palabra (máximo número de bits que puede procesar el CPU en un ciclo) de la computadora. Los GCL más eficientes tienen \\(m\\) igual a una potencia de 2 (es usual 232 o 264) de esta manera la operación módulo se calcula truncando todos los dígitos excepto los últimos 32 ó 64 bits. ¿podemos elegir \\(a\\) y \\(c\\) de tal manera que logremos alcanzar el periodo máximo (\\(m\\))? Un generador congruencial mixto (\\(c&gt;0\\)) tendrá periodo completo para todas las semillas sí y sólo sí: \\(m\\) y \\(c\\) son primos relativos. \\(a-1\\) es divisible por todos los factores primos de \\(m\\). \\(a-1\\) es divisible por 4 si \\(m\\) es divisible por 4. Vale la pena notar que un periodo grande no determina que el generador congruencial es bueno, debemos verificar que los números que generan se comportan como si fueran aleatorios. Los GCLs tienden a exhibir defectos, por ejemplo, si se utiliza un GCL para elegir puntos en un espacio de dimensión \\(k\\) los puntos van a caer en a lo más \\((k!m)^{1/k}\\) hiperplanos paralelos \\(k\\) dimensionales (como observamos con RANDU), donde \\(k\\) se refiere a la dimensión de \\([0,1]^k\\). Los GCLs continuan siendo utilizados en muchas aplicaciones porque con una elección cuidadosa de los parámetros pueden pasar muchas pruebas de aleatoriedad, son rápidos y requiren poca memoria. Un ejemplo es Java, su generador de números aleatorios default java.util.Random es un GCL con multiplicador \\(a=25214903917\\), incremento \\(c=11\\) y módulo \\(m=2^{48}\\). , sin embargo, actualmente el generador default de R es el Mersenne-Twister que no pertenece a la clase de GCLs (se puede elegir usar otros generadores para ver los disponible teclea ?Random). El generador Mersenne-Twister se desarrolló en 1997 por Makoto Matsumoto y Takuji Nishimura (Matsumoto and Nishimura (1998)), es el generador default en muchos programas, por ejemplo, en Python, Ruby, C++ estándar, Excel y Matlab (más aquí). Este generador tiene propiedades deseables como un periodo largo (2^19937-1) y el hecho que pasa muchas pruebas de aleatoriedad. Pruebas de aleatoriedad Hasta ahora hemos graficado las secuencias de números aleatorios para evaluar su aleatoriedad, sin embargo, el ojo humano no es muy bueno discriminando aleatoriedad y las gráficas no escalan. Es por ello que resulta conveniente hacer pruebas estadísticas para evaluar la calidad de los generadores de números pseudoaleatorios. Hay dos tipos de pruebas: empíricas: evalúan estadísticas de sucesiones de números. teóricas: se establecen las características de las sucesiones usando métodos de teoría de números con base en la regla de recurrencia que generó la sucesión. Veremos 2 ejemplos de la primera clase: Ejemplo: prueba de bondad de ajuste \\(\\chi^2\\) \\(H_0:\\) Los datos son muestra de una cierta distribución \\(F\\). \\(H_1:\\) Los datos no son una muestra de \\(F\\). Procedimiento: Partir el soporte de \\(F\\) en \\(c\\) celdas que son exhaustivas y mutuamente exculyentes. Contar el número de observaciones en cada celda \\(O_i\\). Calcular el valor esperado en cada celda bajo \\(F\\): \\(e_i=np_i\\). Calcular la estadística de prueba: \\[\\chi^2 = \\sum_{i=1}^c \\frac{(O_i - e_i)^2}{e_i} \\sim \\chi^2_{c-k-1}\\] donde \\(c\\) es el número de celdas y \\(k\\) el número de parámetros estimados en \\(F\\) a partir de los datos observados. u_rand_cat &lt;- cut(u_rand, breaks = seq(0, 1, 0.1)) u_randu_cat &lt;- cut(u_randu, breaks = seq(0, 1, 0.1)) u_mt &lt;- runif(150000) u_mt_cat &lt;- cut(u_mt, breaks = seq(0, 1, 0.1)) chisq.test(table(u_rand_cat)) #&gt; #&gt; Chi-squared test for given probabilities #&gt; #&gt; data: table(u_rand_cat) #&gt; X-squared = 16.7, df = 9, p-value = 0.054 chisq.test(table(u_randu_cat)) #&gt; #&gt; Chi-squared test for given probabilities #&gt; #&gt; data: table(u_randu_cat) #&gt; X-squared = 10.2, df = 9, p-value = 0.34 chisq.test(table(u_mt_cat)) #&gt; #&gt; Chi-squared test for given probabilities #&gt; #&gt; data: table(u_mt_cat) #&gt; X-squared = 17.3, df = 9, p-value = 0.044 Una variación de esta prueba de bondad de ajuste \\(\\chi^2\\), es la prueba de u uniformidad k-dimensional: \\(H_0:\\) Distribución uniforme en \\([0,1]^k\\), con \\(k = 1,2,...\\) En este caso se divide el espacio \\([0,1]^k\\) en celdas exhaustivas y mutuamente excluyentes, y se aplica la prueba \\(\\chi^2\\) a los vectores sucesivos \\((u_1,u_2,...,u_k),(u_{k+1},u_{k+2},...,u_{2k}),...\\) 7.1.0.2 Ejemplo: prueba de espera Busca probar independencia y uniformidad Procedimiento: 1. Seleccionar un subintervalo del \\((0,1)\\). 2. Calcular la probabilidad del subintervalo. 3. Ubicar en la sucesión las posiciones de los elementos que pertenezcan al subintervalo. 4. Calcular el número de elementos consecutivos de la sucesión entre cada una de las ocurrencias consecutivas de elementos del subintervalo (tiempos de espera). 5. La distribución de los tiempos de espera es geométrica con parámetro calculado en 2. 6. Aplicar una prueba \\(\\chi^2\\) a los tiempos de espera. library(randtoolbox) #&gt; Loading required package: rngWELL #&gt; This is randtoolbox. For overview, type &#39;help(&quot;randtoolbox&quot;)&#39;. gap.test(u_mt) #&gt; #&gt; Gap test #&gt; #&gt; chisq stat = 14, df = 17, p-value = 0.64 #&gt; #&gt; (sample size : 150000) #&gt; #&gt; length observed freq theoretical freq #&gt; 1 18746 18750 #&gt; 2 9450 9375 #&gt; 3 4655 4688 #&gt; 4 2337 2344 #&gt; 5 1170 1172 #&gt; 6 610 586 #&gt; 7 289 293 #&gt; 8 139 146 #&gt; 9 65 73 #&gt; 10 45 37 #&gt; 11 16 18 #&gt; 12 7 9.2 #&gt; 13 2 4.6 #&gt; 14 6 2.3 #&gt; 15 1 1.1 #&gt; 16 0 0.57 #&gt; 17 0 0.29 #&gt; 18 0 0.14 gap.test(u_randu) #&gt; #&gt; Gap test #&gt; #&gt; chisq stat = 6.6, df = 10, p-value = 0.77 #&gt; #&gt; (sample size : 1500) #&gt; #&gt; length observed freq theoretical freq #&gt; 1 209 188 #&gt; 2 93 94 #&gt; 3 43 47 #&gt; 4 21 23 #&gt; 5 11 12 #&gt; 6 6 5.9 #&gt; 7 3 2.9 #&gt; 8 0 1.5 #&gt; 9 0 0.73 #&gt; 10 1 0.37 #&gt; 11 0 0.18 Otras pruebas de aleatoriedad son prueba de rachas, Kolmogorov-Smirnov, prueba de poker, puedes leer más de generadores aleatorios y pruebas en Knuth (1997). En R hay pruebas implementadas en los paquetes randtoolboxy RDieHarder. Referencias "],
["variables-aleatorias-1.html", "7.2 Variables aleatorias", " 7.2 Variables aleatorias El segundo requisito práctico de un proyecto de simulación es: Transformar los números aleatorios en variables de entrada del modelo (e.g. generación de muestras con cierta distribución). En nuestro caso, usamos simulación aplicada a modelos estadísticos: Un modelo estadístico \\(F\\) es un conjuto de distribuciones (o densidades o funciones de regresión). Un modelo paramétrico es un conjunto \\(F\\) que puede ser parametrizado por un número finito de parámetros. Por ejemplo, si suponemos que los datos provienen de una distribución Normal, el modelo es: \\[F=\\bigg\\{p(x;\\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp\\bigg(-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\bigg), \\mu \\in \\mathbb{R}, \\sigma&gt;0\\bigg\\}\\] En general, un modelo paramétrico tiene la forma \\[F=\\bigg\\{p(x;\\theta):\\theta \\in \\Theta \\bigg\\}\\] donde \\(\\theta\\) es un parámetro desconocido (o un vector de parámetros) que puede tomar valores en el espacio paramétrico \\(\\Theta\\). En lo que resta de esta sección estudiaremos simulación de modelos paramétricos. Comencemos repasando algunos conceptos: Una variable aleatoria es un mapeo entre el espacio de resultados y los números reales Ejemplo. Lanzamos una moneda justa dos veces, definimos \\(X\\) como en el número de soles, entonces la variable aleatoria se pueden resumir como: \\(\\omega\\) \\(P(\\{\\omega\\})\\) \\(X(\\omega)\\) AA 1/4 0 AS 1/4 1 SA 1/4 1 SS 1/4 2 La función de distribución acumulada es la función \\(P_X:\\mathbb{R}\\to[0,1]\\) definida como: \\[P_X(x)=P(X\\leq x)\\] En el ejemplo: \\[ P_X(x) = \\left\\{ \\begin{array}{lr} 0 &amp; x &lt; 0\\\\ 1/4 &amp; 0 \\leq x &lt; 1 \\\\ 3/4 &amp; 1 \\leq x &lt; 2 \\\\ 1 &amp; x \\ge 2 \\end{array} \\right. \\] Una variable aleatoria \\(X\\) es discreta si toma un número contable de valores \\(\\{x_1,x_2,...\\}\\). En este caso definimos la función de probabilidad o la función masa de probabilidad de X como \\(p_X(x)=P(X=x)\\). Notemos que \\(p_X(x)\\geq 0\\) para toda \\(x \\in \\mathbb{R}\\) y \\(\\sum_i p_X(x)=1\\). Más aún, la función de distribución acumulada esta relacionada con \\(p_X\\) por \\[P_X(x)=P(X \\leq x)= \\sum_{x_i\\leq x} = \\sum_{x_i\\leq x}p_{X}(x_i)\\] \\[ p_X(x) = \\left\\{ \\begin{array}{lr} 1/4 &amp; x = 0 \\\\ 1/2 &amp; x = 1 \\\\ 1/4 &amp; x = 2\\\\ 0 &amp; e.o.c. \\end{array} \\right. \\] Sea \\(X\\) una variable aleatoria con FDA \\(P_X\\). La función de distribución acumulada inversa o función de cuantiles se define como: \\[P_X^{-1}(q) = inf\\{x:P_X(x)&gt;q\\}\\] para \\(q \\in [0,1]\\). Llamamos a \\(P^{-1}(1/4)\\) el primer cuartil, a \\(P^{-1}(1/2)\\) la mediana y \\(P^{-1}(3/4)\\) el tercer cuartil. Familias discretas importantes Muchas variables aleatorias provienen de familias o tipos de experimentos similares lo que nos ahorra tener que determinar las funciones de distribución y sus propiedades cada vez. Por ejemplo, el resultado de interés en muchos experimentos es un resultado que solo puede tomar dos valores: una moneda puede ser águila o sol, un persona puede estar empleada o desempleada, un transistor puede estar defectuoso o no,… La misma distribución de probabilidad que describe a una variable aleatoria que puede tomar el valor de \\(1\\) con probabilidad \\(p\\) y \\(0\\) con probabilidad \\(q=1-p\\), se conoce como distribución Bernoulli. Distribución Bernoulli Sea \\(X\\) la variable aleatoria que representa un lanzamiento de moneda, con \\(P(X=1)=p\\) y \\(P(X=0)=1-p\\) para alguna \\(p\\in[0,1].\\) Decimos que \\(X\\) tiene una distribución Bernoulli (\\(X \\sim Bernoulli(p)\\)), y su función de distribución es: \\[ p(x) = \\left\\{ \\begin{array}{lr} p^x(1-p)^{1-x} &amp; x \\in \\{0,1\\}\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X) = p, Var(X)=p(1-p)\\) Notemos que un experimento Bernoulli es una repetición del experimento que involucra solo dos posibles salidas. Es común que nos interese el resultado de la repetición de experimentos Bernoulli independientes, en este caso se usa la distribución Binomial. Distribución Binomial Supongamos que tenemos una moneda que cae en sol con probabilidad \\(p\\), para alguna \\(p\\in[0,1].\\) Lanzamos la moneda \\(n\\) veces y sea \\(X\\) el número de soles. Suponemos que los lanzamientos son independientes, entonces la función de distribución es: \\[ p(x) = \\left\\{ \\begin{array}{lr} {n \\choose x}p^x(1-p)^{n-x} &amp; x \\in \\{0,1,...,n\\}\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X) = np, Var(X)=np(1-p)\\) Si \\(X_1 \\sim Binomial(n_1, p)\\) y \\(X_2 \\sim Binomial(n_2,p)\\) entonces \\(X_1 + X_2 \\sim Binomial(n_1+n_2, p)\\). En general la distribución binomial describe el comportamiento de una variable \\(X\\) que cuenta número de éxitos tal que: 1) el número de observaciones \\(n\\) esta fijo, 2) cada observación es independiente, 3) cada observación representa uno de dos posibles eventos (éxito o fracaso) y 3) la probabilidad de éxito \\(p\\) es la misma en cada observación. densidades &lt;- ggplot(data.frame(x = -1:20)) + geom_point(aes(x = x, y = dbinom(x, size = 20, prob = 0.5), color = &quot;n=20;p=0.5&quot;), show.legend = FALSE) + geom_path(aes(x = x, y = dbinom(x, size = 20, prob = 0.5), color = &quot;n=20;p=0.5&quot;), alpha = 0.6, linetype = &quot;dashed&quot;, show.legend = FALSE) + geom_point(aes(x = x, y = dbinom(x, size = 20, prob = 0.1), color = &quot;n=20;p=0.1&quot;), show.legend = FALSE) + geom_path(aes(x = x, y = dbinom(x, size = 20, prob = 0.1), color = &quot;n=20;p=0.1&quot;), alpha = 0.6, linetype = &quot;dashed&quot;, show.legend = FALSE) + labs(color = &quot;&quot;, y = &quot;&quot;, title = &quot;Distribución binomial&quot;) dists &lt;- ggplot(data_frame(x = -1:20), aes(x)) + stat_function(fun = pbinom, args = list(size = 20, prob = 0.5), aes(colour = &quot;n=20;p=0.5&quot;), alpha = 0.8) + stat_function(fun = pbinom, args = list(size = 20, prob = 0.1), aes(colour = &quot;n=20;p=0.1&quot;), alpha = 0.8) + labs(y = &quot;&quot;, title = &quot;FDA&quot;, color = &quot;&quot;) grid.arrange(densidades, dists, ncol = 2, newpage = FALSE) Distribución Uniforme Decimos que \\(X\\) tiene una distribución uniforme en \\(\\{a,...,b\\}\\) (\\(a,b\\) enteros) si tiene una función de probailidad dada por: \\[ p(x) = \\left\\{ \\begin{array}{lr} 1/n &amp; x \\in \\{a,...,b\\}\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] donde \\(n = b-a+1\\), \\(E(X) = (a+b)/2, Var(X)=(n^2-1)/12\\) El ejemplo más común es el lanzamiento de un dado. Distribución Poisson \\(X\\) tienen una distribución Poisson con prámetro \\(\\lambda\\) si \\[ p(x) = \\left\\{ \\begin{array}{lr} e^{-\\lambda} \\frac{\\lambda^x}{x!} &amp; x \\in \\{0,1,...\\}\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X) = \\lambda, Var(X)=\\lambda\\) La distribución Poisson se utiliza con frecuencia para modelar conteos de eventos raros, por ejemplo número de accidentes de tráfico. La distribución Poisson es un caso límite de la distribución binomial cuando el número de casos es muy grande y la probabilidad de éxito \\(p\\) es chica. Una propiedad de la distribución Poisson es: \\(X_1 \\sim Poisson(\\lambda_1)\\) y \\(X_2 \\sim Poisson(\\lambda_2)\\) entonces \\(X_1 + X_2 \\sim Poisson(\\lambda_1 + \\lambda_2)\\). densidades &lt;- ggplot(data.frame(x = -1:20)) + geom_point(aes(x = x, y = dpois(x, lambda = 4), color = &quot;lambda=4&quot;), show.legend = FALSE) + geom_path(aes(x = x, y = dpois(x, lambda = 4), color = &quot;lambda=4&quot;), alpha = 0.6, linetype = &quot;dashed&quot;, show.legend = FALSE) + geom_point(aes(x = x, y = dpois(x, lambda = 10), color = &quot;lambda=10&quot;), show.legend = FALSE) + geom_path(aes(x = x, y = dpois(x, lambda = 10), color = &quot;lambda=10&quot;), alpha = 0.6, linetype = &quot;dashed&quot;, show.legend = FALSE) + labs(color = &quot;&quot;, y = &quot;&quot;, title = &quot;Distribución Poisson&quot;) dists &lt;- ggplot(data_frame(x = -1:20), aes(x)) + stat_function(fun = ppois, args = list(lambda = 4), aes(colour = &quot;lambda=4&quot;), alpha = 0.8) + stat_function(fun = ppois, args = list(lambda = 10), aes(colour = &quot;lambda=10&quot;), alpha = 0.8) + labs(y = &quot;&quot;, title = &quot;FDA&quot;, color = &quot;&quot;) grid.arrange(densidades, dists, ncol = 2, newpage = FALSE) Distribución geométrica \\(X\\) tiene distribución geométrica con parámetro \\(p \\in (0,1)\\), \\(X \\sim Geom(p)\\) si, \\[ p(x) = \\left\\{ \\begin{array}{lr} p(1-p)^{k-1} &amp; x \\in \\{1,2,...\\}\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X)=1/p, Var(X)=(1-p)/p^2\\) con \\(k \\geq 1\\). Podemos pensar en \\(X\\) como el número de lanzamientos necesarios hasta que obtenemos el primer sol en los lanzamientos de una moneda. densidades &lt;- ggplot(data.frame(x = -1:20)) + geom_point(aes(x = x, y = dgeom(x, p = 0.5), color = &quot;p=0.5&quot;), show.legend = FALSE) + geom_path(aes(x = x, y = dgeom(x, p = 0.5), color = &quot;p=0.5&quot;), show.legend = FALSE, alpha = 0.6, linetype = &quot;dashed&quot;) + geom_point(aes(x = x, y = dgeom(x, p = 0.1), color = &quot;p=0.1&quot;), show.legend = FALSE) + geom_path(aes(x = x, y = dgeom(x, p = 0.1), color = &quot;p=0.1&quot;), show.legend = FALSE, alpha = 0.6, linetype = &quot;dashed&quot;) + labs(title = &quot;Distribución geométrica&quot;, y = &quot;&quot;) dists &lt;- ggplot(data_frame(x = -1:20), aes(x)) + stat_function(fun = pgeom, args = list(p = 0.5), aes(colour = &quot;p=0.5&quot;), alpha = 0.8) + stat_function(fun = pgeom, args = list(p = 0.1), aes(colour = &quot;p=0.1&quot;), alpha = 0.8) + labs(y = &quot;&quot;, title = &quot;FDA&quot;, color = &quot;&quot;) grid.arrange(densidades, dists, ncol = 2, newpage = FALSE) Variables aleatorias continuas Una variable aleatoria \\(X\\) es continua si existe una función \\(p_x\\) tal que \\(p_X(x) \\geq 0\\) para toda \\(x\\), \\(\\int_{-\\infty}^{\\infty}p_X(x)dx=1\\) y para toda \\(a\\leq b\\), \\[P(a &lt; X &lt; b) = \\int_{a}^b p_X(x)dx\\] La función \\(p_X(x)\\) se llama la función de densidad de probabilidad (fdp). Tenemos que \\[P_X(x)=\\int_{-\\infty}^x p_X(t)dt\\] y \\(p_X(x)=P_X^{\\&#39;}(x)\\) en todos los puntos \\(x\\) en los que la FDA \\(P_X\\) es diferenciable. Ejemplo. Supongamos que elegimos un número al azar entre cero y uno, entonces \\[ p(x) = \\left\\{ \\begin{array}{lr} \\frac{1}{b-a} &amp; x \\in [0, 1]\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] es claro que \\(p_X(x) \\geq 0\\) para toda \\(x\\) y \\(\\int_{-\\infty}^{\\infty}p_X(x)dx=1\\), la FDA esta dada por \\[ P_X(x) = \\left\\{ \\begin{array}{lr} 0 &amp; x &lt; 0 \\\\ x &amp; x \\in [0,1]\\\\ 1 &amp; x&gt;b \\\\ \\end{array} \\right. \\] Vale la pena notar que en el caso de variables aleatorias continuas \\(P(X=x)=0\\) para toda \\(x\\) y pensar en \\(p_X(x)\\) como \\(P(X=x)\\) solo tiene sentido en el caso discreto. Familias Continuas importantes Distribución Uniforme \\(X\\) tiene una distribución \\(Uniforme(a,b)\\) si \\[ p(x) = \\left\\{ \\begin{array}{lr} \\frac{1}{b-a} &amp; x \\in [a,b]\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] donde \\(a &lt; b\\). La función de distribución acumualda es \\[ P_X(x) = \\left\\{ \\begin{array}{lr} 0 &amp; x &lt; a \\\\ \\frac{x-a}{b-a} &amp; x \\in [a,b]\\\\ 1 &amp; x&gt;b \\\\ \\end{array} \\right. \\] \\(E(X) = (a+b)/2, Var(X)= (b-a)^2/12\\) densidades &lt;- ggplot(data_frame(x = c(-5 , 5)), aes(x)) + stat_function(fun = dunif, aes(colour = &quot;a=0; b=1&quot;), show.legend = FALSE) + stat_function(fun = dunif, args = list(min = -5, max = 5), aes(colour = &quot;a=-5; b=5&quot;), show.legend = FALSE) + stat_function(fun = dunif, args = list(min = 0, max = 2), aes(colour = &quot;a=0; b=2&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, title = &quot;Distribución uniforme&quot;, colour = &quot;&quot;) dists &lt;- ggplot(data_frame(x = c(-5 , 5)), aes(x)) + stat_function(fun = punif, aes(colour = &quot;a=0; b=1&quot;), show.legend = FALSE) + stat_function(fun = punif, args = list(min = -5, max = 5), aes(colour = &quot;a=-5; b=5&quot;), show.legend = FALSE) + stat_function(fun = punif, args = list(min = 0, max = 2), aes(colour = &quot;a=0; b=2&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, title = &quot;FDA&quot;) grid.arrange(densidades, dists, ncol = 3, newpage = FALSE) Distribución Normal \\(X\\) tiene una distribución normal con parámetros \\(\\mu\\) y \\(\\sigma\\), denotado \\(X\\sim N(\\mu, \\sigma^2)\\) si \\[p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp\\bigg(-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\bigg)\\] \\(E(X)=\\mu, Var(X)=\\sigma^2\\) donde \\(\\mu \\in \\mathbb{R}\\) y \\(\\sigma&gt;0\\). Decimos que \\(X\\) tiene una distribución Normal estándar si \\(\\mu=0\\) y \\(\\sigma=1\\). Una variable aleatoria Normal estándar se denota tradicionalmente por \\(Z\\), su función de densidad de probabilidad por \\(\\phi(z)\\) y la función de probabilidad acumulada por \\(\\Phi(z)\\). Algunas porpiedades importantes son: Si \\(X \\sim N(\\mu, \\sigma^2)\\), entonces \\(Z=(X-\\mu)/\\sigma \\sim N(0,1)\\). Si \\(Z \\sim N(0, 1)\\) entonces \\(X = \\mu + \\sigma Z \\sim N(\\mu, sigma^2)\\). Si \\(X_i \\sim N(\\mu_i, \\sigma_i^2)\\), \\(i=1,...,n\\) independientes, entonces: \\[\\sum_{i=1}^n X_i \\sim N(\\sum_{i=1}^n \\mu_i, \\sum_{i=1}^n \\sigma_i^2)\\] Se sigue de 1 que si \\(X\\sim N(\\mu, \\sigma^2)\\), entonces \\[P(a&lt;X&lt;b) = P\\big(\\frac{a-\\mu}{\\sigma} &lt; Z &lt; \\frac{b-\\mu}{\\sigma}\\big)= \\Phi\\big(\\frac{b-\\mu}{\\sigma}\\big) - \\Phi\\big(\\frac{a-\\mu}{\\sigma}\\big)\\] densidades &lt;- ggplot(data_frame(x = c(-5 , 5)), aes(x)) + stat_function(fun = dnorm, aes(colour = &quot;m=0; s=1&quot;), show.legend = FALSE) + stat_function(fun = dnorm, args = list(mean = 1), aes(colour = &quot;m=1; s=1&quot;), show.legend = FALSE) + stat_function(fun = dnorm, args = list(sd = 2), aes(colour = &quot;m=1; s=2&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, title = &quot;Distribución Normal&quot;, colour = &quot;&quot;) dists &lt;- ggplot(data_frame(x = c(-5 , 5)), aes(x)) + stat_function(fun = pnorm, aes(colour = &quot;m=0; s=1&quot;), show.legend = FALSE) + stat_function(fun = pnorm, args = list(mean = 1), aes(colour = &quot;m=1; s=1&quot;), show.legend = FALSE) + stat_function(fun = pnorm, args = list(sd = 2), aes(colour = &quot;m=1; s=2&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, title = &quot;FDA&quot;) cuantiles &lt;- ggplot(data_frame(x = c(0, 1)), aes(x)) + stat_function(fun = qnorm, aes(colour = &quot;m=0; s=1&quot;)) + stat_function(fun = qnorm, args = list(mean = 1), aes(colour = &quot;m=1; s=1&quot;)) + stat_function(fun = qnorm, args = list(sd = 2), aes(colour = &quot;m=1; s=2&quot;)) + labs(y = &quot;&quot;, title = &quot;Funciones de cuantiles&quot;, colour = &quot;&quot;) grid.arrange(densidades, dists, cuantiles, ncol = 3, newpage = FALSE) Distribución Exponencial Una variable aleatoria \\(X\\) tienen distribución Exponencial con parámetro \\(\\beta\\), \\(X \\sim Exp(\\beta)\\) si, \\[ p(x) = \\left\\{ \\begin{array}{lr} \\frac{1}{\\beta}e^{-x/\\beta} &amp; x &gt;0\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X)=\\beta, Var(X)=\\beta^2\\) donde \\(\\beta &gt; 0\\). La distribución exponencial se utiliza para modelar tiempos de espera hasta un evento, por ejemplo modelar el tiempo de vida de un componente electrónico o el tiempo de espera entre llamadas telefónicas. Distribución Gamma Comencemos definiendo la función Gamma: para \\(\\alpha&gt;0\\), \\(\\Gamma(\\alpha)=\\int_0^{\\infty}y^{\\alpha-1}e^{-y}dy\\), esta función es una extensión de la función factorial, tenemos que si \\(n\\) es un entero positivo, \\(\\Gamma(n)=(n-1)!\\). Ahora, \\(X\\) tienen una distribución Gamma con parámetros \\(\\alpha\\), \\(\\beta\\), denotado como \\(X \\sim Gamma(\\alpha, \\beta)\\) si \\[ p(x) = \\left\\{ \\begin{array}{lr} \\frac{1}{\\beta^\\alpha \\Gamma(\\alpha)}x^{\\alpha-1}e^{-x/\\beta} &amp; x &gt;0\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X)=\\alpha \\beta, Var(X)=\\alpha \\beta^2\\) Vale la pena notar que una distribución exponencial es una \\(Gamma(1, \\beta)\\). Si \\(X_i \\sim Gamma(\\alpha_i, \\beta)\\) independientes, entonces \\(\\sum_{i=1}^n X_i \\sim Gamma(\\sum_{i=1}^n \\alpha_i, \\beta)\\). En la práctica la distribución Gamma se ha usado para modelar el tamaño de las reclamaciones de asegurados, en neurociencia se ha usado para describir la distribución de los intervalos entre los que ocurren picos. Finalmente, la distribución Gamma es muy usada en estadística bayesiana como a priori conjugada para el parámetro de precisión de una distribución Normal. densidades &lt;- ggplot(data_frame(x = c(0 , 12)), aes(x)) + stat_function(fun = dgamma, args = list(shape = 1), aes(colour = &quot;a=1;b=1&quot;), show.legend = FALSE) + stat_function(fun = dgamma, args = list(scale = 0.5, shape = 2), aes(colour = &quot;a=2;b=0.5&quot;), show.legend = FALSE) + stat_function(fun = dgamma, args = list(scale = 3, shape = 4), aes(colour = &quot;a=4,b=3&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, title = &quot;Distribución Gamma&quot;, colour = &quot;&quot;) dists &lt;- ggplot(data_frame(x = c(0 , 12)), aes(x)) + stat_function(fun = dgamma, args = list(shape = 1), aes(colour = &quot;a=1;b=1&quot;)) + stat_function(fun = dgamma, args = list(scale = 0.5, shape = 2), aes(colour = &quot;a=2;b=0.5&quot;)) + stat_function(fun = dgamma, args = list(scale = 3, shape = 4), aes(colour = &quot;a=4,b=3&quot;)) + labs(y = &quot;&quot;, title = &quot;FDA&quot;, color=&quot;&quot;) grid.arrange(densidades, dists, ncol = 2, newpage = FALSE) Distribución Beta \\(X\\) tiene una distrinución Beta con parámetros \\(\\alpha &gt; 0\\) y \\(\\beta &gt;0\\), \\(X \\sim Beta(\\alpha, \\beta)\\) si \\[ p(x) = \\left\\{ \\begin{array}{lr} \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} &amp; 0 &lt; x &lt; 1\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X)=\\alpha/(\\alpha+\\beta), Var(X)=\\alpha \\beta /[(\\alpha+\\beta)^2(\\alpha + \\beta + 1)]\\) La distribución Beta se ha utilizado para describir variables aleatorias limitadas a intervalos de longitud finita, por ejemplo, distribución del tiempo en sistemas de control o administración de proyectos, proporción de minerales en rocas, etc. densidades &lt;- ggplot(data_frame(x = c(0 , 1)), aes(x)) + stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 2), aes(colour = &quot;a=2; b=2&quot;), show.legend = FALSE) + stat_function(fun = dbeta, args = list(shape1 = 5, shape2 = 2), aes(colour = &quot;a=5; b=2&quot;), show.legend = FALSE) + stat_function(fun = dbeta, args = list(shape1 = .5, shape2 = .5), aes(colour = &quot;a=.5; b=.5&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, title = &quot;Distribución Beta&quot;, colour = &quot;&quot;) dists &lt;- ggplot(data_frame(x = c(0 , 1)), aes(x)) + stat_function(fun = pbeta, args = list(shape1 = 2, shape2 = 2), aes(colour = &quot;a=2; b=2&quot;)) + stat_function(fun = pbeta, args = list(shape1 = 5, shape2 = 2), aes(colour = &quot;a=5; b=2&quot;)) + stat_function(fun = pbeta, args = list(shape1 = .5, shape2 = .5), aes(colour = &quot;a=.5; b=.5&quot;)) + labs(y = &quot;&quot;, title = &quot;FDA&quot;, color=&quot;&quot;) grid.arrange(densidades, dists, ncol = 2, newpage = FALSE) "],
["simulacion-de-variables-aleatorias.html", "7.3 Simulación de variables aleatorias", " 7.3 Simulación de variables aleatorias Veremos métodos generales para simular muestras de distribuciones univariadas, generales se refiere a que se pueden utilizar independientemente de la forma de la función de densidad. Para utilizar estos métodos debemos tener un generador de números aleatorios confiable, pues la mayoría de los métodos consisten en una transformación de números aleatorios. 7.3.1 Variables aletaorias discretas Método de Inversión Supongamos que deseamos generar el valor de una variable aleatoria discreta \\(X\\) con función de probabilidad: \\[P(X=x_j) = p_j\\] con \\(j=0,1,2,..\\). Para lograr esto generamos un número aleatorio \\(U\\), esto es \\(U\\sim Uniforme(0,1)\\) y definimos \\[ X = \\left\\{ \\begin{array}{lr} x_0 &amp; U &lt; p_0\\\\ x_1 &amp; p_0 \\leq U &lt; p_0 + p_1\\\\ \\vdots &amp;\\\\ x_j &amp; \\sum_{i=0}^{j-1}p_i \\leq U &lt; \\sum_{i=0}^j p_i \\\\ \\vdots &amp; \\\\ \\end{array} \\right. \\] Como para \\(0&lt;a&lt;b&lt;1\\) tenemos que \\(P(a\\leq U &lt; b)=b-a\\), tenemos que \\[P(X=x_j)=P\\bigg\\{\\sum_{i=0}^{j-1}p_i \\leq U &lt; \\sum_{i=0}^{j}p_i \\bigg \\}=p_j\\] y por tanto \\(X\\) tiene la distribución deseada. Método de inversión Genera un número aleatorio \\(U\\), tal que \\(U \\in (0,1)\\). Si \\(U&lt;p_0\\) define \\(X=x_0\\) y para. Si \\(U&lt; p_0+p_1\\) define \\(X = x_1\\) y para. Si \\(U &lt; p_0 + p_1 + p_2\\) define \\(X=x_2\\) y para. \\(\\vdots\\) Si las \\(x_i\\), están ordenadas de tal manera que \\(x_0&lt;x_1&lt;x_2&lt;\\cdots\\) y si denotamos por \\(P\\) la función de distribución acumulada de \\(X\\), entonces \\(P(x_k)=\\sum_{i=0}^kp_i\\) y por tanto, \\(X\\) será igual a \\(x_j\\) si \\[P(x_{j-1}) \\leq U \\leq P(x_j)\\] En otras palabras, tras generar un número aleatorio \\(U\\) determinamos el valor de \\(X\\) encontrando el intervalo \\([P(x_{j-1}),P(x_j))\\) en el que cae \\(U\\), esto es equivalente a encontrar la inversa de \\(P(U)\\). El tiempo que uno tarda en generar una variable aleatoria discreta usando el método de arriba es proporcional al número de intervalos que uno debe buscar, es por esto que en ocasiones vale la pena considerar los posibles valores \\(x_j\\) en orden decreciente de \\(p_j\\). Utiliza la función runif de R y el método de inversión para generar 1000 simulaciones de una variable aleatoria \\(X\\) tal que \\(p_1=0.20, p_2= 0.15, p_3=0.25, p_4=0.40\\) donde \\(p_j=P(X=j)\\). Ejemplos Uniforme discreta. Supongamos que deseamos simular de una variable aleatoria uniforme discreta que toma valores \\(1,...,k\\), usando los resultados anteriores tenemos que: \\(X=j\\) si \\(\\frac{j-1}{n} \\leq U &lt; \\frac{j}{n}\\) Entonces \\(X=[kU] + 1\\), donde \\([x]\\) representa la parte entera de x. # uniforme discreta: donde n es el número de simulaciones y k el número de elementos runifD &lt;- function(n = 1, k) floor(k * runif(n)) + 1 # veamos un histograma de 1000 simulaciones de una distribución Uniforme # discreta con parámetro k = 20 x &lt;- runifD(n = 1000, k = 20) qplot(x, binwidth = 1) También podmeos usar la función sample de R: qplot(sample(1:20, size = 1000, replace= TRUE), binwidth = 1) Poisson: la clave para usar el método de la transformación inversa en este ejemplo es notar que: \\[p_{i+1}=\\frac{\\lambda}{i+1}p_i\\] donde \\(p_i=P(X=i) = e^-{\\lambda} \\lambda^i/i!\\), con \\(i=0,1,...\\). Ahora, la cantidad \\(i\\) se refiere al valor que estamos considerando, \\(p=p_i\\) es la probabilidad de \\(X = i\\) y \\(P=P(i)\\) es la probabilidad de \\(X\\leq i\\). Entonces, para generar una observación sequimos los siguientes pasos: Generar un número aleatorio \\(U\\), tal que \\(U \\in (0,1)\\). Inicializar: \\(i=0\\), \\(p=e^{-\\lambda}\\), \\(F=p\\). Si \\(U&lt;F\\), definir \\(X=i\\) y parar. \\(p=\\lambda p/(i+1)\\), \\(F=F+p\\), \\(i=i+1\\). Volver a 3. # Poisson usando Inversión rpoisI &lt;- function(lambda = 1){ U &lt;- runif(1) i &lt;- 0 p &lt;- exp(-lambda) P &lt;- p while(U &gt;= P){ p &lt;- lambda * p / (i + 1) P &lt;- P + p i &lt;- i + 1 } i } sims_pois &lt;- rerun(2000, rpoisI()) %&gt;% flatten_dbl() qplot(sims_pois, binwidth = 1) El algoritmo que propusimos verifica de manera sucesiva si el valor es 0, 1, etc. por lo que el número de comparaciones necesarias será uno más que el valor de la variable. Ahora, el valor esperado de una variable aleatoria Poisson es \\(\\lambda\\) por lo que en promedio se harían \\(1+\\lambda\\) busquedas. Cuando \\(\\lambda\\) es grande se puede mejorar el algoritmo buscando primero en valores cercanos a \\(\\lambda\\). Escribe una función en R que genere simulaciones de una variable aleatoria Poisson de la siguiente manera: define \\(I=[\\lambda]\\), y usa que \\(p_{i+1}=\\lambda p_i /(i+1)\\) para determinar \\(F\\) de manera recursiva. Genera un número aleatorio \\(U\\), determina si \\(X \\leq I\\) comparando si \\(U \\leq F(I)\\). Si \\(X \\leq I\\) busca hacia abajo comenzando en \\(I\\), de lo contrario busca hacia arriba comenzando por \\(I+1\\). Compara el tiempo que tardan los dos algoritmos en 5000 simulaciones de una variable aleatoria Poisson con parámetro \\(\\lambda=10, 200, 500\\). Aceptación y rechazo Supongamos que tenemos un método eficiente para generar simulaciones de una variable aleatoria con función de probabilidad masa \\(\\{q_j, j\\geq 0\\}\\), podemos usarla como la base para simular de una distribución que tiene función de probabilidad masa \\(\\{p_j, j \\geq 0\\}\\), para hacer esto comenzamos simulando una variable aleatoria \\(Y\\) con función \\(\\{q_j\\}\\) y después aceptamos o rechazamos el valor simulado con una probabilidad proporcional a \\(p_Y/q_Y\\). En particular, sea \\(c\\) una constante tal que \\[\\frac{p_j}{q_j}\\leq c\\] para toda \\(j\\) con \\(p_j &gt; 0\\). Entonces el método de aceptación y rechazo para simular una variable aleatoria \\(X\\) con función masa de probabilidad \\(p_j=P(x=j)\\) es como sigue: Método de aceptación y rechazo Simula el valor de \\(Y\\), con función de probabilidad masa \\(q_j\\). Genera un número aleatorio \\(U\\), tal que \\(U \\in (0,1)\\). Si \\(U &lt; p_y/(cq_y)\\) definimos \\(X=Y\\) y paramos, en otro caso regresamos a 1. . Supongamos que queremos simular el valor de una variable aleatoria \\(X\\) que toma uno de los valores \\(1, 2,3,4\\) con probabilidades \\(p_1=0.20, p_2= 0.15, p_3=0.25, p_4=0.40\\) donde \\(p_j=P(X=j)\\). Usemos el método de aceptación y rechazo con \\(q\\) la densidad uniforme en \\(1,...,10\\). Implementa una función usando el método de aceptación y rechazo. ¿Cómo se compara en velocidad con la función que implementaste usando el método de la transformación inversa? En promedio este algoritmo requiere \\(1/c\\) iteraciones para obtener un valor generado para \\(X\\). 7.3.2 Variables aleatorias continuas Transformación inversa Sea \\(U\\) una variable aleatoria con ditribución \\(U(0,1)\\). Para cualquier función de distribución \\(F\\) la variable aleatoria \\(X\\) definida como \\[X = F^{-1}(U)\\] tiene distribución \\(F\\). La proposición anterior nos da un camino para simular variables aleatorias continuas generando un número aleatorio \\(U\\) y definiendo \\(X = F^{-1}(U)\\): ggplot(data_frame(x = c(-2 , 2)), aes(x)) + geom_hline(yintercept = 0, color = &quot;gray&quot;) + geom_vline(xintercept = 0, color = &quot;gray&quot;) + stat_function(fun = qnorm, aes(color = &quot;fq&quot;)) + stat_function(fun = dnorm, aes(color = &quot;fdp&quot;)) + stat_function(fun = pnorm, aes(color = &quot;fda&quot;)) + coord_fixed() + labs(color = &quot;&quot;, title = &quot;Método de transformación inversa caso Normal&quot;) Ejemplo: Exponencial Si \\(X\\) es una variable aleatoria exponencial con tasa 1, entonces \\[F(x)=1-e^{-x}\\] Si definimos \\(x=F^{-1}(u)\\), entonces \\[u=F(x)=1-e^{-x}\\] o \\[x = -log(1-u)\\] Vale la pena notar que si \\(U\\) tiene distribución \\(U(0,1)\\), \\(1-U\\) también se distribuye uniforme(0,1). simExp &lt;- function(){ u &lt;- runif(1) x &lt;- -log(u) } Notemos que para cualquier constante positiva \\(c\\), \\(cX\\) tiene distribución exponencial con media \\(c\\), por tanto una variable aleatoria exponencial con parámetro \\(\\beta\\) se puede generar de la siguiente manera: \\[X=-\\beta log(U)\\] simExpBeta &lt;- function(beta){ -beta * log(runif(1)) } sims_exp &lt;- rerun(1000, simExpBeta(2)) %&gt;% flatten_dbl() mean(sims_exp) #&gt; [1] 2 ggplot() + geom_histogram(aes(x = sims_exp, y = ..density..), binwidth = 0.7) El algoritmo anterior también provee una manera de generar variables aleatorias Poisson. Primero, recordemos que un proceso Poisson con tasa \\(\\lambda\\) resulta cuando los tiempos de espera entre eventos sucesivos son exponenciales independientes con parámetro \\(\\beta\\), para este proceso el número de eventos al tiempo 1 se distribuye Poisson con parámetro \\(\\lambda = 1/\\beta\\). Para este proceso, \\(N(1)\\), el número de eventos en el tiempo 1 se distribuye Poisson con media \\(1/\\beta\\). Si denotamos por \\(X_i\\) los tiempos entre eventos, el \\(n\\)-ésimo evento ocurrirá en el tiempo \\(\\sum_{i=1}^n X_i\\) y por tanto el número de eventos al tiempo 1 se puede expresar como: \\[N(1)=max\\bigg\\{n: \\sum_{i=1}^nX_i \\leq 1\\bigg\\}\\] Esto es, el número de eventos al tiempo 1 es igual a la \\(n\\) mayor para la cual el n-ésimo evento ocurrió al tiempo 1. Por ejemplo, si el cuarto evento ocurrió al tiempo uno pero el quinto no, habría 4 eventos al tiempo 1. Por tanto, usando el ejemplo anterior, podemos generar una variable aleatoria Poisson con media \\(\\lambda = 1/\\beta\\) generando números aleatorios \\(U_1,...U_n,...\\) y definiendo \\[N=max\\bigg\\{n: \\sum_{i=1}^n -\\beta log(U_i)\\bigg\\}\\] \\[=max\\bigg\\{n: \\sum_{i=1}^n -1/\\lambda log(U_i)\\bigg\\}\\] \\[=max\\bigg\\{n:\\sum_{i=1}^n log(U_i)\\geq -\\lambda \\bigg\\}\\] \\[=max\\{n:log(U_1\\cdot\\cdot\\cdot U_n) \\geq -\\lambda\\}\\] \\[=max\\{n: U_1\\cdot \\cdot \\cdot U_n \\geq e^{-\\lambda}\\}\\] Entonces, una variable aleatoria Poisson con media \\(\\lambda\\) se puede generar a partir de una sucesión de números aleatorios, generando números hasta que el producto sea menor a \\(e^{-\\lambda}\\) y definiendo \\(X\\) como uno menos del número de números aleatorios requeridos. \\[N = min\\{n: U_1\\cdot\\cdot\\cdot U_n &lt; e^{-\\lambda}\\} - 1\\] poisson &lt;- function(lambda){ u &lt;- runif(1) N &lt;- 1 while(u &gt; exp(-lambda)){ u &lt;- u * runif(1) N &lt;- N + 1 } N - 1 } poisson(10) #&gt; [1] 8 mean(rdply(1000, poisson(10))$V1) #&gt; [1] 10 Ejemplo: Gamma Supongamos que deseamos generar el valor de una variable aleatoria \\(gamma(n,\\beta)\\), la función de distribución es, \\[\\int_{0}^x \\frac{1}{\\beta^n \\Gamma(n)}x^{n-1}e^{-x/\\beta}dy\\] la inversa de la función de distribución acumulada anterior no se puede escribir de forma cerrada. Sin embargo, podemos simular de ella usando que una \\(gamma(n,\\beta)\\) se puede ver como la suma de \\(n\\) exponenciales independientes, cada una con parámetro \\(\\beta\\): \\[X=-\\beta log(U_1)-\\cdot\\cdot\\cdot - \\beta log(U_n)\\] \\[=-\\beta log(U_1\\cdot\\cdot\\cdot U_n)\\] donde la identidad \\(\\sum log(x_i) = log(x_1\\cdot\\cdot\\cdot x_n)\\) deriva en ganancias computacionales. gamma_nb &lt;- function(n, beta){ -beta * log(Reduce(`*`,runif(10))) } sims_gamma &lt;- rdply(1000, gamma_nb(n = 10, beta = 2)) mean(sims_gamma$V1) #&gt; [1] 20 var(sims_gamma$V1) #&gt; [1] 39 Aceptación y rechazo Supongamos que tenemos un método para generar variables aleatorias con función de densidad \\(g(x)\\), podemos usarla como base para generar observaciones de una variable aleatoria con densidad \\(f(x)\\) generando \\(Y\\) de \\(g\\) y después aceptando el valor generado con una probabilidad proporcional a \\(f(Y)/g(Y)\\). Sea \\(c\\) una constante tal que \\[\\frac{f(y)}{g(y)} \\leq c\\] para toda \\(c\\), entonces el método se puede escribir como sigue: Aceptación y rechazo Genera \\(Y\\) con densidad \\(g\\). Genera un número aleatorio \\(U\\). Si \\(U \\leq \\frac{f(Y)}{cg(Y)}\\) define \\(X=Y\\), de lo contrario regresa a 1. El método de aceptación y rechazo es análogo al correspondiente a variables aleatorias discretas. La variable aleatoria generada usando el método de aceptación y rechazo tiene densidad \\(f\\). El número de iteraciones del algoritmo que se necesitan es una variable aleatoria geométrica con media \\(c\\). Ejemplo: Beta Usemos el método de aceptación y rechazo para generar observaciones de una variable aleatoria \\(beta(2,4)\\): \\[f(x)=20x(1-x)^3\\] La variable aleatoria beta toma valores en el intervalo (0,1) por lo que consideremos \\(g(x)=1\\), para \\(0&lt;x&lt;1\\). Para determinar la menor \\(c\\) tal que \\(f(x)/g(x)\\leq c\\) podemos derivar y obtenemos \\(c = 135/64\\), \\[\\frac{f(x)}{g(x)} \\leq 20 \\cdot \\frac{1}{4} \\bigg(\\frac{3}{4}\\bigg)^3 = \\frac{135}{64}\\] y \\[\\frac{f(x)}{cg(x)}=\\frac{256}{27}x(1-x)^3\\] por lo que el procedimiento para simular sería el siguiente: beta24 &lt;- function(){ # 1. Generar dos números aleatorios U_1, U_2. u1 &lt;- runif(1) u2 &lt;- runif(1) # 2. Comparar con f(x)/cg(x) while(u2 &gt; 256 / 27 * u1 * (1 - u1) ^ 3){ u1 &lt;- runif(1) u2 &lt;- runif(1) } u1 } sims &lt;- rdply(1000, beta24) mean(sims$V1) #&gt; [1] 0.33 Ejemplo: Gamma(3/2, 1) Supongamos que deseamos generar simulaciones de una variable aleatoria con densidad gamma(3/2, 1): \\[f(x)=\\frac{1}{\\Gamma(3/2)}x^{1/2}e^{-x}\\] dado que la variable aleatoria de nuestro interés se concentra en los números positivos, y tiene media \\(3/2\\), es conveniente usar el método de aceptación y rechazo con la variable aleatoria exponencial de la misma media. \\[g(x)=\\frac{2}{3}e^{-x2/3}\\] Usa el método de aceptación y rechazo para generar 1000 observaciones de una variable aleatoria con distribución gamma(3/2,1). Ejemplo: Variable aleatoria normal Nuestro objetivo es primero, simular una variable aleatoria normal estándar Z, para ello comencemos notando que el valor absoluto de Z tiene función de densidad: \\[f(x)=\\frac{2}{\\sqrt{2\\pi}}e^{-x^2/2}\\] con soporte en los reales positivos. Generaremos observaciones de la densidad anterior usando el método de aceptación y rechazo con \\(g\\) una densidad exponencial com media 1: \\[g(x)= e^{-x}\\] Ahora, \\(\\frac{f(x)}{g(x)}=\\sqrt{2/\\pi}e^{x - x^2/2}\\) y por tanto el máximo valor de \\(f(x)/g(x)\\) ocurre en el valor \\(x\\) que maximiza \\(x - x^2/2\\), esto ocurre en \\(x=1\\), y podemos tomar \\(c=\\sqrt{2e/\\pi}\\), \\[\\frac{f(x)}{cg(x)}=exp\\bigg\\{x - \\frac{x^2}{2}-{1}{2}\\bigg\\}\\] \\[=exp\\bigg\\{\\frac{(x-1)^2}{2}\\bigg\\}\\] y por tanto podemos generar el valor absoluto de una variable aleatoria con distribución normal estándar de la siguiente manera: Genera \\(Y\\) una variable aleatoria exponencial con tasa 1. Genera un número aleatorio \\(U\\). Si \\(U \\leq exp\\{-(Y-1)^2/2\\}\\) define \\(X=Y\\), en otro caso vuelve a 1. Para generar una variable aleatoria con distribución normal estándar \\(Z\\) simplemente elegimos \\(X\\) o \\(-X\\) con igual probabilidad. Notemos además que en paso 3 \\(Y\\) es aceptado si \\(U \\leq exp(-(Y-1)^2/2)\\) esto es equivalente a \\(-log(U) \\geq (Y-1)^2/2\\) y recordemos que \\(-log(U)\\) es exponencial con parámetro 1, por lo que podems escribir los pasos como: Genera 2 exponenciales independientes con parámetro 1: \\(Y_1, Y_2\\). Si \\(Y_2 \\geq (Y_1 - 1)^2/2\\) define \\(X=Y\\), de lo contrario vuelve a 1. Supongamos ahora que aceptamos \\(Y_1\\), esto es equivalente a decir que \\(Y_2\\) es mayor a \\((Y_1 - 1)^2/2\\), y la diferencia \\(Y_2 - (Y_1 - 1)^2/2\\) se distribuye exponencial con parámetro 1. Esto es, cuando aceptamos en el segundo paso no sólo obtenemos \\(X\\) sino que calculando \\(Y_2 - (Y_1 - 1)^2/2\\) podemos generar una variable aleatoria exponencial con parámetro 1 independiente de \\(X\\). Esto es relevante pues si estamos generando una sucesión de variables aleatorias normales obtendríamos un algoritmo más eficiente. "],
["tareas.html", "Tareas", " Tareas Las tareas se envían por correo a teresa.ortiz.mancera@gmail.com con título: EstComp-TareaXX (donde XX corresponde al número de tarea, 01..). Las tareas deben incluir código y resultados (si conocen Rmarkdown es muy conveniente para este propósito). "],
["transformacion-de-datos-1.html", "2-Transformación de datos", " 2-Transformación de datos Entrega: Lunes 27 de agosto. Utiliza los datos de vuelos (flights) para responder la siguientes preguntas. ¿A qué hora del día debo volar para evitar, lo más posible, retrasos de salida? Para cada destino calcula el total de minutos de retraso de salida. Para cada vuelo calcula su proporción del total de retrasos de su destino. Los retrasos suelen estar correlacionados temporalmente, incluso cuando se ha resuelto el problema que ocasionó los retrasos iniciales, vuelos posteriores suelen mantener algo de retraso. Usando la función lag() explora como el retraso de salida de un vuelo se relaciona con el retraso de salida del vuelo anterior. Realiza una gráfica para visualizar tus hallazgos. Para cada destino, puedes encontrar vuelos sospechosamente rápidos o lentos? (quizá debido a porblemas en la captura de los datos). Calcula el tiempo de vuelo relativo a la mediana de tiempo de vuelo a su destino. Qué vuelos se retrasaron más en el aire? Encuentra los destinos que se vuelan por al menos dos compañías (carriers). "],
["datos-limpios-1.html", "3-Datos Limpios", " 3-Datos Limpios Entrega: Lunes 3 de septiembre. Descarga los datos aquí. En la carpeta de arriba encontrarás un archivo de excel (m_013.xls), este archivo contiene información de causas de mortalidad en México entre 2000 y 2008. Contesta las siguientes preguntas: ¿Cuáles son las variables en esta base de datos? ¿La tabla de datos cumple con los principios de datos limpios? ¿Qué problemas presenta? La información del archivo de excel se ha guardado también en archivos de texto (csv) 2001-2008, lee y limpia los datos para que cumplan los principios de datos limpios. Recuerda que las modificaciones deben de ser reproducibles, para esto guarda tu trabajo en un script. El archivo de excel indice_marginacion.xlsx contiene el índice por entidad para los años 2000 y 2010. Realiza una gráfica donde compares la marginación por entidad con las tasas de mortalidad correspondientes al 2000. Deberás unir las dos fuentes de información. Observaciones: Puedes filtrar/eliminar los valores a Total si crees que es más claro. Intenta usar las funciones que estudiamos en la clase (gather, separate, select, filter). Si aún no te sientes cómodx con las funciones de clase (y lo intentaste varias veces) puedes hacer las manipulaciones usando otra herramienta (incluso Excel, una combinación de Excel y R o cualquier software que conozcas); sin embargo, debes documentar tus pasos claramente, con la intención de mantener métodos reproducibles. "],
["probabilidad.html", "4-Probabilidad", " 4-Probabilidad Urna: 10 personas (con nombres distintos) escriben sus nombres y los ponen en una urna, después seleccionan un nombre (al azar). Sea A el evento en el que ninguna persona selecciona su nombre, ¿Cuál es la probabilidad del evento A? Supongamos que hay 3 personas con el mismo nombre, ¿Cómo calcularías la probabilidad del evento A en este nuevo experimento? Definimos \\(X\\) como la variable aleatoria del número de juegos antes de que termine el experimento de la ruina del jugador, grafica la distribución de probabilidad de \\(X\\) (calcula \\(P(X=1), P(X=2),...,P(X=100)\\)). "],
["bootstrap.html", "5-Bootstrap", " 5-Bootstrap Distribución muestral. Consideramos la base de datos primaria, y la columna de calificaciones de español 3o de primaria (esp_3). Selecciona una muestra de tamaño \\(n = 10, 100, 200\\). Para cada muestra calcula media y el error estándar de la media usando el principio del plug-in: \\(\\hat{\\mu}=\\bar{x}\\), y \\(\\hat{se}(\\bar{x})=\\hat{\\sigma}_{P_n}/\\sqrt{n}\\). Ahora aproximareos la distribución muestral, para cada tamaño de muestra \\(n\\): simula 10,000 muestras aleatorias, ii) calcula la media en cada muestra, iii) Realiza un histograma de la distribución muestral de las medias (las medias del paso anterior) iv) aproxima el error estándar calculando la desviación estándar de las medias del paso ii. Calcula el error estándar de la media para cada tamaño de muestra usando la información poblacional (ésta no es una aproximación), usa la fórmula: \\(se_P(\\bar{x}) = \\sigma_P/ \\sqrt{n}\\). ¿Cómo se comparan los errores estándar correspondientes a los distintos tamaños de muestra? Bootstrap correlación. Nuevamente trabaja con los datos primaria, selecciona una muestra aleatoria de tamaño 100 y utiliza el principio del plug-in para estimar la correlación entre la calificación de \\(y=\\)español 3 y la de \\(z=\\)español 6: \\(\\hat{corr}(y,z)\\). Usa bootstrap para calcular el error estándar de la estimación. Solución 7.3.2.1 1. Distribución muestral Suponemos que me interesa hacer inferencia del promedio de las calificaciones de los estudiantes de tercero de primaria en Ciudad de México. En este ejercicio planteamos 3 escenarios (que simulamos): 1) que tengo una muestra de tamaño 10, 2) que tengo una muestra de tamaño 100, y 3) que tengo una muestra de tamaño 1000 Selección de muestras: library(tidyverse) primarias &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/tereom/est-computacional-2018/master/data/primarias.csv&quot;) set.seed(373783326) muestras &lt;- data_frame(tamanos = c(10, 100, 1000)) %&gt;% mutate(muestras = map(tamanos, ~sample(primarias$esp_3, size = .))) Ahora procedemos de manera usual en estadística (usando fórmulas y no simulación), estimo la media de la muestra con el estimador plug-in \\[\\bar{x}={1/n\\sum x_i}\\] y el error estándar de \\(\\bar{x}\\) con el estimador plug-in \\[\\hat{se}(\\bar{x}) =\\bigg\\{\\frac{1}{n^2}\\sum_{i=1}^n(x_i-\\bar{x})^2\\bigg\\}^{1/2}\\] Estimadores plug-in: se_plug_in &lt;- function(x){ x_bar &lt;- mean(x) n_x &lt;- length(x) var_x &lt;- 1 / n_x ^ 2 * sum((x - x_bar) ^ 2) sqrt(var_x) } muestras_est &lt;- muestras %&gt;% mutate( medias = map_dbl(muestras, mean), e_estandar_plug_in = map_dbl(muestras, se_plug_in) ) muestras_est ## # A tibble: 3 x 4 ## tamanos muestras medias e_estandar_plug_in ## &lt;dbl&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 &lt;dbl [10]&gt; 567. 21.1 ## 2 100 &lt;dbl [100]&gt; 575. 6.11 ## 3 1000 &lt;dbl [1,000]&gt; 576. 2.11 Ahora, recordemos que la distribución muestral es la distribución de una estadística, considerada como una variable aleatoria. Usando esta definción podemos aproximarla, para cada tamaño de muestra, simulando: 1) simulamos muestras de tamaño \\(n\\) de la población, 2) calculamos la estadística de interés (en este caso \\(\\bar{x}\\)), 3) vemos la distribución de la estadística a lo largo de simulaciones. Histogramas de distribución muestral y aproximación de errores estándar con simulación muestras_sims &lt;- muestras_est %&gt;% mutate( sims_muestras = map(tamanos, ~rerun(10000, sample(primarias$esp_3, size = ., replace = TRUE))), sims_medias = map(sims_muestras, ~map_dbl(., mean)), e_estandar_aprox = map_dbl(sims_medias, sd) ) sims_medias &lt;- muestras_sims %&gt;% select(tamanos, sims_medias) %&gt;% unnest(sims_medias) ggplot(sims_medias, aes(x = sims_medias)) + geom_histogram(binwidth = 2) + facet_wrap(~tamanos, nrow = 1) Notamos que la variación en la distribución muestral decrece conforme aumenta el tamaño de muestra, esto es esperado pues el error estándar de una media es \\(\\sigma_P / \\sqrt{n}\\), y dado que en este ejemplo estamos calculando la media para la misma población el valor poblacional \\(\\sigma_P\\) es constante y solo cambia el denominador. Nuestros valores de error estándar con simulación están en la columna e_estandar_aprox: muestras_sims %&gt;% select(tamanos, medias, e_estandar_plug_in, e_estandar_aprox) ## # A tibble: 3 x 4 ## tamanos medias e_estandar_plug_in e_estandar_aprox ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 567. 21.1 21.0 ## 2 100 575. 6.11 6.69 ## 3 1000 576. 2.11 2.13 En este ejercicio estamos simulando para examinar las distribuciones muestrales y para ver que podemos aproximar el error estándar de la media usando simulación; sin embargo, dado que en este caso hipotético conocemos la varianza poblacional y la fórmula del error estándar de una media, por lo que podemos calcular el verdadero error estándar para una muestra de cada tamaño. Calcula el error estándar de la media para cada tamaño de muestra usando la información poblacional: muestras_sims_est &lt;- muestras_sims %&gt;% mutate(e_estandar_pob = sd(primarias$esp_3) / sqrt(tamanos)) muestras_sims_est %&gt;% select(tamanos, e_estandar_plug_in, e_estandar_aprox, e_estandar_pob) ## # A tibble: 3 x 4 ## tamanos e_estandar_plug_in e_estandar_aprox e_estandar_pob ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 21.1 21.0 21.1 ## 2 100 6.11 6.69 6.67 ## 3 1000 2.11 2.13 2.11 En la tabla de arriba podemos comparar los 3 errores estándar que calculamos, recordemos que de estos 3 el plug-in es el único que podríamos obtener en un escenario real pues los otros dos los calculamos usando la población. Una alternativa al estimador plug-in del error estándar es usar bootstrap (en muchos casos no podemos calcular el error estándar plug-in por falta de fórmulas) pero podemos usar bootstrap: utilizamos una estimación de la distribución poblacional y calculamos el error estándar bootstrap usando simulación. Hacemos el mismo procedimiento que usamos para calcular e_estandar_apox pero sustituimos la distribuciín poblacional por la distriución empírica. Hagámoslo usando las muestras que sacamos en el primer paso: muestras_sims_est_boot &lt;- muestras_sims_est %&gt;% mutate( sims_muestras_boot = map2(muestras, tamanos, ~rerun(10000, sample(.x, size = .y, replace = TRUE))), sims_medias_boot = map(sims_muestras_boot, ~map_dbl(., mean)), e_estandar_boot = map_dbl(sims_medias_boot, sd) ) Y la tabla con todos los errores estándar quedaría: muestras_sims_est_boot %&gt;% select(tamanos, e_estandar_boot, e_estandar_plug_in, e_estandar_aprox, e_estandar_pob) ## # A tibble: 3 x 5 ## tamanos e_estandar_boot e_estandar_plug… e_estandar_aprox e_estandar_pob ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 21.3 21.1 21.0 21.1 ## 2 100 6.11 6.11 6.69 6.67 ## 3 1000 2.08 2.11 2.13 2.11 Observamos que el estimador bootstrap del error estándar es muy similar al estimador plug-in del error estándar, esto es esperado pues se calcularon con la misma muestra y el error estándar bootstrap converge al plug-in conforme incrementamos el número de muestras bootstrap. 2. Correlación Bootstrap correlación. Nuevamente trabaja con los datos primaria, selecciona una muestra aleatoria de tamaño 100 y utiliza el principio del plug-in para estimar la correlación entre la calificación de \\(y=\\)español 3 y la de \\(z=\\)español 6: \\(\\hat{corr}(y,z)\\). Usa bootstrap para calcular el error estándar de la estimación. Selección de la muestra set.seed(11729874) muestra &lt;- sample_n(primarias, size = 100) Estimador de la correlación: cor(muestra$esp_3, muestra$esp_6) ## [1] 0.7760901 Error estándar con bootstrap cor_rep &lt;- function(){ muestra_boot &lt;- sample_n(muestra, size = 100, replace = TRUE) cor(muestra_boot$esp_3, muestra_boot$esp_6) } replicaciones &lt;- rerun(10000, cor_rep()) %&gt;% flatten_dbl() sd(replicaciones) ## [1] 0.04548587 "],
["cobertura-de-intervalos-de-confianza.html", "6-Cobertura de intervalos de confianza", " 6-Cobertura de intervalos de confianza En este problema realizarás un ejercicio de simulación para comparar la exactitud de distintos intervalos de confianza. Simularás muestras de una distribución Poisson con parámetro \\(\\lambda=2.5\\) y el estadístico de interés es \\(\\theta=exp(-2\\lambda)\\). Sigue el siguiente proceso: Genera una muestra aleatoria de tamaño \\(n=60\\) con distribución \\(Poisson(\\lambda)\\), parámetro \\(\\lambda=2.5\\) (en R usa la función rpois()). Genera \\(10,000\\) muestras bootstrap y calcula intervalos de confianza del 95% para \\(\\hat{\\theta}\\) usando 1) el método normal, 2) percentiles y 3) \\(BC_a\\). Revisa si el intervalo de confianza contiene el verdadero valor del parámetro (\\(\\theta=exp(-2\\cdot2.5)\\)), en caso de que no lo contenga registra si falló por la izquierda (el límite inferior &gt;1) o falló por la derecha (el límite superior &lt;1). Repite el proceso descrito 1000 veces y llena la siguiente tabla: Método % fallo izquierda % fallo derecha Cobertura Longitud promedio Normal Percentiles BC_a La columna cobertura es una estimación de la cobertura del intervalo basada en las simulaciones, para calcularla simplemente escribe el porcentaje de los intervalos que incluyeron el verdadero valor del parámetro. La longitud promedio es la longitud promedio de los intervalos de confianza bajo cada método. Realiza una gráfica de páneles, en cada panel mostrarás los resultados de uno de los métodos (normal, percentiles y BC_a), el eje x corresponderá al número de intervalo de confianza (\\(1,...,1000\\)) y en el vertical graficarás los límites de los intervalos, es decir graficarás 2 líneas (usa geom_line()) una corresponderá a los límites inferiores de los intervalos, y otra a los superiores. Repite los incisos a) y b) seleccionando muestras de tamaño \\(300\\). Nota: Un ejemplo en donde la cantidad \\(P(X=0)^2 = e^{-\\lambda}\\) es de interés es como sigue: las llamadas telefónicas a un conmutador se modelan con un proceso Poisson y \\(\\lambda\\) es el número promedio de llamadas por minuto, entonce \\(e^{- \\lambda}\\) es la probabilidad de que no se reciban llamadas en 1 minuto. "],
["referencias.html", "Referencias", " Referencias "]
]
